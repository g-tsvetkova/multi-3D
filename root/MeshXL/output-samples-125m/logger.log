	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='dummy_dataset', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
Epoch [0/16]; Iter [0/80]; loss 5.1218; gen_loss 5.1218; LR 1.00e-06; Iter time 0.25s; ETA 0:00:20; Mem 449.63MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=16, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=16, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
Epoch [0/10]; Iter [0/5650]; loss 4.9258; gen_loss 4.9258; LR 1.00e-06; Iter time 0.50s; ETA 0:47:15; Mem 14467.61MB
Epoch [0/10]; Iter [10/5650]; loss 4.9654; gen_loss 4.9654; LR 1.99e-06; Iter time 0.26s; ETA 0:24:32; Mem 14467.61MB
Epoch [0/10]; Iter [20/5650]; loss 4.9346; gen_loss 4.9346; LR 2.98e-06; Iter time 0.26s; ETA 0:24:29; Mem 14467.61MB
Epoch [0/10]; Iter [30/5650]; loss 4.9471; gen_loss 4.9471; LR 3.97e-06; Iter time 0.26s; ETA 0:24:26; Mem 14467.61MB
Epoch [0/10]; Iter [40/5650]; loss 4.9779; gen_loss 4.9779; LR 4.96e-06; Iter time 0.26s; ETA 0:24:25; Mem 14467.61MB
Epoch [0/10]; Iter [50/5650]; loss 4.9744; gen_loss 4.9744; LR 5.95e-06; Iter time 0.26s; ETA 0:24:23; Mem 14467.61MB
Epoch [0/10]; Iter [60/5650]; loss 4.9361; gen_loss 4.9361; LR 6.94e-06; Iter time 0.26s; ETA 0:24:23; Mem 14467.61MB
Epoch [0/10]; Iter [70/5650]; loss 4.8974; gen_loss 4.8974; LR 7.93e-06; Iter time 0.26s; ETA 0:24:20; Mem 14467.61MB
Epoch [0/10]; Iter [80/5650]; loss 4.9286; gen_loss 4.9286; LR 8.92e-06; Iter time 0.26s; ETA 0:24:17; Mem 14467.61MB
Epoch [0/10]; Iter [90/5650]; loss 4.9100; gen_loss 4.9100; LR 9.91e-06; Iter time 0.26s; ETA 0:24:14; Mem 14467.61MB
Epoch [0/10]; Iter [100/5650]; loss 4.8863; gen_loss 4.8863; LR 1.09e-05; Iter time 0.26s; ETA 0:24:12; Mem 14467.61MB
Epoch [0/10]; Iter [110/5650]; loss 4.8546; gen_loss 4.8546; LR 1.19e-05; Iter time 0.26s; ETA 0:24:11; Mem 14467.61MB
Epoch [0/10]; Iter [120/5650]; loss 4.9117; gen_loss 4.9117; LR 1.29e-05; Iter time 0.26s; ETA 0:24:07; Mem 14467.61MB
Epoch [0/10]; Iter [130/5650]; loss 4.8373; gen_loss 4.8373; LR 1.39e-05; Iter time 0.26s; ETA 0:24:06; Mem 14467.61MB
Epoch [0/10]; Iter [140/5650]; loss 4.8300; gen_loss 4.8300; LR 1.49e-05; Iter time 0.26s; ETA 0:24:03; Mem 14467.61MB
Epoch [0/10]; Iter [150/5650]; loss 4.8164; gen_loss 4.8164; LR 1.58e-05; Iter time 0.26s; ETA 0:23:58; Mem 14467.61MB
Epoch [0/10]; Iter [160/5650]; loss 4.8361; gen_loss 4.8361; LR 1.68e-05; Iter time 0.26s; ETA 0:23:58; Mem 14467.61MB
Epoch [0/10]; Iter [170/5650]; loss 4.8258; gen_loss 4.8258; LR 1.78e-05; Iter time 0.26s; ETA 0:23:55; Mem 14467.61MB
Epoch [0/10]; Iter [180/5650]; loss 4.7242; gen_loss 4.7242; LR 1.88e-05; Iter time 0.26s; ETA 0:23:53; Mem 14467.61MB
Epoch [0/10]; Iter [190/5650]; loss 4.8113; gen_loss 4.8113; LR 1.98e-05; Iter time 0.26s; ETA 0:23:49; Mem 14467.61MB
Epoch [0/10]; Iter [200/5650]; loss 4.7130; gen_loss 4.7130; LR 2.08e-05; Iter time 0.26s; ETA 0:23:47; Mem 14467.61MB
Epoch [0/10]; Iter [210/5650]; loss 4.7060; gen_loss 4.7060; LR 2.18e-05; Iter time 0.26s; ETA 0:23:44; Mem 14467.61MB
Epoch [0/10]; Iter [220/5650]; loss 4.7313; gen_loss 4.7313; LR 2.28e-05; Iter time 0.26s; ETA 0:23:42; Mem 14467.61MB
Epoch [0/10]; Iter [230/5650]; loss 4.7456; gen_loss 4.7456; LR 2.38e-05; Iter time 0.26s; ETA 0:23:38; Mem 14467.61MB
Epoch [0/10]; Iter [240/5650]; loss 4.7091; gen_loss 4.7091; LR 2.48e-05; Iter time 0.26s; ETA 0:23:38; Mem 14467.61MB
Epoch [0/10]; Iter [250/5650]; loss 4.6924; gen_loss 4.6924; LR 2.58e-05; Iter time 0.26s; ETA 0:23:33; Mem 14467.61MB
Epoch [0/10]; Iter [260/5650]; loss 4.6691; gen_loss 4.6691; LR 2.67e-05; Iter time 0.26s; ETA 0:23:32; Mem 14467.61MB
Epoch [0/10]; Iter [270/5650]; loss 4.7137; gen_loss 4.7137; LR 2.77e-05; Iter time 0.26s; ETA 0:23:30; Mem 14467.61MB
Epoch [0/10]; Iter [280/5650]; loss 4.6611; gen_loss 4.6611; LR 2.87e-05; Iter time 0.26s; ETA 0:23:26; Mem 14467.61MB
Epoch [0/10]; Iter [290/5650]; loss 4.6398; gen_loss 4.6398; LR 2.97e-05; Iter time 0.26s; ETA 0:23:25; Mem 14467.61MB
Epoch [0/10]; Iter [300/5650]; loss 4.5794; gen_loss 4.5794; LR 3.07e-05; Iter time 0.26s; ETA 0:23:22; Mem 14467.61MB
Epoch [0/10]; Iter [310/5650]; loss 4.5947; gen_loss 4.5947; LR 3.17e-05; Iter time 0.26s; ETA 0:23:20; Mem 14467.61MB
Epoch [0/10]; Iter [320/5650]; loss 4.6152; gen_loss 4.6152; LR 3.27e-05; Iter time 0.26s; ETA 0:23:17; Mem 14467.61MB
Epoch [0/10]; Iter [330/5650]; loss 4.5277; gen_loss 4.5277; LR 3.37e-05; Iter time 0.26s; ETA 0:23:15; Mem 14467.61MB
Epoch [0/10]; Iter [340/5650]; loss 4.3445; gen_loss 4.3445; LR 3.47e-05; Iter time 0.26s; ETA 0:23:14; Mem 14467.61MB
Epoch [0/10]; Iter [350/5650]; loss 4.5443; gen_loss 4.5443; LR 3.56e-05; Iter time 0.26s; ETA 0:23:10; Mem 14467.61MB
Epoch [0/10]; Iter [360/5650]; loss 4.5401; gen_loss 4.5401; LR 3.66e-05; Iter time 0.26s; ETA 0:23:05; Mem 14467.61MB
Epoch [0/10]; Iter [370/5650]; loss 4.4643; gen_loss 4.4643; LR 3.76e-05; Iter time 0.26s; ETA 0:23:04; Mem 14467.61MB
Epoch [0/10]; Iter [380/5650]; loss 4.5108; gen_loss 4.5108; LR 3.86e-05; Iter time 0.26s; ETA 0:23:01; Mem 14467.61MB
Epoch [0/10]; Iter [390/5650]; loss 4.4216; gen_loss 4.4216; LR 3.96e-05; Iter time 0.26s; ETA 0:22:59; Mem 14467.61MB
Epoch [0/10]; Iter [400/5650]; loss 4.6270; gen_loss 4.6270; LR 4.06e-05; Iter time 0.26s; ETA 0:22:55; Mem 14467.61MB
Epoch [0/10]; Iter [410/5650]; loss 4.4603; gen_loss 4.4603; LR 4.16e-05; Iter time 0.26s; ETA 0:22:54; Mem 14467.61MB
Epoch [0/10]; Iter [420/5650]; loss 4.4648; gen_loss 4.4648; LR 4.26e-05; Iter time 0.26s; ETA 0:22:51; Mem 14467.61MB
Epoch [0/10]; Iter [430/5650]; loss 4.5191; gen_loss 4.5191; LR 4.36e-05; Iter time 0.26s; ETA 0:22:48; Mem 14467.61MB
Epoch [0/10]; Iter [440/5650]; loss 4.4359; gen_loss 4.4359; LR 4.46e-05; Iter time 0.26s; ETA 0:22:46; Mem 14467.61MB
Epoch [0/10]; Iter [450/5650]; loss 4.4491; gen_loss 4.4491; LR 4.56e-05; Iter time 0.26s; ETA 0:22:42; Mem 14467.61MB
Epoch [0/10]; Iter [460/5650]; loss 4.4468; gen_loss 4.4468; LR 4.65e-05; Iter time 0.26s; ETA 0:22:40; Mem 14467.61MB
Epoch [0/10]; Iter [470/5650]; loss 4.4839; gen_loss 4.4839; LR 4.75e-05; Iter time 0.26s; ETA 0:22:38; Mem 14467.61MB
Epoch [0/10]; Iter [480/5650]; loss 4.4977; gen_loss 4.4977; LR 4.85e-05; Iter time 0.26s; ETA 0:22:36; Mem 14467.61MB
Epoch [0/10]; Iter [490/5650]; loss 4.3821; gen_loss 4.3821; LR 4.95e-05; Iter time 0.26s; ETA 0:22:33; Mem 14467.61MB
Epoch [0/10]; Iter [500/5650]; loss 4.4409; gen_loss 4.4409; LR 5.05e-05; Iter time 0.26s; ETA 0:22:31; Mem 14467.61MB
Epoch [0/10]; Iter [510/5650]; loss 4.3755; gen_loss 4.3755; LR 5.15e-05; Iter time 0.26s; ETA 0:22:28; Mem 14467.61MB
Epoch [0/10]; Iter [520/5650]; loss 4.3028; gen_loss 4.3028; LR 5.25e-05; Iter time 0.26s; ETA 0:22:25; Mem 14467.61MB
Epoch [0/10]; Iter [530/5650]; loss 4.4701; gen_loss 4.4701; LR 5.35e-05; Iter time 0.26s; ETA 0:22:21; Mem 14467.61MB
Epoch [0/10]; Iter [540/5650]; loss 4.5679; gen_loss 4.5679; LR 5.45e-05; Iter time 0.26s; ETA 0:22:19; Mem 14467.61MB
Epoch [0/10]; Iter [550/5650]; loss 4.4569; gen_loss 4.4569; LR 5.55e-05; Iter time 0.26s; ETA 0:22:17; Mem 14467.61MB
Epoch [0/10]; Iter [560/5650]; loss 4.6279; gen_loss 4.6279; LR 5.64e-05; Iter time 0.26s; ETA 0:22:14; Mem 14467.61MB
Epoch [1/10]; Iter [570/5650]; loss 4.2712; gen_loss 4.2712; LR 5.74e-05; Iter time 0.26s; ETA 0:22:11; Mem 14467.61MB
Epoch [1/10]; Iter [580/5650]; loss 4.5627; gen_loss 4.5627; LR 5.84e-05; Iter time 0.26s; ETA 0:22:10; Mem 14467.61MB
Epoch [1/10]; Iter [590/5650]; loss 4.4524; gen_loss 4.4524; LR 5.94e-05; Iter time 0.26s; ETA 0:22:07; Mem 14467.61MB
Epoch [1/10]; Iter [600/5650]; loss 4.3118; gen_loss 4.3118; LR 6.04e-05; Iter time 0.26s; ETA 0:22:03; Mem 14467.61MB
Epoch [1/10]; Iter [610/5650]; loss 4.2676; gen_loss 4.2676; LR 6.14e-05; Iter time 0.26s; ETA 0:22:00; Mem 14467.61MB
Epoch [1/10]; Iter [620/5650]; loss 4.3621; gen_loss 4.3621; LR 6.24e-05; Iter time 0.26s; ETA 0:21:59; Mem 14467.61MB
Epoch [1/10]; Iter [630/5650]; loss 4.3565; gen_loss 4.3565; LR 6.34e-05; Iter time 0.26s; ETA 0:21:56; Mem 14467.61MB
Epoch [1/10]; Iter [640/5650]; loss 4.2224; gen_loss 4.2224; LR 6.44e-05; Iter time 0.26s; ETA 0:21:53; Mem 14467.61MB
Epoch [1/10]; Iter [650/5650]; loss 4.3650; gen_loss 4.3650; LR 6.54e-05; Iter time 0.26s; ETA 0:21:51; Mem 14467.61MB
Epoch [1/10]; Iter [660/5650]; loss 4.4116; gen_loss 4.4116; LR 6.63e-05; Iter time 0.26s; ETA 0:21:49; Mem 14467.61MB
Epoch [1/10]; Iter [670/5650]; loss 4.4004; gen_loss 4.4004; LR 6.73e-05; Iter time 0.26s; ETA 0:21:44; Mem 14467.61MB
Epoch [1/10]; Iter [680/5650]; loss 4.3117; gen_loss 4.3117; LR 6.83e-05; Iter time 0.26s; ETA 0:21:44; Mem 14467.61MB
Epoch [1/10]; Iter [690/5650]; loss 4.2860; gen_loss 4.2860; LR 6.93e-05; Iter time 0.26s; ETA 0:21:40; Mem 14467.61MB
Epoch [1/10]; Iter [700/5650]; loss 4.4371; gen_loss 4.4371; LR 7.03e-05; Iter time 0.26s; ETA 0:21:38; Mem 14467.61MB
Epoch [1/10]; Iter [710/5650]; loss 4.2935; gen_loss 4.2935; LR 7.13e-05; Iter time 0.26s; ETA 0:21:35; Mem 14467.61MB
Epoch [1/10]; Iter [720/5650]; loss 4.3600; gen_loss 4.3600; LR 7.23e-05; Iter time 0.26s; ETA 0:21:33; Mem 14467.61MB
Epoch [1/10]; Iter [730/5650]; loss 4.5213; gen_loss 4.5213; LR 7.33e-05; Iter time 0.26s; ETA 0:21:30; Mem 14467.61MB
Epoch [1/10]; Iter [740/5650]; loss 4.4170; gen_loss 4.4170; LR 7.43e-05; Iter time 0.26s; ETA 0:21:28; Mem 14467.61MB
Epoch [1/10]; Iter [750/5650]; loss 4.3112; gen_loss 4.3112; LR 7.52e-05; Iter time 0.26s; ETA 0:21:23; Mem 14467.61MB
Epoch [1/10]; Iter [760/5650]; loss 4.4536; gen_loss 4.4536; LR 7.62e-05; Iter time 0.26s; ETA 0:21:22; Mem 14467.61MB
Epoch [1/10]; Iter [770/5650]; loss 4.3083; gen_loss 4.3083; LR 7.72e-05; Iter time 0.26s; ETA 0:21:19; Mem 14467.61MB
Epoch [1/10]; Iter [780/5650]; loss 4.4421; gen_loss 4.4421; LR 7.82e-05; Iter time 0.26s; ETA 0:21:16; Mem 14467.61MB
Epoch [1/10]; Iter [790/5650]; loss 4.5098; gen_loss 4.5098; LR 7.92e-05; Iter time 0.26s; ETA 0:21:15; Mem 14467.61MB
Epoch [1/10]; Iter [800/5650]; loss 4.4755; gen_loss 4.4755; LR 8.02e-05; Iter time 0.26s; ETA 0:21:11; Mem 14467.61MB
Epoch [1/10]; Iter [810/5650]; loss 4.4066; gen_loss 4.4066; LR 8.12e-05; Iter time 0.26s; ETA 0:21:10; Mem 14467.61MB
Epoch [1/10]; Iter [820/5650]; loss 4.3725; gen_loss 4.3725; LR 8.22e-05; Iter time 0.26s; ETA 0:21:08; Mem 14467.61MB
Epoch [1/10]; Iter [830/5650]; loss 4.4235; gen_loss 4.4235; LR 8.32e-05; Iter time 0.26s; ETA 0:21:03; Mem 14467.61MB
Epoch [1/10]; Iter [840/5650]; loss 4.3213; gen_loss 4.3213; LR 8.42e-05; Iter time 0.26s; ETA 0:21:01; Mem 14467.61MB
Epoch [1/10]; Iter [850/5650]; loss 4.2048; gen_loss 4.2048; LR 8.51e-05; Iter time 0.26s; ETA 0:20:57; Mem 14467.61MB
Epoch [1/10]; Iter [860/5650]; loss 4.4304; gen_loss 4.4304; LR 8.61e-05; Iter time 0.26s; ETA 0:20:56; Mem 14467.61MB
Epoch [1/10]; Iter [870/5650]; loss 4.3460; gen_loss 4.3460; LR 8.71e-05; Iter time 0.26s; ETA 0:20:52; Mem 14467.61MB
Epoch [1/10]; Iter [880/5650]; loss 4.4478; gen_loss 4.4478; LR 8.81e-05; Iter time 0.26s; ETA 0:20:50; Mem 14467.61MB
Epoch [1/10]; Iter [890/5650]; loss 4.4070; gen_loss 4.4070; LR 8.91e-05; Iter time 0.26s; ETA 0:20:48; Mem 14467.61MB
Epoch [1/10]; Iter [900/5650]; loss 4.4240; gen_loss 4.4240; LR 9.01e-05; Iter time 0.26s; ETA 0:20:45; Mem 14467.61MB
Epoch [1/10]; Iter [910/5650]; loss 4.3654; gen_loss 4.3654; LR 9.11e-05; Iter time 0.26s; ETA 0:20:44; Mem 14467.61MB
Epoch [1/10]; Iter [920/5650]; loss 4.0632; gen_loss 4.0632; LR 9.21e-05; Iter time 0.26s; ETA 0:20:40; Mem 14467.61MB
Epoch [1/10]; Iter [930/5650]; loss 4.2979; gen_loss 4.2979; LR 9.31e-05; Iter time 0.26s; ETA 0:20:38; Mem 14467.61MB
Epoch [1/10]; Iter [940/5650]; loss 4.2934; gen_loss 4.2934; LR 9.41e-05; Iter time 0.26s; ETA 0:20:35; Mem 14467.61MB
Epoch [1/10]; Iter [950/5650]; loss 4.3043; gen_loss 4.3043; LR 9.51e-05; Iter time 0.26s; ETA 0:20:33; Mem 14467.61MB
Epoch [1/10]; Iter [960/5650]; loss 4.2933; gen_loss 4.2933; LR 9.60e-05; Iter time 0.26s; ETA 0:20:29; Mem 14467.61MB
Epoch [1/10]; Iter [970/5650]; loss 4.4264; gen_loss 4.4264; LR 9.70e-05; Iter time 0.26s; ETA 0:20:28; Mem 14467.61MB
Epoch [1/10]; Iter [980/5650]; loss 4.4439; gen_loss 4.4439; LR 9.80e-05; Iter time 0.26s; ETA 0:20:26; Mem 14467.61MB
Epoch [1/10]; Iter [990/5650]; loss 4.3922; gen_loss 4.3922; LR 9.90e-05; Iter time 0.26s; ETA 0:20:22; Mem 14467.61MB
Epoch [1/10]; Iter [1000/5650]; loss 4.2786; gen_loss 4.2786; LR 1.00e-04; Iter time 0.26s; ETA 0:20:21; Mem 14467.61MB
Epoch [1/10]; Iter [1010/5650]; loss 4.1610; gen_loss 4.1610; LR 9.24e-05; Iter time 0.26s; ETA 0:20:17; Mem 14467.61MB
Epoch [1/10]; Iter [1020/5650]; loss 4.3700; gen_loss 4.3700; LR 9.22e-05; Iter time 0.26s; ETA 0:20:14; Mem 14467.61MB
Epoch [1/10]; Iter [1030/5650]; loss 4.3034; gen_loss 4.3034; LR 9.21e-05; Iter time 0.26s; ETA 0:20:13; Mem 14467.61MB
Epoch [1/10]; Iter [1040/5650]; loss 4.2365; gen_loss 4.2365; LR 9.20e-05; Iter time 0.26s; ETA 0:20:08; Mem 14467.61MB
Epoch [1/10]; Iter [1050/5650]; loss 4.0258; gen_loss 4.0258; LR 9.18e-05; Iter time 0.26s; ETA 0:20:07; Mem 14467.61MB
Epoch [1/10]; Iter [1060/5650]; loss 4.4539; gen_loss 4.4539; LR 9.16e-05; Iter time 0.26s; ETA 0:20:03; Mem 14467.61MB
Epoch [1/10]; Iter [1070/5650]; loss 4.3960; gen_loss 4.3960; LR 9.15e-05; Iter time 0.26s; ETA 0:19:59; Mem 14467.61MB
Epoch [1/10]; Iter [1080/5650]; loss 4.3580; gen_loss 4.3580; LR 9.13e-05; Iter time 0.26s; ETA 0:19:57; Mem 14467.61MB
Epoch [1/10]; Iter [1090/5650]; loss 4.2481; gen_loss 4.2481; LR 9.12e-05; Iter time 0.26s; ETA 0:19:57; Mem 14467.61MB
Epoch [1/10]; Iter [1100/5650]; loss 4.1365; gen_loss 4.1365; LR 9.10e-05; Iter time 0.26s; ETA 0:19:52; Mem 14467.61MB
Epoch [1/10]; Iter [1110/5650]; loss 4.2863; gen_loss 4.2863; LR 9.09e-05; Iter time 0.26s; ETA 0:19:50; Mem 14467.61MB
Epoch [1/10]; Iter [1120/5650]; loss 4.3726; gen_loss 4.3726; LR 9.07e-05; Iter time 0.26s; ETA 0:19:48; Mem 14467.61MB
Epoch [2/10]; Iter [1130/5650]; loss 4.0144; gen_loss 4.0144; LR 9.05e-05; Iter time 0.26s; ETA 0:19:45; Mem 14467.61MB
Epoch [2/10]; Iter [1140/5650]; loss 4.4446; gen_loss 4.4446; LR 9.04e-05; Iter time 0.26s; ETA 0:19:42; Mem 14467.61MB
Epoch [2/10]; Iter [1150/5650]; loss 4.2268; gen_loss 4.2268; LR 9.02e-05; Iter time 0.26s; ETA 0:19:40; Mem 14467.61MB
Epoch [2/10]; Iter [1160/5650]; loss 4.2784; gen_loss 4.2784; LR 9.01e-05; Iter time 0.26s; ETA 0:19:37; Mem 14467.61MB
Epoch [2/10]; Iter [1170/5650]; loss 4.2405; gen_loss 4.2405; LR 8.99e-05; Iter time 0.26s; ETA 0:19:36; Mem 14467.61MB
Epoch [2/10]; Iter [1180/5650]; loss 4.2030; gen_loss 4.2030; LR 8.97e-05; Iter time 0.26s; ETA 0:19:33; Mem 14467.61MB
Epoch [2/10]; Iter [1190/5650]; loss 4.3195; gen_loss 4.3195; LR 8.96e-05; Iter time 0.26s; ETA 0:19:30; Mem 14467.61MB
Epoch [2/10]; Iter [1200/5650]; loss 4.3618; gen_loss 4.3618; LR 8.94e-05; Iter time 0.26s; ETA 0:19:26; Mem 14467.61MB
Epoch [2/10]; Iter [1210/5650]; loss 4.2496; gen_loss 4.2496; LR 8.92e-05; Iter time 0.26s; ETA 0:19:24; Mem 14467.61MB
Epoch [2/10]; Iter [1220/5650]; loss 4.0608; gen_loss 4.0608; LR 8.90e-05; Iter time 0.26s; ETA 0:19:21; Mem 14467.61MB
Epoch [2/10]; Iter [1230/5650]; loss 4.3076; gen_loss 4.3076; LR 8.89e-05; Iter time 0.26s; ETA 0:19:18; Mem 14467.61MB
Epoch [2/10]; Iter [1240/5650]; loss 4.3435; gen_loss 4.3435; LR 8.87e-05; Iter time 0.26s; ETA 0:19:17; Mem 14467.61MB
Epoch [2/10]; Iter [1250/5650]; loss 4.1740; gen_loss 4.1740; LR 8.85e-05; Iter time 0.26s; ETA 0:19:14; Mem 14467.61MB
Epoch [2/10]; Iter [1260/5650]; loss 4.1721; gen_loss 4.1721; LR 8.83e-05; Iter time 0.26s; ETA 0:19:11; Mem 14467.61MB
Epoch [2/10]; Iter [1270/5650]; loss 4.3216; gen_loss 4.3216; LR 8.82e-05; Iter time 0.26s; ETA 0:19:08; Mem 14467.61MB
Epoch [2/10]; Iter [1280/5650]; loss 4.4014; gen_loss 4.4014; LR 8.80e-05; Iter time 0.26s; ETA 0:19:06; Mem 14467.61MB
Epoch [2/10]; Iter [1290/5650]; loss 4.3147; gen_loss 4.3147; LR 8.78e-05; Iter time 0.26s; ETA 0:19:03; Mem 14467.61MB
Epoch [2/10]; Iter [1300/5650]; loss 4.1783; gen_loss 4.1783; LR 8.76e-05; Iter time 0.26s; ETA 0:19:00; Mem 14467.61MB
Epoch [2/10]; Iter [1310/5650]; loss 4.4383; gen_loss 4.4383; LR 8.74e-05; Iter time 0.26s; ETA 0:18:56; Mem 14467.61MB
Epoch [2/10]; Iter [1320/5650]; loss 4.1733; gen_loss 4.1733; LR 8.73e-05; Iter time 0.26s; ETA 0:18:55; Mem 14467.61MB
Epoch [2/10]; Iter [1330/5650]; loss 4.0062; gen_loss 4.0062; LR 8.71e-05; Iter time 0.26s; ETA 0:18:52; Mem 14467.61MB
Epoch [2/10]; Iter [1340/5650]; loss 4.0739; gen_loss 4.0739; LR 8.69e-05; Iter time 0.26s; ETA 0:18:50; Mem 14467.61MB
Epoch [2/10]; Iter [1350/5650]; loss 4.0427; gen_loss 4.0427; LR 8.67e-05; Iter time 0.26s; ETA 0:18:47; Mem 14467.61MB
Epoch [2/10]; Iter [1360/5650]; loss 4.1972; gen_loss 4.1972; LR 8.65e-05; Iter time 0.26s; ETA 0:18:44; Mem 14467.61MB
Epoch [2/10]; Iter [1370/5650]; loss 4.2364; gen_loss 4.2364; LR 8.63e-05; Iter time 0.26s; ETA 0:18:42; Mem 14467.61MB
Epoch [2/10]; Iter [1380/5650]; loss 4.3035; gen_loss 4.3035; LR 8.61e-05; Iter time 0.26s; ETA 0:18:39; Mem 14467.61MB
Epoch [2/10]; Iter [1390/5650]; loss 4.2024; gen_loss 4.2024; LR 8.59e-05; Iter time 0.26s; ETA 0:18:37; Mem 14467.61MB
Epoch [2/10]; Iter [1400/5650]; loss 4.2437; gen_loss 4.2437; LR 8.57e-05; Iter time 0.26s; ETA 0:18:33; Mem 14467.61MB
Epoch [2/10]; Iter [1410/5650]; loss 4.0535; gen_loss 4.0535; LR 8.56e-05; Iter time 0.26s; ETA 0:18:31; Mem 14467.61MB
Epoch [2/10]; Iter [1420/5650]; loss 4.2960; gen_loss 4.2960; LR 8.54e-05; Iter time 0.26s; ETA 0:18:29; Mem 14467.61MB
Epoch [2/10]; Iter [1430/5650]; loss 4.0913; gen_loss 4.0913; LR 8.52e-05; Iter time 0.26s; ETA 0:18:25; Mem 14467.61MB
Epoch [2/10]; Iter [1440/5650]; loss 4.2694; gen_loss 4.2694; LR 8.50e-05; Iter time 0.26s; ETA 0:18:23; Mem 14467.61MB
Epoch [2/10]; Iter [1450/5650]; loss 4.3104; gen_loss 4.3104; LR 8.48e-05; Iter time 0.26s; ETA 0:18:21; Mem 14467.61MB
Epoch [2/10]; Iter [1460/5650]; loss 4.0149; gen_loss 4.0149; LR 8.46e-05; Iter time 0.26s; ETA 0:18:17; Mem 14467.61MB
Epoch [2/10]; Iter [1470/5650]; loss 4.0622; gen_loss 4.0622; LR 8.44e-05; Iter time 0.26s; ETA 0:18:15; Mem 14467.61MB
Epoch [2/10]; Iter [1480/5650]; loss 4.3353; gen_loss 4.3353; LR 8.42e-05; Iter time 0.26s; ETA 0:18:14; Mem 14467.61MB
Epoch [2/10]; Iter [1490/5650]; loss 4.4069; gen_loss 4.4069; LR 8.40e-05; Iter time 0.26s; ETA 0:18:10; Mem 14467.61MB
Epoch [2/10]; Iter [1500/5650]; loss 4.3170; gen_loss 4.3170; LR 8.38e-05; Iter time 0.26s; ETA 0:18:06; Mem 14467.61MB
Epoch [2/10]; Iter [1510/5650]; loss 4.2239; gen_loss 4.2239; LR 8.36e-05; Iter time 0.26s; ETA 0:18:05; Mem 14467.61MB
Epoch [2/10]; Iter [1520/5650]; loss 4.1338; gen_loss 4.1338; LR 8.33e-05; Iter time 0.26s; ETA 0:18:02; Mem 14467.61MB
Epoch [2/10]; Iter [1530/5650]; loss 4.0248; gen_loss 4.0248; LR 8.31e-05; Iter time 0.26s; ETA 0:17:59; Mem 14467.61MB
Epoch [2/10]; Iter [1540/5650]; loss 4.0858; gen_loss 4.0858; LR 8.29e-05; Iter time 0.26s; ETA 0:17:57; Mem 14467.61MB
Epoch [2/10]; Iter [1550/5650]; loss 4.1471; gen_loss 4.1471; LR 8.27e-05; Iter time 0.26s; ETA 0:17:56; Mem 14467.61MB
Epoch [2/10]; Iter [1560/5650]; loss 4.2194; gen_loss 4.2194; LR 8.25e-05; Iter time 0.26s; ETA 0:17:52; Mem 14467.61MB
Epoch [2/10]; Iter [1570/5650]; loss 4.0997; gen_loss 4.0997; LR 8.23e-05; Iter time 0.26s; ETA 0:17:49; Mem 14467.61MB
Epoch [2/10]; Iter [1580/5650]; loss 3.9699; gen_loss 3.9699; LR 8.21e-05; Iter time 0.26s; ETA 0:17:46; Mem 14467.61MB
Epoch [2/10]; Iter [1590/5650]; loss 4.0688; gen_loss 4.0688; LR 8.19e-05; Iter time 0.26s; ETA 0:17:44; Mem 14467.61MB
Epoch [2/10]; Iter [1600/5650]; loss 4.1589; gen_loss 4.1589; LR 8.17e-05; Iter time 0.26s; ETA 0:17:42; Mem 14467.61MB
Epoch [2/10]; Iter [1610/5650]; loss 4.2078; gen_loss 4.2078; LR 8.15e-05; Iter time 0.26s; ETA 0:17:40; Mem 14467.61MB
Epoch [2/10]; Iter [1620/5650]; loss 4.0196; gen_loss 4.0196; LR 8.12e-05; Iter time 0.26s; ETA 0:17:37; Mem 14467.61MB
Epoch [2/10]; Iter [1630/5650]; loss 4.1758; gen_loss 4.1758; LR 8.10e-05; Iter time 0.26s; ETA 0:17:34; Mem 14467.61MB
Epoch [2/10]; Iter [1640/5650]; loss 4.1254; gen_loss 4.1254; LR 8.08e-05; Iter time 0.26s; ETA 0:17:33; Mem 14467.61MB
Epoch [2/10]; Iter [1650/5650]; loss 4.1330; gen_loss 4.1330; LR 8.06e-05; Iter time 0.26s; ETA 0:17:29; Mem 14467.61MB
Epoch [2/10]; Iter [1660/5650]; loss 4.0860; gen_loss 4.0860; LR 8.04e-05; Iter time 0.26s; ETA 0:17:25; Mem 14467.61MB
Epoch [2/10]; Iter [1670/5650]; loss 4.2501; gen_loss 4.2501; LR 8.01e-05; Iter time 0.26s; ETA 0:17:24; Mem 14467.61MB
Epoch [2/10]; Iter [1680/5650]; loss 4.2315; gen_loss 4.2315; LR 7.99e-05; Iter time 0.26s; ETA 0:17:22; Mem 14467.61MB
Epoch [2/10]; Iter [1690/5650]; loss 4.1874; gen_loss 4.1874; LR 7.97e-05; Iter time 0.26s; ETA 0:17:18; Mem 14467.61MB
Epoch [3/10]; Iter [1700/5650]; loss 3.8910; gen_loss 3.8910; LR 7.95e-05; Iter time 0.26s; ETA 0:17:13; Mem 14467.61MB
Epoch [3/10]; Iter [1710/5650]; loss 4.2497; gen_loss 4.2497; LR 7.93e-05; Iter time 0.26s; ETA 0:17:13; Mem 14467.61MB
Epoch [3/10]; Iter [1720/5650]; loss 4.0591; gen_loss 4.0591; LR 7.90e-05; Iter time 0.26s; ETA 0:17:09; Mem 14467.61MB
Epoch [3/10]; Iter [1730/5650]; loss 4.2085; gen_loss 4.2085; LR 7.88e-05; Iter time 0.26s; ETA 0:17:08; Mem 14467.61MB
Epoch [3/10]; Iter [1740/5650]; loss 4.1749; gen_loss 4.1749; LR 7.86e-05; Iter time 0.26s; ETA 0:17:05; Mem 14467.61MB
Epoch [3/10]; Iter [1750/5650]; loss 4.0411; gen_loss 4.0411; LR 7.84e-05; Iter time 0.26s; ETA 0:17:03; Mem 14467.61MB
Epoch [3/10]; Iter [1760/5650]; loss 4.2204; gen_loss 4.2204; LR 7.81e-05; Iter time 0.26s; ETA 0:16:59; Mem 14467.61MB
Epoch [3/10]; Iter [1770/5650]; loss 4.2538; gen_loss 4.2538; LR 7.79e-05; Iter time 0.26s; ETA 0:16:57; Mem 14467.61MB
Epoch [3/10]; Iter [1780/5650]; loss 4.0021; gen_loss 4.0021; LR 7.77e-05; Iter time 0.26s; ETA 0:16:55; Mem 14467.61MB
Epoch [3/10]; Iter [1790/5650]; loss 4.0582; gen_loss 4.0582; LR 7.74e-05; Iter time 0.26s; ETA 0:16:51; Mem 14467.61MB
Epoch [3/10]; Iter [1800/5650]; loss 4.0754; gen_loss 4.0754; LR 7.72e-05; Iter time 0.26s; ETA 0:16:50; Mem 14467.61MB
Epoch [3/10]; Iter [1810/5650]; loss 4.0756; gen_loss 4.0756; LR 7.70e-05; Iter time 0.26s; ETA 0:16:46; Mem 14467.61MB
Epoch [3/10]; Iter [1820/5650]; loss 4.1066; gen_loss 4.1066; LR 7.67e-05; Iter time 0.26s; ETA 0:16:43; Mem 14467.61MB
Epoch [3/10]; Iter [1830/5650]; loss 4.2981; gen_loss 4.2981; LR 7.65e-05; Iter time 0.26s; ETA 0:16:41; Mem 14467.61MB
Epoch [3/10]; Iter [1840/5650]; loss 4.0273; gen_loss 4.0273; LR 7.63e-05; Iter time 0.26s; ETA 0:16:38; Mem 14467.61MB
Epoch [3/10]; Iter [1850/5650]; loss 4.0225; gen_loss 4.0225; LR 7.60e-05; Iter time 0.26s; ETA 0:16:37; Mem 14467.61MB
Epoch [3/10]; Iter [1860/5650]; loss 4.0967; gen_loss 4.0967; LR 7.58e-05; Iter time 0.26s; ETA 0:16:34; Mem 14467.61MB
Epoch [3/10]; Iter [1870/5650]; loss 4.2760; gen_loss 4.2760; LR 7.56e-05; Iter time 0.26s; ETA 0:16:29; Mem 14467.61MB
Epoch [3/10]; Iter [1880/5650]; loss 4.0421; gen_loss 4.0421; LR 7.53e-05; Iter time 0.26s; ETA 0:16:27; Mem 14467.61MB
Epoch [3/10]; Iter [1890/5650]; loss 4.1576; gen_loss 4.1576; LR 7.51e-05; Iter time 0.26s; ETA 0:16:26; Mem 14467.61MB
Epoch [3/10]; Iter [1900/5650]; loss 4.0851; gen_loss 4.0851; LR 7.49e-05; Iter time 0.26s; ETA 0:16:23; Mem 14467.61MB
Epoch [3/10]; Iter [1910/5650]; loss 4.2256; gen_loss 4.2256; LR 7.46e-05; Iter time 0.26s; ETA 0:16:21; Mem 14467.61MB
Epoch [3/10]; Iter [1920/5650]; loss 4.0342; gen_loss 4.0342; LR 7.44e-05; Iter time 0.26s; ETA 0:16:16; Mem 14467.61MB
Epoch [3/10]; Iter [1930/5650]; loss 4.2622; gen_loss 4.2622; LR 7.41e-05; Iter time 0.26s; ETA 0:16:15; Mem 14467.61MB
Epoch [3/10]; Iter [1940/5650]; loss 4.3001; gen_loss 4.3001; LR 7.39e-05; Iter time 0.26s; ETA 0:16:12; Mem 14467.61MB
Epoch [3/10]; Iter [1950/5650]; loss 3.9145; gen_loss 3.9145; LR 7.36e-05; Iter time 0.26s; ETA 0:16:10; Mem 14467.61MB
Epoch [3/10]; Iter [1960/5650]; loss 4.1268; gen_loss 4.1268; LR 7.34e-05; Iter time 0.26s; ETA 0:16:07; Mem 14467.61MB
Epoch [3/10]; Iter [1970/5650]; loss 4.2224; gen_loss 4.2224; LR 7.32e-05; Iter time 0.26s; ETA 0:16:05; Mem 14467.61MB
Epoch [3/10]; Iter [1980/5650]; loss 4.1224; gen_loss 4.1224; LR 7.29e-05; Iter time 0.26s; ETA 0:16:02; Mem 14467.61MB
Epoch [3/10]; Iter [1990/5650]; loss 4.0474; gen_loss 4.0474; LR 7.27e-05; Iter time 0.26s; ETA 0:15:59; Mem 14467.61MB
Epoch [3/10]; Iter [2000/5650]; loss 4.2036; gen_loss 4.2036; LR 7.24e-05; Iter time 0.26s; ETA 0:15:58; Mem 14467.61MB
Epoch [3/10]; Iter [2010/5650]; loss 4.2082; gen_loss 4.2082; LR 7.22e-05; Iter time 0.26s; ETA 0:15:55; Mem 14467.61MB
Epoch [3/10]; Iter [2020/5650]; loss 4.0193; gen_loss 4.0193; LR 7.19e-05; Iter time 0.26s; ETA 0:15:53; Mem 14467.61MB
Epoch [3/10]; Iter [2030/5650]; loss 4.2695; gen_loss 4.2695; LR 7.17e-05; Iter time 0.26s; ETA 0:15:50; Mem 14467.61MB
Epoch [3/10]; Iter [2040/5650]; loss 4.1631; gen_loss 4.1631; LR 7.14e-05; Iter time 0.26s; ETA 0:15:47; Mem 14467.61MB
Epoch [3/10]; Iter [2050/5650]; loss 4.1656; gen_loss 4.1656; LR 7.12e-05; Iter time 0.26s; ETA 0:15:45; Mem 14467.61MB
Epoch [3/10]; Iter [2060/5650]; loss 4.0771; gen_loss 4.0771; LR 7.09e-05; Iter time 0.26s; ETA 0:15:42; Mem 14467.61MB
Epoch [3/10]; Iter [2070/5650]; loss 4.0183; gen_loss 4.0183; LR 7.07e-05; Iter time 0.26s; ETA 0:15:40; Mem 14467.61MB
Epoch [3/10]; Iter [2080/5650]; loss 3.9729; gen_loss 3.9729; LR 7.04e-05; Iter time 0.26s; ETA 0:15:37; Mem 14467.61MB
Epoch [3/10]; Iter [2090/5650]; loss 4.3412; gen_loss 4.3412; LR 7.02e-05; Iter time 0.26s; ETA 0:15:34; Mem 14467.61MB
Epoch [3/10]; Iter [2100/5650]; loss 4.2170; gen_loss 4.2170; LR 6.99e-05; Iter time 0.26s; ETA 0:15:31; Mem 14467.61MB
Epoch [3/10]; Iter [2110/5650]; loss 4.0886; gen_loss 4.0886; LR 6.97e-05; Iter time 0.26s; ETA 0:15:29; Mem 14467.61MB
Epoch [3/10]; Iter [2120/5650]; loss 4.0890; gen_loss 4.0890; LR 6.94e-05; Iter time 0.26s; ETA 0:15:26; Mem 14467.61MB
Epoch [3/10]; Iter [2130/5650]; loss 4.1890; gen_loss 4.1890; LR 6.92e-05; Iter time 0.26s; ETA 0:15:23; Mem 14467.61MB
Epoch [3/10]; Iter [2140/5650]; loss 4.0345; gen_loss 4.0345; LR 6.89e-05; Iter time 0.26s; ETA 0:15:21; Mem 14467.61MB
Epoch [3/10]; Iter [2150/5650]; loss 4.0879; gen_loss 4.0879; LR 6.86e-05; Iter time 0.26s; ETA 0:15:19; Mem 14467.61MB
Epoch [3/10]; Iter [2160/5650]; loss 4.0363; gen_loss 4.0363; LR 6.84e-05; Iter time 0.26s; ETA 0:15:15; Mem 14467.61MB
Epoch [3/10]; Iter [2170/5650]; loss 4.0308; gen_loss 4.0308; LR 6.81e-05; Iter time 0.26s; ETA 0:15:13; Mem 14467.61MB
Epoch [3/10]; Iter [2180/5650]; loss 4.0672; gen_loss 4.0672; LR 6.79e-05; Iter time 0.26s; ETA 0:15:11; Mem 14467.61MB
Epoch [3/10]; Iter [2190/5650]; loss 4.0712; gen_loss 4.0712; LR 6.76e-05; Iter time 0.26s; ETA 0:15:08; Mem 14467.61MB
Epoch [3/10]; Iter [2200/5650]; loss 3.8755; gen_loss 3.8755; LR 6.74e-05; Iter time 0.26s; ETA 0:15:06; Mem 14467.61MB
Epoch [3/10]; Iter [2210/5650]; loss 3.9237; gen_loss 3.9237; LR 6.71e-05; Iter time 0.26s; ETA 0:15:02; Mem 14467.61MB
Epoch [3/10]; Iter [2220/5650]; loss 4.2220; gen_loss 4.2220; LR 6.68e-05; Iter time 0.26s; ETA 0:15:00; Mem 14467.61MB
Epoch [3/10]; Iter [2230/5650]; loss 4.1027; gen_loss 4.1027; LR 6.66e-05; Iter time 0.26s; ETA 0:14:57; Mem 14467.61MB
Epoch [3/10]; Iter [2240/5650]; loss 3.8657; gen_loss 3.8657; LR 6.63e-05; Iter time 0.26s; ETA 0:14:55; Mem 14467.61MB
Epoch [3/10]; Iter [2250/5650]; loss 3.9213; gen_loss 3.9213; LR 6.61e-05; Iter time 0.26s; ETA 0:14:52; Mem 14467.61MB
Epoch [4/10]; Iter [2260/5650]; loss 4.0654; gen_loss 4.0654; LR 6.58e-05; Iter time 0.26s; ETA 0:14:50; Mem 14467.61MB
Epoch [4/10]; Iter [2270/5650]; loss 3.8840; gen_loss 3.8840; LR 6.55e-05; Iter time 0.26s; ETA 0:14:47; Mem 14467.61MB
Epoch [4/10]; Iter [2280/5650]; loss 3.9753; gen_loss 3.9753; LR 6.53e-05; Iter time 0.26s; ETA 0:14:44; Mem 14467.61MB
Epoch [4/10]; Iter [2290/5650]; loss 3.9774; gen_loss 3.9774; LR 6.50e-05; Iter time 0.26s; ETA 0:14:42; Mem 14467.61MB
Epoch [4/10]; Iter [2300/5650]; loss 4.1719; gen_loss 4.1719; LR 6.47e-05; Iter time 0.26s; ETA 0:14:39; Mem 14467.61MB
Epoch [4/10]; Iter [2310/5650]; loss 4.1026; gen_loss 4.1026; LR 6.45e-05; Iter time 0.26s; ETA 0:14:36; Mem 14467.61MB
Epoch [4/10]; Iter [2320/5650]; loss 4.0473; gen_loss 4.0473; LR 6.42e-05; Iter time 0.26s; ETA 0:14:34; Mem 14467.61MB
Epoch [4/10]; Iter [2330/5650]; loss 4.0778; gen_loss 4.0778; LR 6.40e-05; Iter time 0.26s; ETA 0:14:32; Mem 14467.61MB
Epoch [4/10]; Iter [2340/5650]; loss 3.9225; gen_loss 3.9225; LR 6.37e-05; Iter time 0.26s; ETA 0:14:29; Mem 14467.61MB
Epoch [4/10]; Iter [2350/5650]; loss 4.1192; gen_loss 4.1192; LR 6.34e-05; Iter time 0.26s; ETA 0:14:27; Mem 14467.61MB
Epoch [4/10]; Iter [2360/5650]; loss 4.0232; gen_loss 4.0232; LR 6.32e-05; Iter time 0.26s; ETA 0:14:24; Mem 14467.61MB
Epoch [4/10]; Iter [2370/5650]; loss 4.0743; gen_loss 4.0743; LR 6.29e-05; Iter time 0.26s; ETA 0:14:20; Mem 14467.61MB
Epoch [4/10]; Iter [2380/5650]; loss 4.1075; gen_loss 4.1075; LR 6.26e-05; Iter time 0.26s; ETA 0:14:18; Mem 14467.61MB
Epoch [4/10]; Iter [2390/5650]; loss 4.0866; gen_loss 4.0866; LR 6.24e-05; Iter time 0.26s; ETA 0:14:15; Mem 14467.61MB
Epoch [4/10]; Iter [2400/5650]; loss 4.0647; gen_loss 4.0647; LR 6.21e-05; Iter time 0.26s; ETA 0:14:13; Mem 14467.61MB
Epoch [4/10]; Iter [2410/5650]; loss 3.8562; gen_loss 3.8562; LR 6.18e-05; Iter time 0.26s; ETA 0:14:11; Mem 14467.61MB
Epoch [4/10]; Iter [2420/5650]; loss 4.2365; gen_loss 4.2365; LR 6.16e-05; Iter time 0.26s; ETA 0:14:07; Mem 14467.61MB
Epoch [4/10]; Iter [2430/5650]; loss 3.8454; gen_loss 3.8454; LR 6.13e-05; Iter time 0.26s; ETA 0:14:04; Mem 14467.61MB
Epoch [4/10]; Iter [2440/5650]; loss 4.0005; gen_loss 4.0005; LR 6.10e-05; Iter time 0.26s; ETA 0:14:02; Mem 14467.61MB
Epoch [4/10]; Iter [2450/5650]; loss 4.0818; gen_loss 4.0818; LR 6.07e-05; Iter time 0.26s; ETA 0:14:00; Mem 14467.61MB
Epoch [4/10]; Iter [2460/5650]; loss 3.8191; gen_loss 3.8191; LR 6.05e-05; Iter time 0.26s; ETA 0:13:58; Mem 14467.61MB
Epoch [4/10]; Iter [2470/5650]; loss 4.0396; gen_loss 4.0396; LR 6.02e-05; Iter time 0.26s; ETA 0:13:55; Mem 14467.61MB
Epoch [4/10]; Iter [2480/5650]; loss 4.1265; gen_loss 4.1265; LR 5.99e-05; Iter time 0.26s; ETA 0:13:51; Mem 14467.61MB
Epoch [4/10]; Iter [2490/5650]; loss 3.8973; gen_loss 3.8973; LR 5.97e-05; Iter time 0.26s; ETA 0:13:49; Mem 14467.61MB
Epoch [4/10]; Iter [2500/5650]; loss 3.9540; gen_loss 3.9540; LR 5.94e-05; Iter time 0.26s; ETA 0:13:47; Mem 14467.61MB
Epoch [4/10]; Iter [2510/5650]; loss 4.0168; gen_loss 4.0168; LR 5.91e-05; Iter time 0.26s; ETA 0:13:44; Mem 14467.61MB
Epoch [4/10]; Iter [2520/5650]; loss 4.0614; gen_loss 4.0614; LR 5.89e-05; Iter time 0.26s; ETA 0:13:41; Mem 14467.61MB
Epoch [4/10]; Iter [2530/5650]; loss 4.1128; gen_loss 4.1128; LR 5.86e-05; Iter time 0.26s; ETA 0:13:39; Mem 14467.61MB
Epoch [4/10]; Iter [2540/5650]; loss 3.9520; gen_loss 3.9520; LR 5.83e-05; Iter time 0.26s; ETA 0:13:36; Mem 14467.61MB
Epoch [4/10]; Iter [2550/5650]; loss 4.0775; gen_loss 4.0775; LR 5.80e-05; Iter time 0.26s; ETA 0:13:34; Mem 14467.61MB
Epoch [4/10]; Iter [2560/5650]; loss 4.0620; gen_loss 4.0620; LR 5.78e-05; Iter time 0.26s; ETA 0:13:32; Mem 14467.61MB
Epoch [4/10]; Iter [2570/5650]; loss 4.1331; gen_loss 4.1331; LR 5.75e-05; Iter time 0.26s; ETA 0:13:28; Mem 14467.61MB
Epoch [4/10]; Iter [2580/5650]; loss 4.1148; gen_loss 4.1148; LR 5.72e-05; Iter time 0.26s; ETA 0:13:24; Mem 14467.61MB
Epoch [4/10]; Iter [2590/5650]; loss 4.0632; gen_loss 4.0632; LR 5.69e-05; Iter time 0.26s; ETA 0:13:24; Mem 14467.61MB
Epoch [4/10]; Iter [2600/5650]; loss 3.9668; gen_loss 3.9668; LR 5.67e-05; Iter time 0.26s; ETA 0:13:21; Mem 14467.61MB
Epoch [4/10]; Iter [2610/5650]; loss 4.1629; gen_loss 4.1629; LR 5.64e-05; Iter time 0.26s; ETA 0:13:17; Mem 14467.61MB
Epoch [4/10]; Iter [2620/5650]; loss 4.0769; gen_loss 4.0769; LR 5.61e-05; Iter time 0.26s; ETA 0:13:17; Mem 14467.61MB
Epoch [4/10]; Iter [2630/5650]; loss 4.1498; gen_loss 4.1498; LR 5.59e-05; Iter time 0.26s; ETA 0:13:12; Mem 14467.61MB
Epoch [4/10]; Iter [2640/5650]; loss 4.1549; gen_loss 4.1549; LR 5.56e-05; Iter time 0.26s; ETA 0:13:10; Mem 14467.61MB
Epoch [4/10]; Iter [2650/5650]; loss 4.0099; gen_loss 4.0099; LR 5.53e-05; Iter time 0.26s; ETA 0:13:07; Mem 14467.61MB
Epoch [4/10]; Iter [2660/5650]; loss 3.9302; gen_loss 3.9302; LR 5.50e-05; Iter time 0.26s; ETA 0:13:05; Mem 14467.61MB
Epoch [4/10]; Iter [2670/5650]; loss 3.8976; gen_loss 3.8976; LR 5.48e-05; Iter time 0.26s; ETA 0:13:01; Mem 14467.61MB
Epoch [4/10]; Iter [2680/5650]; loss 4.1281; gen_loss 4.1281; LR 5.45e-05; Iter time 0.26s; ETA 0:13:00; Mem 14467.61MB
Epoch [4/10]; Iter [2690/5650]; loss 4.0761; gen_loss 4.0761; LR 5.42e-05; Iter time 0.26s; ETA 0:12:56; Mem 14467.61MB
Epoch [4/10]; Iter [2700/5650]; loss 4.1536; gen_loss 4.1536; LR 5.39e-05; Iter time 0.26s; ETA 0:12:55; Mem 14467.61MB
Epoch [4/10]; Iter [2710/5650]; loss 3.8768; gen_loss 3.8768; LR 5.37e-05; Iter time 0.26s; ETA 0:12:52; Mem 14467.61MB
Epoch [4/10]; Iter [2720/5650]; loss 3.8825; gen_loss 3.8825; LR 5.34e-05; Iter time 0.26s; ETA 0:12:49; Mem 14467.61MB
Epoch [4/10]; Iter [2730/5650]; loss 4.0094; gen_loss 4.0094; LR 5.31e-05; Iter time 0.26s; ETA 0:12:46; Mem 14467.61MB
Epoch [4/10]; Iter [2740/5650]; loss 4.2511; gen_loss 4.2511; LR 5.28e-05; Iter time 0.26s; ETA 0:12:44; Mem 14467.61MB
Epoch [4/10]; Iter [2750/5650]; loss 4.0711; gen_loss 4.0711; LR 5.26e-05; Iter time 0.26s; ETA 0:12:41; Mem 14467.61MB
Epoch [4/10]; Iter [2760/5650]; loss 4.0000; gen_loss 4.0000; LR 5.23e-05; Iter time 0.26s; ETA 0:12:38; Mem 14467.61MB
Epoch [4/10]; Iter [2770/5650]; loss 4.0106; gen_loss 4.0106; LR 5.20e-05; Iter time 0.26s; ETA 0:12:35; Mem 14467.61MB
Epoch [4/10]; Iter [2780/5650]; loss 3.9349; gen_loss 3.9349; LR 5.17e-05; Iter time 0.26s; ETA 0:12:32; Mem 14467.61MB
Epoch [4/10]; Iter [2790/5650]; loss 4.1109; gen_loss 4.1109; LR 5.15e-05; Iter time 0.26s; ETA 0:12:31; Mem 14467.61MB
Epoch [4/10]; Iter [2800/5650]; loss 4.1731; gen_loss 4.1731; LR 5.12e-05; Iter time 0.26s; ETA 0:12:27; Mem 14467.61MB
Epoch [4/10]; Iter [2810/5650]; loss 4.2070; gen_loss 4.2070; LR 5.09e-05; Iter time 0.26s; ETA 0:12:25; Mem 14467.61MB
Epoch [4/10]; Iter [2820/5650]; loss 4.0831; gen_loss 4.0831; LR 5.06e-05; Iter time 0.26s; ETA 0:12:22; Mem 14467.61MB
Epoch [5/10]; Iter [2830/5650]; loss 3.9480; gen_loss 3.9480; LR 5.04e-05; Iter time 0.26s; ETA 0:12:18; Mem 14467.61MB
Epoch [5/10]; Iter [2840/5650]; loss 4.0352; gen_loss 4.0352; LR 5.01e-05; Iter time 0.26s; ETA 0:12:17; Mem 14467.61MB
Epoch [5/10]; Iter [2850/5650]; loss 3.9985; gen_loss 3.9985; LR 4.98e-05; Iter time 0.26s; ETA 0:12:15; Mem 14467.61MB
Epoch [5/10]; Iter [2860/5650]; loss 4.0090; gen_loss 4.0090; LR 4.95e-05; Iter time 0.26s; ETA 0:12:12; Mem 14467.61MB
Epoch [5/10]; Iter [2870/5650]; loss 4.1233; gen_loss 4.1233; LR 4.93e-05; Iter time 0.26s; ETA 0:12:09; Mem 14467.61MB
Epoch [5/10]; Iter [2880/5650]; loss 3.9321; gen_loss 3.9321; LR 4.90e-05; Iter time 0.26s; ETA 0:12:06; Mem 14467.61MB
Epoch [5/10]; Iter [2890/5650]; loss 3.7842; gen_loss 3.7842; LR 4.87e-05; Iter time 0.26s; ETA 0:12:03; Mem 14467.61MB
Epoch [5/10]; Iter [2900/5650]; loss 3.9341; gen_loss 3.9341; LR 4.84e-05; Iter time 0.26s; ETA 0:12:01; Mem 14467.61MB
Epoch [5/10]; Iter [2910/5650]; loss 3.8881; gen_loss 3.8881; LR 4.82e-05; Iter time 0.26s; ETA 0:11:59; Mem 14467.61MB
Epoch [5/10]; Iter [2920/5650]; loss 4.1046; gen_loss 4.1046; LR 4.79e-05; Iter time 0.26s; ETA 0:11:56; Mem 14467.61MB
Epoch [5/10]; Iter [2930/5650]; loss 4.0982; gen_loss 4.0982; LR 4.76e-05; Iter time 0.26s; ETA 0:11:53; Mem 14467.61MB
Epoch [5/10]; Iter [2940/5650]; loss 4.1514; gen_loss 4.1514; LR 4.73e-05; Iter time 0.26s; ETA 0:11:51; Mem 14467.61MB
Epoch [5/10]; Iter [2950/5650]; loss 3.8654; gen_loss 3.8654; LR 4.71e-05; Iter time 0.26s; ETA 0:11:49; Mem 14467.61MB
Epoch [5/10]; Iter [2960/5650]; loss 4.0367; gen_loss 4.0367; LR 4.68e-05; Iter time 0.26s; ETA 0:11:45; Mem 14467.61MB
Epoch [5/10]; Iter [2970/5650]; loss 3.9123; gen_loss 3.9123; LR 4.65e-05; Iter time 0.26s; ETA 0:11:42; Mem 14467.61MB
Epoch [5/10]; Iter [2980/5650]; loss 4.0881; gen_loss 4.0881; LR 4.62e-05; Iter time 0.26s; ETA 0:11:41; Mem 14467.61MB
Epoch [5/10]; Iter [2990/5650]; loss 3.9629; gen_loss 3.9629; LR 4.60e-05; Iter time 0.26s; ETA 0:11:38; Mem 14467.61MB
Epoch [5/10]; Iter [3000/5650]; loss 4.1145; gen_loss 4.1145; LR 4.57e-05; Iter time 0.26s; ETA 0:11:36; Mem 14467.61MB
Epoch [5/10]; Iter [3010/5650]; loss 4.0022; gen_loss 4.0022; LR 4.54e-05; Iter time 0.26s; ETA 0:11:32; Mem 14467.61MB
Epoch [5/10]; Iter [3020/5650]; loss 3.9738; gen_loss 3.9738; LR 4.51e-05; Iter time 0.26s; ETA 0:11:30; Mem 14467.61MB
Epoch [5/10]; Iter [3030/5650]; loss 4.1191; gen_loss 4.1191; LR 4.49e-05; Iter time 0.26s; ETA 0:11:26; Mem 14467.61MB
Epoch [5/10]; Iter [3040/5650]; loss 3.9639; gen_loss 3.9639; LR 4.46e-05; Iter time 0.26s; ETA 0:11:25; Mem 14467.61MB
Epoch [5/10]; Iter [3050/5650]; loss 3.7781; gen_loss 3.7781; LR 4.43e-05; Iter time 0.26s; ETA 0:11:21; Mem 14467.61MB
Epoch [5/10]; Iter [3060/5650]; loss 4.2388; gen_loss 4.2388; LR 4.41e-05; Iter time 0.26s; ETA 0:11:20; Mem 14467.61MB
Epoch [5/10]; Iter [3070/5650]; loss 4.0865; gen_loss 4.0865; LR 4.38e-05; Iter time 0.26s; ETA 0:11:17; Mem 14467.61MB
Epoch [5/10]; Iter [3080/5650]; loss 4.1473; gen_loss 4.1473; LR 4.35e-05; Iter time 0.26s; ETA 0:11:15; Mem 14467.61MB
Epoch [5/10]; Iter [3090/5650]; loss 3.8639; gen_loss 3.8639; LR 4.32e-05; Iter time 0.26s; ETA 0:11:12; Mem 14467.61MB
Epoch [5/10]; Iter [3100/5650]; loss 4.0162; gen_loss 4.0162; LR 4.30e-05; Iter time 0.26s; ETA 0:11:09; Mem 14467.61MB
Epoch [5/10]; Iter [3110/5650]; loss 3.9310; gen_loss 3.9310; LR 4.27e-05; Iter time 0.26s; ETA 0:11:06; Mem 14467.61MB
Epoch [5/10]; Iter [3120/5650]; loss 4.0592; gen_loss 4.0592; LR 4.24e-05; Iter time 0.26s; ETA 0:11:04; Mem 14467.61MB
Epoch [5/10]; Iter [3130/5650]; loss 3.9741; gen_loss 3.9741; LR 4.21e-05; Iter time 0.26s; ETA 0:11:01; Mem 14467.61MB
Epoch [5/10]; Iter [3140/5650]; loss 3.9404; gen_loss 3.9404; LR 4.19e-05; Iter time 0.26s; ETA 0:10:59; Mem 14467.61MB
Epoch [5/10]; Iter [3150/5650]; loss 4.3061; gen_loss 4.3061; LR 4.16e-05; Iter time 0.26s; ETA 0:10:55; Mem 14467.61MB
Epoch [5/10]; Iter [3160/5650]; loss 3.9397; gen_loss 3.9397; LR 4.13e-05; Iter time 0.26s; ETA 0:10:53; Mem 14467.61MB
Epoch [5/10]; Iter [3170/5650]; loss 3.9999; gen_loss 3.9999; LR 4.11e-05; Iter time 0.26s; ETA 0:10:51; Mem 14467.61MB
Epoch [5/10]; Iter [3180/5650]; loss 3.7286; gen_loss 3.7286; LR 4.08e-05; Iter time 0.26s; ETA 0:10:48; Mem 14467.61MB
Epoch [5/10]; Iter [3190/5650]; loss 4.1949; gen_loss 4.1949; LR 4.05e-05; Iter time 0.26s; ETA 0:10:45; Mem 14467.61MB
Epoch [5/10]; Iter [3200/5650]; loss 4.0069; gen_loss 4.0069; LR 4.03e-05; Iter time 0.26s; ETA 0:10:43; Mem 14467.61MB
Epoch [5/10]; Iter [3210/5650]; loss 4.0301; gen_loss 4.0301; LR 4.00e-05; Iter time 0.26s; ETA 0:10:41; Mem 14467.61MB
Epoch [5/10]; Iter [3220/5650]; loss 3.9839; gen_loss 3.9839; LR 3.97e-05; Iter time 0.26s; ETA 0:10:37; Mem 14467.61MB
Epoch [5/10]; Iter [3230/5650]; loss 3.9662; gen_loss 3.9662; LR 3.94e-05; Iter time 0.26s; ETA 0:10:34; Mem 14467.61MB
Epoch [5/10]; Iter [3240/5650]; loss 3.8467; gen_loss 3.8467; LR 3.92e-05; Iter time 0.26s; ETA 0:10:32; Mem 14467.61MB
Epoch [5/10]; Iter [3250/5650]; loss 4.0454; gen_loss 4.0454; LR 3.89e-05; Iter time 0.26s; ETA 0:10:30; Mem 14467.61MB
Epoch [5/10]; Iter [3260/5650]; loss 3.8662; gen_loss 3.8662; LR 3.86e-05; Iter time 0.26s; ETA 0:10:27; Mem 14467.61MB
Epoch [5/10]; Iter [3270/5650]; loss 4.0877; gen_loss 4.0877; LR 3.84e-05; Iter time 0.26s; ETA 0:10:25; Mem 14467.61MB
Epoch [5/10]; Iter [3280/5650]; loss 4.0672; gen_loss 4.0672; LR 3.81e-05; Iter time 0.26s; ETA 0:10:22; Mem 14467.61MB
Epoch [5/10]; Iter [3290/5650]; loss 3.8943; gen_loss 3.8943; LR 3.78e-05; Iter time 0.26s; ETA 0:10:20; Mem 14467.61MB
Epoch [5/10]; Iter [3300/5650]; loss 3.9359; gen_loss 3.9359; LR 3.76e-05; Iter time 0.26s; ETA 0:10:18; Mem 14467.61MB
Epoch [5/10]; Iter [3310/5650]; loss 3.8534; gen_loss 3.8534; LR 3.73e-05; Iter time 0.26s; ETA 0:10:14; Mem 14467.61MB
Epoch [5/10]; Iter [3320/5650]; loss 3.8779; gen_loss 3.8779; LR 3.70e-05; Iter time 0.26s; ETA 0:10:12; Mem 14467.61MB
Epoch [5/10]; Iter [3330/5650]; loss 4.0987; gen_loss 4.0987; LR 3.68e-05; Iter time 0.26s; ETA 0:10:09; Mem 14467.61MB
Epoch [5/10]; Iter [3340/5650]; loss 4.0215; gen_loss 4.0215; LR 3.65e-05; Iter time 0.26s; ETA 0:10:06; Mem 14467.61MB
Epoch [5/10]; Iter [3350/5650]; loss 3.7831; gen_loss 3.7831; LR 3.63e-05; Iter time 0.26s; ETA 0:10:04; Mem 14467.61MB
Epoch [5/10]; Iter [3360/5650]; loss 4.3044; gen_loss 4.3044; LR 3.60e-05; Iter time 0.26s; ETA 0:10:01; Mem 14467.61MB
Epoch [5/10]; Iter [3370/5650]; loss 3.8377; gen_loss 3.8377; LR 3.57e-05; Iter time 0.26s; ETA 0:09:57; Mem 14467.61MB
Epoch [5/10]; Iter [3380/5650]; loss 4.0134; gen_loss 4.0134; LR 3.55e-05; Iter time 0.26s; ETA 0:09:55; Mem 14467.61MB
Epoch [6/10]; Iter [3390/5650]; loss 4.0012; gen_loss 4.0012; LR 3.52e-05; Iter time 0.26s; ETA 0:09:52; Mem 14467.61MB
Epoch [6/10]; Iter [3400/5650]; loss 3.8422; gen_loss 3.8422; LR 3.49e-05; Iter time 0.26s; ETA 0:09:50; Mem 14467.61MB
Epoch [6/10]; Iter [3410/5650]; loss 4.1752; gen_loss 4.1752; LR 3.47e-05; Iter time 0.26s; ETA 0:09:48; Mem 14467.61MB
Epoch [6/10]; Iter [3420/5650]; loss 3.8040; gen_loss 3.8040; LR 3.44e-05; Iter time 0.26s; ETA 0:09:45; Mem 14467.61MB
Epoch [6/10]; Iter [3430/5650]; loss 3.9034; gen_loss 3.9034; LR 3.42e-05; Iter time 0.26s; ETA 0:09:42; Mem 14467.61MB
Epoch [6/10]; Iter [3440/5650]; loss 4.0759; gen_loss 4.0759; LR 3.39e-05; Iter time 0.26s; ETA 0:09:40; Mem 14467.61MB
Epoch [6/10]; Iter [3450/5650]; loss 4.0160; gen_loss 4.0160; LR 3.36e-05; Iter time 0.26s; ETA 0:09:36; Mem 14467.61MB
Epoch [6/10]; Iter [3460/5650]; loss 4.1134; gen_loss 4.1134; LR 3.34e-05; Iter time 0.26s; ETA 0:09:34; Mem 14467.61MB
Epoch [6/10]; Iter [3470/5650]; loss 4.1309; gen_loss 4.1309; LR 3.31e-05; Iter time 0.26s; ETA 0:09:32; Mem 14467.61MB
Epoch [6/10]; Iter [3480/5650]; loss 3.8127; gen_loss 3.8127; LR 3.29e-05; Iter time 0.26s; ETA 0:09:29; Mem 14467.61MB
Epoch [6/10]; Iter [3490/5650]; loss 3.8642; gen_loss 3.8642; LR 3.26e-05; Iter time 0.26s; ETA 0:09:26; Mem 14467.61MB
Epoch [6/10]; Iter [3500/5650]; loss 4.1541; gen_loss 4.1541; LR 3.24e-05; Iter time 0.26s; ETA 0:09:23; Mem 14467.61MB
Epoch [6/10]; Iter [3510/5650]; loss 3.8340; gen_loss 3.8340; LR 3.21e-05; Iter time 0.26s; ETA 0:09:21; Mem 14467.61MB
Epoch [6/10]; Iter [3520/5650]; loss 3.9343; gen_loss 3.9343; LR 3.18e-05; Iter time 0.26s; ETA 0:09:18; Mem 14467.61MB
Epoch [6/10]; Iter [3530/5650]; loss 3.8905; gen_loss 3.8905; LR 3.16e-05; Iter time 0.26s; ETA 0:09:16; Mem 14467.61MB
Epoch [6/10]; Iter [3540/5650]; loss 3.9806; gen_loss 3.9806; LR 3.13e-05; Iter time 0.26s; ETA 0:09:13; Mem 14467.61MB
Epoch [6/10]; Iter [3550/5650]; loss 3.7152; gen_loss 3.7152; LR 3.11e-05; Iter time 0.26s; ETA 0:09:11; Mem 14467.61MB
Epoch [6/10]; Iter [3560/5650]; loss 3.8725; gen_loss 3.8725; LR 3.08e-05; Iter time 0.26s; ETA 0:09:08; Mem 14467.61MB
Epoch [6/10]; Iter [3570/5650]; loss 3.9526; gen_loss 3.9526; LR 3.06e-05; Iter time 0.26s; ETA 0:09:05; Mem 14467.61MB
Epoch [6/10]; Iter [3580/5650]; loss 4.0238; gen_loss 4.0238; LR 3.03e-05; Iter time 0.26s; ETA 0:09:03; Mem 14467.61MB
Epoch [6/10]; Iter [3590/5650]; loss 3.9561; gen_loss 3.9561; LR 3.01e-05; Iter time 0.26s; ETA 0:09:00; Mem 14467.61MB
Epoch [6/10]; Iter [3600/5650]; loss 3.7717; gen_loss 3.7717; LR 2.98e-05; Iter time 0.26s; ETA 0:08:58; Mem 14467.61MB
Epoch [6/10]; Iter [3610/5650]; loss 4.1269; gen_loss 4.1269; LR 2.96e-05; Iter time 0.26s; ETA 0:08:55; Mem 14467.61MB
Epoch [6/10]; Iter [3620/5650]; loss 4.1820; gen_loss 4.1820; LR 2.93e-05; Iter time 0.26s; ETA 0:08:53; Mem 14467.61MB
Epoch [6/10]; Iter [3630/5650]; loss 3.9967; gen_loss 3.9967; LR 2.91e-05; Iter time 0.26s; ETA 0:08:50; Mem 14467.61MB
Epoch [6/10]; Iter [3640/5650]; loss 3.8345; gen_loss 3.8345; LR 2.88e-05; Iter time 0.26s; ETA 0:08:47; Mem 14467.61MB
Epoch [6/10]; Iter [3650/5650]; loss 3.8119; gen_loss 3.8119; LR 2.86e-05; Iter time 0.26s; ETA 0:08:45; Mem 14467.61MB
Epoch [6/10]; Iter [3660/5650]; loss 4.1865; gen_loss 4.1865; LR 2.83e-05; Iter time 0.26s; ETA 0:08:42; Mem 14467.61MB
Epoch [6/10]; Iter [3670/5650]; loss 3.9859; gen_loss 3.9859; LR 2.81e-05; Iter time 0.26s; ETA 0:08:39; Mem 14467.61MB
Epoch [6/10]; Iter [3680/5650]; loss 3.7817; gen_loss 3.7817; LR 2.78e-05; Iter time 0.26s; ETA 0:08:36; Mem 14467.61MB
Epoch [6/10]; Iter [3690/5650]; loss 3.9368; gen_loss 3.9368; LR 2.76e-05; Iter time 0.26s; ETA 0:08:34; Mem 14467.61MB
Epoch [6/10]; Iter [3700/5650]; loss 3.9823; gen_loss 3.9823; LR 2.74e-05; Iter time 0.26s; ETA 0:08:31; Mem 14467.61MB
Epoch [6/10]; Iter [3710/5650]; loss 4.0579; gen_loss 4.0579; LR 2.71e-05; Iter time 0.26s; ETA 0:08:29; Mem 14467.61MB
Epoch [6/10]; Iter [3720/5650]; loss 3.8161; gen_loss 3.8161; LR 2.69e-05; Iter time 0.26s; ETA 0:08:26; Mem 14467.61MB
Epoch [6/10]; Iter [3730/5650]; loss 4.0820; gen_loss 4.0820; LR 2.66e-05; Iter time 0.26s; ETA 0:08:24; Mem 14467.61MB
Epoch [6/10]; Iter [3740/5650]; loss 3.8675; gen_loss 3.8675; LR 2.64e-05; Iter time 0.26s; ETA 0:08:21; Mem 14467.61MB
Epoch [6/10]; Iter [3750/5650]; loss 3.9732; gen_loss 3.9732; LR 2.61e-05; Iter time 0.26s; ETA 0:08:19; Mem 14467.61MB
Epoch [6/10]; Iter [3760/5650]; loss 3.8388; gen_loss 3.8388; LR 2.59e-05; Iter time 0.26s; ETA 0:08:16; Mem 14467.61MB
Epoch [6/10]; Iter [3770/5650]; loss 3.9713; gen_loss 3.9713; LR 2.57e-05; Iter time 0.26s; ETA 0:08:13; Mem 14467.61MB
Epoch [6/10]; Iter [3780/5650]; loss 3.9632; gen_loss 3.9632; LR 2.54e-05; Iter time 0.26s; ETA 0:08:11; Mem 14467.61MB
Epoch [6/10]; Iter [3790/5650]; loss 4.0009; gen_loss 4.0009; LR 2.52e-05; Iter time 0.26s; ETA 0:08:07; Mem 14467.61MB
Epoch [6/10]; Iter [3800/5650]; loss 4.1398; gen_loss 4.1398; LR 2.50e-05; Iter time 0.26s; ETA 0:08:06; Mem 14467.61MB
Epoch [6/10]; Iter [3810/5650]; loss 4.1066; gen_loss 4.1066; LR 2.47e-05; Iter time 0.26s; ETA 0:08:02; Mem 14467.61MB
Epoch [6/10]; Iter [3820/5650]; loss 3.8041; gen_loss 3.8041; LR 2.45e-05; Iter time 0.26s; ETA 0:08:00; Mem 14467.61MB
Epoch [6/10]; Iter [3830/5650]; loss 3.9086; gen_loss 3.9086; LR 2.43e-05; Iter time 0.26s; ETA 0:07:57; Mem 14467.61MB
Epoch [6/10]; Iter [3840/5650]; loss 3.9476; gen_loss 3.9476; LR 2.40e-05; Iter time 0.26s; ETA 0:07:55; Mem 14467.61MB
Epoch [6/10]; Iter [3850/5650]; loss 4.0223; gen_loss 4.0223; LR 2.38e-05; Iter time 0.26s; ETA 0:07:52; Mem 14467.61MB
Epoch [6/10]; Iter [3860/5650]; loss 3.9162; gen_loss 3.9162; LR 2.36e-05; Iter time 0.26s; ETA 0:07:50; Mem 14467.61MB
Epoch [6/10]; Iter [3870/5650]; loss 3.9622; gen_loss 3.9622; LR 2.33e-05; Iter time 0.26s; ETA 0:07:46; Mem 14467.61MB
Epoch [6/10]; Iter [3880/5650]; loss 3.8556; gen_loss 3.8556; LR 2.31e-05; Iter time 0.26s; ETA 0:07:44; Mem 14467.61MB
Epoch [6/10]; Iter [3890/5650]; loss 4.1732; gen_loss 4.1732; LR 2.29e-05; Iter time 0.26s; ETA 0:07:41; Mem 14467.61MB
Epoch [6/10]; Iter [3900/5650]; loss 3.9609; gen_loss 3.9609; LR 2.26e-05; Iter time 0.26s; ETA 0:07:39; Mem 14467.61MB
Epoch [6/10]; Iter [3910/5650]; loss 3.9497; gen_loss 3.9497; LR 2.24e-05; Iter time 0.26s; ETA 0:07:36; Mem 14467.61MB
Epoch [6/10]; Iter [3920/5650]; loss 4.0270; gen_loss 4.0270; LR 2.22e-05; Iter time 0.26s; ETA 0:07:33; Mem 14467.61MB
Epoch [6/10]; Iter [3930/5650]; loss 4.0426; gen_loss 4.0426; LR 2.20e-05; Iter time 0.26s; ETA 0:07:31; Mem 14467.61MB
Epoch [6/10]; Iter [3940/5650]; loss 4.0687; gen_loss 4.0687; LR 2.17e-05; Iter time 0.26s; ETA 0:07:29; Mem 14467.61MB
Epoch [6/10]; Iter [3950/5650]; loss 4.0571; gen_loss 4.0571; LR 2.15e-05; Iter time 0.26s; ETA 0:07:25; Mem 14467.61MB
Epoch [7/10]; Iter [3960/5650]; loss 3.9106; gen_loss 3.9106; LR 2.13e-05; Iter time 0.26s; ETA 0:07:23; Mem 14467.61MB
Epoch [7/10]; Iter [3970/5650]; loss 3.8261; gen_loss 3.8261; LR 2.11e-05; Iter time 0.26s; ETA 0:07:21; Mem 14467.61MB
Epoch [7/10]; Iter [3980/5650]; loss 4.0994; gen_loss 4.0994; LR 2.09e-05; Iter time 0.26s; ETA 0:07:17; Mem 14467.61MB
Epoch [7/10]; Iter [3990/5650]; loss 4.0805; gen_loss 4.0805; LR 2.06e-05; Iter time 0.26s; ETA 0:07:16; Mem 14467.61MB
====================
Evaluate Epoch [7/10]
====================
Evaluate [7/10]; Batch [0/62]; perplexity: 81.0781; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [10/62]; perplexity: 57.7175; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [20/62]; perplexity: 55.0390; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [30/62]; perplexity: 57.4815; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [40/62]; perplexity: 54.4811; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [50/62]; perplexity: 56.0170; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Evaluate [7/10]; Batch [60/62]; perplexity: 55.0947; Evaluating on iter: 3999; Iter time 0.08; Mem 14467.61MB
Epoch [7/10]; Iter [4000/5650]; loss 4.0714; gen_loss 4.0714; LR 2.04e-05; Iter time 0.26s; ETA 0:07:13; Mem 14467.61MB
Epoch [7/10]; Iter [4010/5650]; loss 3.8232; gen_loss 3.8232; LR 2.02e-05; Iter time 0.26s; ETA 0:07:09; Mem 14467.61MB
Epoch [7/10]; Iter [4020/5650]; loss 3.9783; gen_loss 3.9783; LR 2.00e-05; Iter time 0.26s; ETA 0:07:07; Mem 14467.61MB
Epoch [7/10]; Iter [4030/5650]; loss 4.0651; gen_loss 4.0651; LR 1.98e-05; Iter time 0.26s; ETA 0:07:04; Mem 14467.61MB
Epoch [7/10]; Iter [4040/5650]; loss 4.0290; gen_loss 4.0290; LR 1.95e-05; Iter time 0.26s; ETA 0:07:02; Mem 14467.61MB
Epoch [7/10]; Iter [4050/5650]; loss 4.0397; gen_loss 4.0397; LR 1.93e-05; Iter time 0.26s; ETA 0:06:59; Mem 14467.61MB
Epoch [7/10]; Iter [4060/5650]; loss 3.9361; gen_loss 3.9361; LR 1.91e-05; Iter time 0.26s; ETA 0:06:56; Mem 14467.61MB
Epoch [7/10]; Iter [4070/5650]; loss 4.1153; gen_loss 4.1153; LR 1.89e-05; Iter time 0.26s; ETA 0:06:54; Mem 14467.61MB
Epoch [7/10]; Iter [4080/5650]; loss 3.9435; gen_loss 3.9435; LR 1.87e-05; Iter time 0.26s; ETA 0:06:52; Mem 14467.61MB
Epoch [7/10]; Iter [4090/5650]; loss 3.8373; gen_loss 3.8373; LR 1.85e-05; Iter time 0.26s; ETA 0:06:49; Mem 14467.61MB
Epoch [7/10]; Iter [4100/5650]; loss 3.7258; gen_loss 3.7258; LR 1.83e-05; Iter time 0.26s; ETA 0:06:46; Mem 14467.61MB
Epoch [7/10]; Iter [4110/5650]; loss 3.8840; gen_loss 3.8840; LR 1.81e-05; Iter time 0.26s; ETA 0:06:44; Mem 14467.61MB
Epoch [7/10]; Iter [4120/5650]; loss 3.8832; gen_loss 3.8832; LR 1.79e-05; Iter time 0.26s; ETA 0:06:41; Mem 14467.61MB
Epoch [7/10]; Iter [4130/5650]; loss 3.8192; gen_loss 3.8192; LR 1.77e-05; Iter time 0.26s; ETA 0:06:38; Mem 14467.61MB
Epoch [7/10]; Iter [4140/5650]; loss 3.8803; gen_loss 3.8803; LR 1.74e-05; Iter time 0.26s; ETA 0:06:35; Mem 14467.61MB
Epoch [7/10]; Iter [4150/5650]; loss 3.8204; gen_loss 3.8204; LR 1.72e-05; Iter time 0.26s; ETA 0:06:33; Mem 14467.61MB
Epoch [7/10]; Iter [4160/5650]; loss 3.8909; gen_loss 3.8909; LR 1.70e-05; Iter time 0.26s; ETA 0:06:30; Mem 14467.61MB
Epoch [7/10]; Iter [4170/5650]; loss 3.8675; gen_loss 3.8675; LR 1.68e-05; Iter time 0.26s; ETA 0:06:28; Mem 14467.61MB
Epoch [7/10]; Iter [4180/5650]; loss 3.9830; gen_loss 3.9830; LR 1.66e-05; Iter time 0.26s; ETA 0:06:25; Mem 14467.61MB
Epoch [7/10]; Iter [4190/5650]; loss 4.0087; gen_loss 4.0087; LR 1.64e-05; Iter time 0.26s; ETA 0:06:22; Mem 14467.61MB
Epoch [7/10]; Iter [4200/5650]; loss 3.9399; gen_loss 3.9399; LR 1.62e-05; Iter time 0.26s; ETA 0:06:20; Mem 14467.61MB
Epoch [7/10]; Iter [4210/5650]; loss 3.7997; gen_loss 3.7997; LR 1.60e-05; Iter time 0.26s; ETA 0:06:17; Mem 14467.61MB
Epoch [7/10]; Iter [4220/5650]; loss 3.9186; gen_loss 3.9186; LR 1.58e-05; Iter time 0.26s; ETA 0:06:15; Mem 14467.61MB
Epoch [7/10]; Iter [4230/5650]; loss 3.7835; gen_loss 3.7835; LR 1.56e-05; Iter time 0.26s; ETA 0:06:12; Mem 14467.61MB
Epoch [7/10]; Iter [4240/5650]; loss 3.9636; gen_loss 3.9636; LR 1.54e-05; Iter time 0.26s; ETA 0:06:09; Mem 14467.61MB
Epoch [7/10]; Iter [4250/5650]; loss 4.1311; gen_loss 4.1311; LR 1.53e-05; Iter time 0.26s; ETA 0:06:07; Mem 14467.61MB
Epoch [7/10]; Iter [4260/5650]; loss 4.0055; gen_loss 4.0055; LR 1.51e-05; Iter time 0.26s; ETA 0:06:04; Mem 14467.61MB
Epoch [7/10]; Iter [4270/5650]; loss 3.8295; gen_loss 3.8295; LR 1.49e-05; Iter time 0.26s; ETA 0:06:01; Mem 14467.61MB
Epoch [7/10]; Iter [4280/5650]; loss 4.0521; gen_loss 4.0521; LR 1.47e-05; Iter time 0.26s; ETA 0:05:59; Mem 14467.61MB
Epoch [7/10]; Iter [4290/5650]; loss 3.9511; gen_loss 3.9511; LR 1.45e-05; Iter time 0.26s; ETA 0:05:56; Mem 14467.61MB
Epoch [7/10]; Iter [4300/5650]; loss 3.9410; gen_loss 3.9410; LR 1.43e-05; Iter time 0.26s; ETA 0:05:54; Mem 14467.61MB
Epoch [7/10]; Iter [4310/5650]; loss 3.9736; gen_loss 3.9736; LR 1.41e-05; Iter time 0.26s; ETA 0:05:51; Mem 14467.61MB
Epoch [7/10]; Iter [4320/5650]; loss 4.0408; gen_loss 4.0408; LR 1.39e-05; Iter time 0.26s; ETA 0:05:49; Mem 14467.61MB
Epoch [7/10]; Iter [4330/5650]; loss 3.9679; gen_loss 3.9679; LR 1.37e-05; Iter time 0.26s; ETA 0:05:46; Mem 14467.61MB
Epoch [7/10]; Iter [4340/5650]; loss 4.0866; gen_loss 4.0866; LR 1.36e-05; Iter time 0.26s; ETA 0:05:43; Mem 14467.61MB
Epoch [7/10]; Iter [4350/5650]; loss 4.1661; gen_loss 4.1661; LR 1.34e-05; Iter time 0.26s; ETA 0:05:41; Mem 14467.61MB
Epoch [7/10]; Iter [4360/5650]; loss 4.0066; gen_loss 4.0066; LR 1.32e-05; Iter time 0.26s; ETA 0:05:38; Mem 14467.61MB
Epoch [7/10]; Iter [4370/5650]; loss 4.0205; gen_loss 4.0205; LR 1.30e-05; Iter time 0.26s; ETA 0:05:36; Mem 14467.61MB
Epoch [7/10]; Iter [4380/5650]; loss 3.7692; gen_loss 3.7692; LR 1.28e-05; Iter time 0.26s; ETA 0:05:32; Mem 14467.61MB
Epoch [7/10]; Iter [4390/5650]; loss 4.0310; gen_loss 4.0310; LR 1.27e-05; Iter time 0.26s; ETA 0:05:30; Mem 14467.61MB
Epoch [7/10]; Iter [4400/5650]; loss 4.0558; gen_loss 4.0558; LR 1.25e-05; Iter time 0.26s; ETA 0:05:27; Mem 14467.61MB
Epoch [7/10]; Iter [4410/5650]; loss 3.9390; gen_loss 3.9390; LR 1.23e-05; Iter time 0.26s; ETA 0:05:25; Mem 14467.61MB
Epoch [7/10]; Iter [4420/5650]; loss 4.0276; gen_loss 4.0276; LR 1.21e-05; Iter time 0.26s; ETA 0:05:22; Mem 14467.61MB
Epoch [7/10]; Iter [4430/5650]; loss 4.0006; gen_loss 4.0006; LR 1.20e-05; Iter time 0.26s; ETA 0:05:20; Mem 14467.61MB
Epoch [7/10]; Iter [4440/5650]; loss 3.8284; gen_loss 3.8284; LR 1.18e-05; Iter time 0.26s; ETA 0:05:17; Mem 14467.61MB
Epoch [7/10]; Iter [4450/5650]; loss 3.9616; gen_loss 3.9616; LR 1.16e-05; Iter time 0.26s; ETA 0:05:14; Mem 14467.61MB
Epoch [7/10]; Iter [4460/5650]; loss 3.7757; gen_loss 3.7757; LR 1.14e-05; Iter time 0.26s; ETA 0:05:12; Mem 14467.61MB
Epoch [7/10]; Iter [4470/5650]; loss 4.0644; gen_loss 4.0644; LR 1.13e-05; Iter time 0.26s; ETA 0:05:09; Mem 14467.61MB
Epoch [7/10]; Iter [4480/5650]; loss 4.0795; gen_loss 4.0795; LR 1.11e-05; Iter time 0.26s; ETA 0:05:06; Mem 14467.61MB
Epoch [7/10]; Iter [4490/5650]; loss 3.8297; gen_loss 3.8297; LR 1.09e-05; Iter time 0.26s; ETA 0:05:04; Mem 14467.61MB
Epoch [7/10]; Iter [4500/5650]; loss 3.9743; gen_loss 3.9743; LR 1.08e-05; Iter time 0.26s; ETA 0:05:01; Mem 14467.61MB
Epoch [7/10]; Iter [4510/5650]; loss 3.8537; gen_loss 3.8537; LR 1.06e-05; Iter time 0.26s; ETA 0:04:58; Mem 14467.61MB
Epoch [8/10]; Iter [4520/5650]; loss 3.8751; gen_loss 3.8751; LR 1.05e-05; Iter time 0.26s; ETA 0:04:56; Mem 14467.61MB
Epoch [8/10]; Iter [4530/5650]; loss 3.8253; gen_loss 3.8253; LR 1.03e-05; Iter time 0.26s; ETA 0:04:54; Mem 14467.61MB
Epoch [8/10]; Iter [4540/5650]; loss 3.9169; gen_loss 3.9169; LR 1.01e-05; Iter time 0.26s; ETA 0:04:51; Mem 14467.61MB
Epoch [8/10]; Iter [4550/5650]; loss 3.9942; gen_loss 3.9942; LR 9.97e-06; Iter time 0.26s; ETA 0:04:48; Mem 14467.61MB
Epoch [8/10]; Iter [4560/5650]; loss 3.9119; gen_loss 3.9119; LR 9.82e-06; Iter time 0.26s; ETA 0:04:46; Mem 14467.61MB
Epoch [8/10]; Iter [4570/5650]; loss 3.7958; gen_loss 3.7958; LR 9.66e-06; Iter time 0.26s; ETA 0:04:43; Mem 14467.61MB
Epoch [8/10]; Iter [4580/5650]; loss 3.7608; gen_loss 3.7608; LR 9.51e-06; Iter time 0.26s; ETA 0:04:41; Mem 14467.61MB
Epoch [8/10]; Iter [4590/5650]; loss 4.0004; gen_loss 4.0004; LR 9.35e-06; Iter time 0.26s; ETA 0:04:38; Mem 14467.61MB
Epoch [8/10]; Iter [4600/5650]; loss 3.9951; gen_loss 3.9951; LR 9.20e-06; Iter time 0.26s; ETA 0:04:35; Mem 14467.61MB
Epoch [8/10]; Iter [4610/5650]; loss 3.9273; gen_loss 3.9273; LR 9.05e-06; Iter time 0.26s; ETA 0:04:32; Mem 14467.61MB
Epoch [8/10]; Iter [4620/5650]; loss 4.1310; gen_loss 4.1310; LR 8.90e-06; Iter time 0.26s; ETA 0:04:30; Mem 14467.61MB
Epoch [8/10]; Iter [4630/5650]; loss 3.8400; gen_loss 3.8400; LR 8.75e-06; Iter time 0.26s; ETA 0:04:27; Mem 14467.61MB
Epoch [8/10]; Iter [4640/5650]; loss 4.0845; gen_loss 4.0845; LR 8.60e-06; Iter time 0.26s; ETA 0:04:24; Mem 14467.61MB
Epoch [8/10]; Iter [4650/5650]; loss 3.6009; gen_loss 3.6009; LR 8.46e-06; Iter time 0.26s; ETA 0:04:22; Mem 14467.61MB
Epoch [8/10]; Iter [4660/5650]; loss 3.7985; gen_loss 3.7985; LR 8.31e-06; Iter time 0.26s; ETA 0:04:19; Mem 14467.61MB
Epoch [8/10]; Iter [4670/5650]; loss 4.0435; gen_loss 4.0435; LR 8.17e-06; Iter time 0.26s; ETA 0:04:17; Mem 14467.61MB
Epoch [8/10]; Iter [4680/5650]; loss 3.9742; gen_loss 3.9742; LR 8.03e-06; Iter time 0.26s; ETA 0:04:14; Mem 14467.61MB
Epoch [8/10]; Iter [4690/5650]; loss 3.9202; gen_loss 3.9202; LR 7.89e-06; Iter time 0.26s; ETA 0:04:11; Mem 14467.61MB
Epoch [8/10]; Iter [4700/5650]; loss 4.0227; gen_loss 4.0227; LR 7.75e-06; Iter time 0.26s; ETA 0:04:09; Mem 14467.61MB
Epoch [8/10]; Iter [4710/5650]; loss 4.1032; gen_loss 4.1032; LR 7.61e-06; Iter time 0.26s; ETA 0:04:06; Mem 14467.61MB
Epoch [8/10]; Iter [4720/5650]; loss 3.8224; gen_loss 3.8224; LR 7.47e-06; Iter time 0.26s; ETA 0:04:03; Mem 14467.61MB
Epoch [8/10]; Iter [4730/5650]; loss 4.0141; gen_loss 4.0141; LR 7.34e-06; Iter time 0.26s; ETA 0:04:01; Mem 14467.61MB
Epoch [8/10]; Iter [4740/5650]; loss 4.0301; gen_loss 4.0301; LR 7.20e-06; Iter time 0.26s; ETA 0:03:58; Mem 14467.61MB
Epoch [8/10]; Iter [4750/5650]; loss 3.8388; gen_loss 3.8388; LR 7.07e-06; Iter time 0.26s; ETA 0:03:56; Mem 14467.61MB
Epoch [8/10]; Iter [4760/5650]; loss 3.9432; gen_loss 3.9432; LR 6.94e-06; Iter time 0.26s; ETA 0:03:53; Mem 14467.61MB
Epoch [8/10]; Iter [4770/5650]; loss 3.8173; gen_loss 3.8173; LR 6.81e-06; Iter time 0.26s; ETA 0:03:50; Mem 14467.61MB
Epoch [8/10]; Iter [4780/5650]; loss 4.0007; gen_loss 4.0007; LR 6.68e-06; Iter time 0.26s; ETA 0:03:48; Mem 14467.61MB
Epoch [8/10]; Iter [4790/5650]; loss 3.8891; gen_loss 3.8891; LR 6.55e-06; Iter time 0.26s; ETA 0:03:45; Mem 14467.61MB
Epoch [8/10]; Iter [4800/5650]; loss 3.9756; gen_loss 3.9756; LR 6.43e-06; Iter time 0.26s; ETA 0:03:42; Mem 14467.61MB
Epoch [8/10]; Iter [4810/5650]; loss 3.9543; gen_loss 3.9543; LR 6.30e-06; Iter time 0.26s; ETA 0:03:40; Mem 14467.61MB
Epoch [8/10]; Iter [4820/5650]; loss 3.7361; gen_loss 3.7361; LR 6.18e-06; Iter time 0.26s; ETA 0:03:37; Mem 14467.61MB
Epoch [8/10]; Iter [4830/5650]; loss 4.0550; gen_loss 4.0550; LR 6.06e-06; Iter time 0.26s; ETA 0:03:35; Mem 14467.61MB
Epoch [8/10]; Iter [4840/5650]; loss 3.8612; gen_loss 3.8612; LR 5.94e-06; Iter time 0.26s; ETA 0:03:32; Mem 14467.61MB
Epoch [8/10]; Iter [4850/5650]; loss 3.9701; gen_loss 3.9701; LR 5.82e-06; Iter time 0.26s; ETA 0:03:29; Mem 14467.61MB
Epoch [8/10]; Iter [4860/5650]; loss 3.8317; gen_loss 3.8317; LR 5.70e-06; Iter time 0.26s; ETA 0:03:27; Mem 14467.61MB
Epoch [8/10]; Iter [4870/5650]; loss 4.1719; gen_loss 4.1719; LR 5.58e-06; Iter time 0.26s; ETA 0:03:24; Mem 14467.61MB
Epoch [8/10]; Iter [4880/5650]; loss 4.1209; gen_loss 4.1209; LR 5.47e-06; Iter time 0.26s; ETA 0:03:21; Mem 14467.61MB
Epoch [8/10]; Iter [4890/5650]; loss 4.0671; gen_loss 4.0671; LR 5.35e-06; Iter time 0.26s; ETA 0:03:19; Mem 14467.61MB
Epoch [8/10]; Iter [4900/5650]; loss 4.0431; gen_loss 4.0431; LR 5.24e-06; Iter time 0.26s; ETA 0:03:16; Mem 14467.61MB
Epoch [8/10]; Iter [4910/5650]; loss 3.9502; gen_loss 3.9502; LR 5.13e-06; Iter time 0.26s; ETA 0:03:14; Mem 14467.61MB
Epoch [8/10]; Iter [4920/5650]; loss 4.0812; gen_loss 4.0812; LR 5.02e-06; Iter time 0.26s; ETA 0:03:11; Mem 14467.61MB
Epoch [8/10]; Iter [4930/5650]; loss 3.9436; gen_loss 3.9436; LR 4.91e-06; Iter time 0.26s; ETA 0:03:08; Mem 14467.61MB
Epoch [8/10]; Iter [4940/5650]; loss 3.8538; gen_loss 3.8538; LR 4.81e-06; Iter time 0.26s; ETA 0:03:06; Mem 14467.61MB
Epoch [8/10]; Iter [4950/5650]; loss 3.9830; gen_loss 3.9830; LR 4.70e-06; Iter time 0.26s; ETA 0:03:03; Mem 14467.61MB
Epoch [8/10]; Iter [4960/5650]; loss 3.8557; gen_loss 3.8557; LR 4.60e-06; Iter time 0.26s; ETA 0:03:01; Mem 14467.61MB
Epoch [8/10]; Iter [4970/5650]; loss 4.0316; gen_loss 4.0316; LR 4.50e-06; Iter time 0.26s; ETA 0:02:58; Mem 14467.61MB
Epoch [8/10]; Iter [4980/5650]; loss 3.9417; gen_loss 3.9417; LR 4.40e-06; Iter time 0.26s; ETA 0:02:55; Mem 14467.61MB
Epoch [8/10]; Iter [4990/5650]; loss 3.9348; gen_loss 3.9348; LR 4.30e-06; Iter time 0.26s; ETA 0:02:52; Mem 14467.61MB
Epoch [8/10]; Iter [5000/5650]; loss 3.9759; gen_loss 3.9759; LR 4.20e-06; Iter time 0.26s; ETA 0:02:50; Mem 14467.61MB
Epoch [8/10]; Iter [5010/5650]; loss 3.9532; gen_loss 3.9532; LR 4.10e-06; Iter time 0.26s; ETA 0:02:47; Mem 14467.61MB
Epoch [8/10]; Iter [5020/5650]; loss 3.7317; gen_loss 3.7317; LR 4.01e-06; Iter time 0.26s; ETA 0:02:45; Mem 14467.61MB
Epoch [8/10]; Iter [5030/5650]; loss 3.9753; gen_loss 3.9753; LR 3.91e-06; Iter time 0.26s; ETA 0:02:42; Mem 14467.61MB
Epoch [8/10]; Iter [5040/5650]; loss 3.9154; gen_loss 3.9154; LR 3.82e-06; Iter time 0.26s; ETA 0:02:40; Mem 14467.61MB
Epoch [8/10]; Iter [5050/5650]; loss 3.9627; gen_loss 3.9627; LR 3.73e-06; Iter time 0.26s; ETA 0:02:37; Mem 14467.61MB
Epoch [8/10]; Iter [5060/5650]; loss 3.8766; gen_loss 3.8766; LR 3.64e-06; Iter time 0.26s; ETA 0:02:34; Mem 14467.61MB
Epoch [8/10]; Iter [5070/5650]; loss 3.9431; gen_loss 3.9431; LR 3.55e-06; Iter time 0.26s; ETA 0:02:32; Mem 14467.61MB
Epoch [8/10]; Iter [5080/5650]; loss 3.8179; gen_loss 3.8179; LR 3.47e-06; Iter time 0.26s; ETA 0:02:29; Mem 14467.61MB
Epoch [9/10]; Iter [5090/5650]; loss 3.9215; gen_loss 3.9215; LR 3.38e-06; Iter time 0.26s; ETA 0:02:26; Mem 14467.61MB
Epoch [9/10]; Iter [5100/5650]; loss 3.6731; gen_loss 3.6731; LR 3.30e-06; Iter time 0.26s; ETA 0:02:24; Mem 14467.61MB
Epoch [9/10]; Iter [5110/5650]; loss 3.8568; gen_loss 3.8568; LR 3.21e-06; Iter time 0.26s; ETA 0:02:21; Mem 14467.61MB
Epoch [9/10]; Iter [5120/5650]; loss 3.9170; gen_loss 3.9170; LR 3.13e-06; Iter time 0.26s; ETA 0:02:19; Mem 14467.61MB
Epoch [9/10]; Iter [5130/5650]; loss 3.4396; gen_loss 3.4396; LR 3.05e-06; Iter time 0.26s; ETA 0:02:16; Mem 14467.61MB
Epoch [9/10]; Iter [5140/5650]; loss 3.7943; gen_loss 3.7943; LR 2.98e-06; Iter time 0.26s; ETA 0:02:13; Mem 14467.61MB
Epoch [9/10]; Iter [5150/5650]; loss 3.9740; gen_loss 3.9740; LR 2.90e-06; Iter time 0.26s; ETA 0:02:11; Mem 14467.61MB
Epoch [9/10]; Iter [5160/5650]; loss 3.9101; gen_loss 3.9101; LR 2.83e-06; Iter time 0.26s; ETA 0:02:08; Mem 14467.61MB
Epoch [9/10]; Iter [5170/5650]; loss 4.0531; gen_loss 4.0531; LR 2.75e-06; Iter time 0.26s; ETA 0:02:05; Mem 14467.61MB
Epoch [9/10]; Iter [5180/5650]; loss 3.9460; gen_loss 3.9460; LR 2.68e-06; Iter time 0.26s; ETA 0:02:03; Mem 14467.61MB
Epoch [9/10]; Iter [5190/5650]; loss 4.0019; gen_loss 4.0019; LR 2.61e-06; Iter time 0.26s; ETA 0:02:00; Mem 14467.61MB
Epoch [9/10]; Iter [5200/5650]; loss 3.8938; gen_loss 3.8938; LR 2.54e-06; Iter time 0.26s; ETA 0:01:58; Mem 14467.61MB
Epoch [9/10]; Iter [5210/5650]; loss 4.0857; gen_loss 4.0857; LR 2.47e-06; Iter time 0.26s; ETA 0:01:55; Mem 14467.61MB
Epoch [9/10]; Iter [5220/5650]; loss 4.0489; gen_loss 4.0489; LR 2.41e-06; Iter time 0.26s; ETA 0:01:52; Mem 14467.61MB
Epoch [9/10]; Iter [5230/5650]; loss 3.8587; gen_loss 3.8587; LR 2.34e-06; Iter time 0.26s; ETA 0:01:50; Mem 14467.61MB
Epoch [9/10]; Iter [5240/5650]; loss 3.9944; gen_loss 3.9944; LR 2.28e-06; Iter time 0.26s; ETA 0:01:47; Mem 14467.61MB
Epoch [9/10]; Iter [5250/5650]; loss 4.0422; gen_loss 4.0422; LR 2.22e-06; Iter time 0.26s; ETA 0:01:44; Mem 14467.61MB
Epoch [9/10]; Iter [5260/5650]; loss 3.7572; gen_loss 3.7572; LR 2.16e-06; Iter time 0.26s; ETA 0:01:42; Mem 14467.61MB
Epoch [9/10]; Iter [5270/5650]; loss 4.0683; gen_loss 4.0683; LR 2.10e-06; Iter time 0.26s; ETA 0:01:39; Mem 14467.61MB
Epoch [9/10]; Iter [5280/5650]; loss 3.9228; gen_loss 3.9228; LR 2.04e-06; Iter time 0.26s; ETA 0:01:37; Mem 14467.61MB
Epoch [9/10]; Iter [5290/5650]; loss 3.9679; gen_loss 3.9679; LR 1.99e-06; Iter time 0.26s; ETA 0:01:34; Mem 14467.61MB
Epoch [9/10]; Iter [5300/5650]; loss 3.8172; gen_loss 3.8172; LR 1.93e-06; Iter time 0.26s; ETA 0:01:31; Mem 14467.61MB
Epoch [9/10]; Iter [5310/5650]; loss 4.1377; gen_loss 4.1377; LR 1.88e-06; Iter time 0.26s; ETA 0:01:29; Mem 14467.61MB
Epoch [9/10]; Iter [5320/5650]; loss 4.0664; gen_loss 4.0664; LR 1.83e-06; Iter time 0.26s; ETA 0:01:26; Mem 14467.61MB
Epoch [9/10]; Iter [5330/5650]; loss 3.7906; gen_loss 3.7906; LR 1.78e-06; Iter time 0.26s; ETA 0:01:24; Mem 14467.61MB
Epoch [9/10]; Iter [5340/5650]; loss 3.8417; gen_loss 3.8417; LR 1.73e-06; Iter time 0.26s; ETA 0:01:21; Mem 14467.61MB
Epoch [9/10]; Iter [5350/5650]; loss 4.0912; gen_loss 4.0912; LR 1.69e-06; Iter time 0.26s; ETA 0:01:18; Mem 14467.61MB
Epoch [9/10]; Iter [5360/5650]; loss 3.9972; gen_loss 3.9972; LR 1.64e-06; Iter time 0.26s; ETA 0:01:16; Mem 14467.61MB
Epoch [9/10]; Iter [5370/5650]; loss 3.9877; gen_loss 3.9877; LR 1.60e-06; Iter time 0.26s; ETA 0:01:13; Mem 14467.61MB
Epoch [9/10]; Iter [5380/5650]; loss 3.7580; gen_loss 3.7580; LR 1.56e-06; Iter time 0.26s; ETA 0:01:10; Mem 14467.61MB
Epoch [9/10]; Iter [5390/5650]; loss 3.7476; gen_loss 3.7476; LR 1.52e-06; Iter time 0.26s; ETA 0:01:08; Mem 14467.61MB
Epoch [9/10]; Iter [5400/5650]; loss 3.9608; gen_loss 3.9608; LR 1.48e-06; Iter time 0.26s; ETA 0:01:05; Mem 14467.61MB
Epoch [9/10]; Iter [5410/5650]; loss 4.0182; gen_loss 4.0182; LR 1.44e-06; Iter time 0.26s; ETA 0:01:03; Mem 14467.61MB
Epoch [9/10]; Iter [5420/5650]; loss 3.7581; gen_loss 3.7581; LR 1.40e-06; Iter time 0.26s; ETA 0:01:00; Mem 14467.61MB
Epoch [9/10]; Iter [5430/5650]; loss 4.0894; gen_loss 4.0894; LR 1.37e-06; Iter time 0.26s; ETA 0:00:57; Mem 14467.61MB
Epoch [9/10]; Iter [5440/5650]; loss 4.0538; gen_loss 4.0538; LR 1.34e-06; Iter time 0.26s; ETA 0:00:55; Mem 14467.61MB
Epoch [9/10]; Iter [5450/5650]; loss 3.9580; gen_loss 3.9580; LR 1.31e-06; Iter time 0.26s; ETA 0:00:52; Mem 14467.61MB
Epoch [9/10]; Iter [5460/5650]; loss 4.0129; gen_loss 4.0129; LR 1.28e-06; Iter time 0.26s; ETA 0:00:49; Mem 14467.61MB
Epoch [9/10]; Iter [5470/5650]; loss 4.0384; gen_loss 4.0384; LR 1.25e-06; Iter time 0.26s; ETA 0:00:47; Mem 14467.61MB
Epoch [9/10]; Iter [5480/5650]; loss 3.6462; gen_loss 3.6462; LR 1.22e-06; Iter time 0.26s; ETA 0:00:44; Mem 14467.61MB
Epoch [9/10]; Iter [5490/5650]; loss 3.6646; gen_loss 3.6646; LR 1.20e-06; Iter time 0.26s; ETA 0:00:42; Mem 14467.61MB
Epoch [9/10]; Iter [5500/5650]; loss 3.9279; gen_loss 3.9279; LR 1.17e-06; Iter time 0.26s; ETA 0:00:39; Mem 14467.61MB
Epoch [9/10]; Iter [5510/5650]; loss 4.0795; gen_loss 4.0795; LR 1.15e-06; Iter time 0.26s; ETA 0:00:36; Mem 14467.61MB
Epoch [9/10]; Iter [5520/5650]; loss 3.8754; gen_loss 3.8754; LR 1.13e-06; Iter time 0.26s; ETA 0:00:34; Mem 14467.61MB
Epoch [9/10]; Iter [5530/5650]; loss 4.0456; gen_loss 4.0456; LR 1.11e-06; Iter time 0.26s; ETA 0:00:31; Mem 14467.61MB
Epoch [9/10]; Iter [5540/5650]; loss 4.0987; gen_loss 4.0987; LR 1.09e-06; Iter time 0.26s; ETA 0:00:28; Mem 14467.61MB
Epoch [9/10]; Iter [5550/5650]; loss 3.9249; gen_loss 3.9249; LR 1.08e-06; Iter time 0.26s; ETA 0:00:26; Mem 14467.61MB
Epoch [9/10]; Iter [5560/5650]; loss 3.9829; gen_loss 3.9829; LR 1.06e-06; Iter time 0.26s; ETA 0:00:23; Mem 14467.61MB
Epoch [9/10]; Iter [5570/5650]; loss 4.0176; gen_loss 4.0176; LR 1.05e-06; Iter time 0.26s; ETA 0:00:20; Mem 14467.61MB
Epoch [9/10]; Iter [5580/5650]; loss 3.9252; gen_loss 3.9252; LR 1.04e-06; Iter time 0.26s; ETA 0:00:18; Mem 14467.61MB
Epoch [9/10]; Iter [5590/5650]; loss 4.1462; gen_loss 4.1462; LR 1.03e-06; Iter time 0.26s; ETA 0:00:15; Mem 14467.61MB
Epoch [9/10]; Iter [5600/5650]; loss 3.8804; gen_loss 3.8804; LR 1.02e-06; Iter time 0.26s; ETA 0:00:13; Mem 14467.61MB
Epoch [9/10]; Iter [5610/5650]; loss 4.0214; gen_loss 4.0214; LR 1.01e-06; Iter time 0.26s; ETA 0:00:10; Mem 14467.61MB
Epoch [9/10]; Iter [5620/5650]; loss 3.9475; gen_loss 3.9475; LR 1.01e-06; Iter time 0.26s; ETA 0:00:07; Mem 14467.61MB
Epoch [9/10]; Iter [5630/5650]; loss 4.0302; gen_loss 4.0302; LR 1.00e-06; Iter time 0.26s; ETA 0:00:05; Mem 14467.61MB
Epoch [9/10]; Iter [5640/5650]; loss 3.8856; gen_loss 3.8856; LR 1.00e-06; Iter time 0.26s; ETA 0:00:02; Mem 14467.61MB
====================
Evaluate Epoch [9/10]
====================
Evaluate [9/10]; Batch [0/62]; perplexity: 80.1201; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [10/62]; perplexity: 56.9688; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [20/62]; perplexity: 54.4196; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [30/62]; perplexity: 56.8386; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [40/62]; perplexity: 53.8181; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [50/62]; perplexity: 55.3777; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
Evaluate [9/10]; Batch [60/62]; perplexity: 54.4657; Evaluating on iter: 5649; Iter time 0.08; Mem 14467.61MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=10, max_epoch=10, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=10, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
Epoch [0/1]; Iter [0/565]; loss 4.9258; gen_loss 4.9258; LR 1.00e-06; Iter time 0.44s; ETA 0:04:06; Mem 14467.61MB
Epoch [0/1]; Iter [10/565]; loss 4.9654; gen_loss 4.9654; LR 1.99e-06; Iter time 0.26s; ETA 0:02:24; Mem 14467.61MB
Epoch [0/1]; Iter [20/565]; loss 4.9346; gen_loss 4.9346; LR 2.98e-06; Iter time 0.26s; ETA 0:02:22; Mem 14467.61MB
Epoch [0/1]; Iter [30/565]; loss 4.9471; gen_loss 4.9471; LR 3.97e-06; Iter time 0.26s; ETA 0:02:19; Mem 14467.61MB
Epoch [0/1]; Iter [40/565]; loss 4.9779; gen_loss 4.9779; LR 4.96e-06; Iter time 0.26s; ETA 0:02:17; Mem 14467.61MB
Epoch [0/1]; Iter [50/565]; loss 4.9744; gen_loss 4.9744; LR 5.95e-06; Iter time 0.26s; ETA 0:02:14; Mem 14467.61MB
Epoch [0/1]; Iter [60/565]; loss 4.9361; gen_loss 4.9361; LR 6.94e-06; Iter time 0.26s; ETA 0:02:12; Mem 14467.61MB
Epoch [0/1]; Iter [70/565]; loss 4.8974; gen_loss 4.8974; LR 7.93e-06; Iter time 0.26s; ETA 0:02:09; Mem 14467.61MB
Epoch [0/1]; Iter [80/565]; loss 4.9286; gen_loss 4.9286; LR 8.92e-06; Iter time 0.26s; ETA 0:02:06; Mem 14467.61MB
Epoch [0/1]; Iter [90/565]; loss 4.9100; gen_loss 4.9100; LR 9.91e-06; Iter time 0.26s; ETA 0:02:04; Mem 14467.61MB
Epoch [0/1]; Iter [100/565]; loss 4.8863; gen_loss 4.8863; LR 1.09e-05; Iter time 0.26s; ETA 0:02:01; Mem 14467.61MB
Epoch [0/1]; Iter [110/565]; loss 4.8546; gen_loss 4.8546; LR 1.19e-05; Iter time 0.26s; ETA 0:01:59; Mem 14467.61MB
Epoch [0/1]; Iter [120/565]; loss 4.9117; gen_loss 4.9117; LR 1.29e-05; Iter time 0.26s; ETA 0:01:56; Mem 14467.61MB
Epoch [0/1]; Iter [130/565]; loss 4.8373; gen_loss 4.8373; LR 1.39e-05; Iter time 0.26s; ETA 0:01:54; Mem 14467.61MB
Epoch [0/1]; Iter [140/565]; loss 4.8300; gen_loss 4.8300; LR 1.49e-05; Iter time 0.26s; ETA 0:01:51; Mem 14467.61MB
Epoch [0/1]; Iter [150/565]; loss 4.8164; gen_loss 4.8164; LR 1.58e-05; Iter time 0.26s; ETA 0:01:48; Mem 14467.61MB
Epoch [0/1]; Iter [160/565]; loss 4.8361; gen_loss 4.8361; LR 1.68e-05; Iter time 0.26s; ETA 0:01:46; Mem 14467.61MB
Epoch [0/1]; Iter [170/565]; loss 4.8258; gen_loss 4.8258; LR 1.78e-05; Iter time 0.26s; ETA 0:01:43; Mem 14467.61MB
Epoch [0/1]; Iter [180/565]; loss 4.7242; gen_loss 4.7242; LR 1.88e-05; Iter time 0.26s; ETA 0:01:40; Mem 14467.61MB
Epoch [0/1]; Iter [190/565]; loss 4.8113; gen_loss 4.8113; LR 1.98e-05; Iter time 0.26s; ETA 0:01:38; Mem 14467.61MB
Epoch [0/1]; Iter [200/565]; loss 4.7130; gen_loss 4.7130; LR 2.08e-05; Iter time 0.26s; ETA 0:01:35; Mem 14467.61MB
Epoch [0/1]; Iter [210/565]; loss 4.7060; gen_loss 4.7060; LR 2.18e-05; Iter time 0.26s; ETA 0:01:33; Mem 14467.61MB
Epoch [0/1]; Iter [220/565]; loss 4.7313; gen_loss 4.7313; LR 2.28e-05; Iter time 0.26s; ETA 0:01:30; Mem 14467.61MB
Epoch [0/1]; Iter [230/565]; loss 4.7456; gen_loss 4.7456; LR 2.38e-05; Iter time 0.26s; ETA 0:01:27; Mem 14467.61MB
Epoch [0/1]; Iter [240/565]; loss 4.7091; gen_loss 4.7091; LR 2.48e-05; Iter time 0.26s; ETA 0:01:25; Mem 14467.61MB
Epoch [0/1]; Iter [250/565]; loss 4.6924; gen_loss 4.6924; LR 2.58e-05; Iter time 0.26s; ETA 0:01:22; Mem 14467.61MB
Epoch [0/1]; Iter [260/565]; loss 4.6691; gen_loss 4.6691; LR 2.67e-05; Iter time 0.26s; ETA 0:01:19; Mem 14467.61MB
Epoch [0/1]; Iter [270/565]; loss 4.7137; gen_loss 4.7137; LR 2.77e-05; Iter time 0.26s; ETA 0:01:17; Mem 14467.61MB
Epoch [0/1]; Iter [280/565]; loss 4.6611; gen_loss 4.6611; LR 2.87e-05; Iter time 0.26s; ETA 0:01:14; Mem 14467.61MB
Epoch [0/1]; Iter [290/565]; loss 4.6398; gen_loss 4.6398; LR 2.97e-05; Iter time 0.26s; ETA 0:01:12; Mem 14467.61MB
Epoch [0/1]; Iter [300/565]; loss 4.5794; gen_loss 4.5794; LR 3.07e-05; Iter time 0.26s; ETA 0:01:09; Mem 14467.61MB
Epoch [0/1]; Iter [310/565]; loss 4.5947; gen_loss 4.5947; LR 3.17e-05; Iter time 0.26s; ETA 0:01:06; Mem 14467.61MB
Epoch [0/1]; Iter [320/565]; loss 4.6152; gen_loss 4.6152; LR 3.27e-05; Iter time 0.26s; ETA 0:01:04; Mem 14467.61MB
Epoch [0/1]; Iter [330/565]; loss 4.5277; gen_loss 4.5277; LR 3.37e-05; Iter time 0.26s; ETA 0:01:01; Mem 14467.61MB
Epoch [0/1]; Iter [340/565]; loss 4.3445; gen_loss 4.3445; LR 3.47e-05; Iter time 0.26s; ETA 0:00:58; Mem 14467.61MB
Epoch [0/1]; Iter [350/565]; loss 4.5443; gen_loss 4.5443; LR 3.56e-05; Iter time 0.26s; ETA 0:00:56; Mem 14467.61MB
Epoch [0/1]; Iter [360/565]; loss 4.5401; gen_loss 4.5401; LR 3.66e-05; Iter time 0.26s; ETA 0:00:53; Mem 14467.61MB
Epoch [0/1]; Iter [370/565]; loss 4.4643; gen_loss 4.4643; LR 3.76e-05; Iter time 0.26s; ETA 0:00:51; Mem 14467.61MB
Epoch [0/1]; Iter [380/565]; loss 4.5108; gen_loss 4.5108; LR 3.86e-05; Iter time 0.26s; ETA 0:00:48; Mem 14467.61MB
Epoch [0/1]; Iter [390/565]; loss 4.4216; gen_loss 4.4216; LR 3.96e-05; Iter time 0.26s; ETA 0:00:45; Mem 14467.61MB
Epoch [0/1]; Iter [400/565]; loss 4.6270; gen_loss 4.6270; LR 4.06e-05; Iter time 0.26s; ETA 0:00:43; Mem 14467.61MB
Epoch [0/1]; Iter [410/565]; loss 4.4603; gen_loss 4.4603; LR 4.16e-05; Iter time 0.26s; ETA 0:00:40; Mem 14467.61MB
Epoch [0/1]; Iter [420/565]; loss 4.4648; gen_loss 4.4648; LR 4.26e-05; Iter time 0.26s; ETA 0:00:38; Mem 14467.61MB
Epoch [0/1]; Iter [430/565]; loss 4.5191; gen_loss 4.5191; LR 4.36e-05; Iter time 0.26s; ETA 0:00:35; Mem 14467.61MB
Epoch [0/1]; Iter [440/565]; loss 4.4359; gen_loss 4.4359; LR 4.46e-05; Iter time 0.26s; ETA 0:00:32; Mem 14467.61MB
Epoch [0/1]; Iter [450/565]; loss 4.4491; gen_loss 4.4491; LR 4.56e-05; Iter time 0.26s; ETA 0:00:30; Mem 14467.61MB
Epoch [0/1]; Iter [460/565]; loss 4.4468; gen_loss 4.4468; LR 4.65e-05; Iter time 0.26s; ETA 0:00:27; Mem 14467.61MB
Epoch [0/1]; Iter [470/565]; loss 4.4839; gen_loss 4.4839; LR 4.75e-05; Iter time 0.26s; ETA 0:00:24; Mem 14467.61MB
Epoch [0/1]; Iter [480/565]; loss 4.4977; gen_loss 4.4977; LR 4.85e-05; Iter time 0.26s; ETA 0:00:22; Mem 14467.61MB
Epoch [0/1]; Iter [490/565]; loss 4.3821; gen_loss 4.3821; LR 4.95e-05; Iter time 0.26s; ETA 0:00:19; Mem 14467.61MB
Epoch [0/1]; Iter [500/565]; loss 4.4409; gen_loss 4.4409; LR 5.05e-05; Iter time 0.26s; ETA 0:00:17; Mem 14467.61MB
Epoch [0/1]; Iter [510/565]; loss 4.3755; gen_loss 4.3755; LR 5.15e-05; Iter time 0.26s; ETA 0:00:14; Mem 14467.61MB
Epoch [0/1]; Iter [520/565]; loss 4.3028; gen_loss 4.3028; LR 5.25e-05; Iter time 0.26s; ETA 0:00:11; Mem 14467.61MB
Epoch [0/1]; Iter [530/565]; loss 4.4701; gen_loss 4.4701; LR 5.35e-05; Iter time 0.26s; ETA 0:00:09; Mem 14467.61MB
Epoch [0/1]; Iter [540/565]; loss 4.5679; gen_loss 4.5679; LR 5.45e-05; Iter time 0.26s; ETA 0:00:06; Mem 14467.61MB
Epoch [0/1]; Iter [550/565]; loss 4.4569; gen_loss 4.4569; LR 5.55e-05; Iter time 0.26s; ETA 0:00:03; Mem 14467.61MB
Epoch [0/1]; Iter [560/565]; loss 4.6279; gen_loss 4.6279; LR 5.64e-05; Iter time 0.26s; ETA 0:00:01; Mem 14467.61MB
====================
Evaluate Epoch [0/1]
====================
Evaluate ; Batch [0/62]; perplexity: 125.1292; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [10/62]; perplexity: 86.5312; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [20/62]; perplexity: 82.3846; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [30/62]; perplexity: 85.3110; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [40/62]; perplexity: 83.0897; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [50/62]; perplexity: 84.1789; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
Evaluate ; Batch [60/62]; perplexity: 84.0575; Evaluating on iter: 564; Iter time 0.08; Mem 14467.61MB
	[train]	trunk.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	trunk.decoder.embed_positions.weight:	torch.Size([8002, 256])
	[train]	trunk.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	heads.0.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.0.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.0.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.0.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.0.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.0.0.linear1.bias:	torch.Size([1024])
	[train]	heads.0.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.0.0.linear2.bias:	torch.Size([256])
	[train]	heads.0.0.norm1.weight:	torch.Size([256])
	[train]	heads.0.0.norm1.bias:	torch.Size([256])
	[train]	heads.0.0.norm2.weight:	torch.Size([256])
	[train]	heads.0.0.norm2.bias:	torch.Size([256])
	[train]	heads.0.1.weight:	torch.Size([256])
	[train]	heads.0.1.bias:	torch.Size([256])
	[train]	heads.0.2.weight:	torch.Size([131, 256])
	[train]	heads.0.2.bias:	torch.Size([131])
	[train]	heads.1.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.1.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.1.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.1.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.1.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.1.0.linear1.bias:	torch.Size([1024])
	[train]	heads.1.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.1.0.linear2.bias:	torch.Size([256])
	[train]	heads.1.0.norm1.weight:	torch.Size([256])
	[train]	heads.1.0.norm1.bias:	torch.Size([256])
	[train]	heads.1.0.norm2.weight:	torch.Size([256])
	[train]	heads.1.0.norm2.bias:	torch.Size([256])
	[train]	heads.1.1.weight:	torch.Size([256])
	[train]	heads.1.1.bias:	torch.Size([256])
	[train]	heads.1.2.weight:	torch.Size([131, 256])
	[train]	heads.1.2.bias:	torch.Size([131])
	[train]	offset_embeddings.weight:	torch.Size([2, 256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl_mtp', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=1, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=False, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MTPMeshXL(
  (tokenizer): MeshTokenizer()
  (trunk): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(131, 256, padding_idx=130)
      (embed_positions): OPTLearnedPositionalEmbedding(8002, 256)
      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-3): 4 x OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (heads): ModuleList(
    (0-1): 2 x Sequential(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=256, out_features=131, bias=True)
    )
  )
  (offset_embeddings): Embedding(2, 256)
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[eval]	transformer.model.decoder.embed_positions.weight:	torch.Size([7302, 512])
	[eval]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[eval]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[eval]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[eval]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[eval]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[eval]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=1, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=1)
        (embed_positions): OPTLearnedPositionalEmbedding(7302, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[eval]	trunk.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[eval]	trunk.decoder.embed_positions.weight:	torch.Size([7302, 256])
	[eval]	trunk.decoder.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.0.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.1.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.2.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	heads.0.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.0.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.0.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.0.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.0.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.0.0.linear1.bias:	torch.Size([1024])
	[train]	heads.0.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.0.0.linear2.bias:	torch.Size([256])
	[train]	heads.0.0.norm1.weight:	torch.Size([256])
	[train]	heads.0.0.norm1.bias:	torch.Size([256])
	[train]	heads.0.0.norm2.weight:	torch.Size([256])
	[train]	heads.0.0.norm2.bias:	torch.Size([256])
	[train]	heads.0.1.weight:	torch.Size([256])
	[train]	heads.0.1.bias:	torch.Size([256])
	[train]	heads.0.2.weight:	torch.Size([131, 256])
	[train]	heads.0.2.bias:	torch.Size([131])
	[train]	heads.1.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.1.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.1.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.1.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.1.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.1.0.linear1.bias:	torch.Size([1024])
	[train]	heads.1.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.1.0.linear2.bias:	torch.Size([256])
	[train]	heads.1.0.norm1.weight:	torch.Size([256])
	[train]	heads.1.0.norm1.bias:	torch.Size([256])
	[train]	heads.1.0.norm2.weight:	torch.Size([256])
	[train]	heads.1.0.norm2.bias:	torch.Size([256])
	[train]	heads.1.1.weight:	torch.Size([256])
	[train]	heads.1.1.bias:	torch.Size([256])
	[train]	heads.1.2.weight:	torch.Size([131, 256])
	[train]	heads.1.2.bias:	torch.Size([131])
	[train]	offset_embeddings.weight:	torch.Size([2, 256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl_mtp', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=1, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MTPMeshXL(
  (tokenizer): MeshTokenizer()
  (trunk): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(131, 256, padding_idx=1)
      (embed_positions): OPTLearnedPositionalEmbedding(7302, 256)
      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-3): 4 x OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (heads): ModuleList(
    (0-1): 2 x Sequential(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=256, out_features=131, bias=True)
    )
  )
  (offset_embeddings): Embedding(2, 256)
)
	[eval]	trunk.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[eval]	trunk.decoder.embed_positions.weight:	torch.Size([7302, 256])
	[eval]	trunk.decoder.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.0.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.1.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[eval]	trunk.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[eval]	trunk.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[eval]	trunk.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[eval]	trunk.decoder.layers.2.fc2.bias:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[eval]	trunk.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	trunk.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	trunk.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	trunk.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	trunk.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	trunk.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	heads.0.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.0.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.0.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.0.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.0.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.0.0.linear1.bias:	torch.Size([1024])
	[train]	heads.0.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.0.0.linear2.bias:	torch.Size([256])
	[train]	heads.0.0.norm1.weight:	torch.Size([256])
	[train]	heads.0.0.norm1.bias:	torch.Size([256])
	[train]	heads.0.0.norm2.weight:	torch.Size([256])
	[train]	heads.0.0.norm2.bias:	torch.Size([256])
	[train]	heads.0.1.weight:	torch.Size([256])
	[train]	heads.0.1.bias:	torch.Size([256])
	[train]	heads.0.2.weight:	torch.Size([131, 256])
	[train]	heads.0.2.bias:	torch.Size([131])
	[train]	heads.1.0.self_attn.in_proj_weight:	torch.Size([768, 256])
	[train]	heads.1.0.self_attn.in_proj_bias:	torch.Size([768])
	[train]	heads.1.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	heads.1.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	heads.1.0.linear1.weight:	torch.Size([1024, 256])
	[train]	heads.1.0.linear1.bias:	torch.Size([1024])
	[train]	heads.1.0.linear2.weight:	torch.Size([256, 1024])
	[train]	heads.1.0.linear2.bias:	torch.Size([256])
	[train]	heads.1.0.norm1.weight:	torch.Size([256])
	[train]	heads.1.0.norm1.bias:	torch.Size([256])
	[train]	heads.1.0.norm2.weight:	torch.Size([256])
	[train]	heads.1.0.norm2.bias:	torch.Size([256])
	[train]	heads.1.1.weight:	torch.Size([256])
	[train]	heads.1.1.bias:	torch.Size([256])
	[train]	heads.1.2.weight:	torch.Size([131, 256])
	[train]	heads.1.2.bias:	torch.Size([131])
	[train]	offset_embeddings.weight:	torch.Size([2, 256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='shapenet_lamp', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl_mtp', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=1, start_eval_after=-1, eval_every_iteration=4000, seed=0, finetune=True, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./output-samples-125m', save_every=20000, log_every=10)
MTPMeshXL(
  (tokenizer): MeshTokenizer()
  (trunk): OPTModel(
    (decoder): OPTDecoder(
      (embed_tokens): Embedding(131, 256, padding_idx=1)
      (embed_positions): OPTLearnedPositionalEmbedding(7302, 256)
      (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (layers): ModuleList(
        (0-3): 4 x OPTDecoderLayer(
          (self_attn): OPTAttention(
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
  (heads): ModuleList(
    (0-1): 2 x Sequential(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=1024, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Linear(in_features=256, out_features=131, bias=True)
    )
  )
  (offset_embeddings): Embedding(2, 256)
)
Epoch [0/1]; Iter [0/565]; loss 5.1690; LR 1.00e-06; Iter time 0.32s; ETA 0:02:58; Mem 4039.94MB
Epoch [0/1]; Iter [10/565]; loss 5.0388; LR 1.99e-06; Iter time 0.06s; ETA 0:00:32; Mem 4058.54MB
Epoch [0/1]; Iter [20/565]; loss 5.0552; LR 2.98e-06; Iter time 0.06s; ETA 0:00:32; Mem 4058.54MB
Epoch [0/1]; Iter [30/565]; loss 4.9689; LR 3.97e-06; Iter time 0.06s; ETA 0:00:32; Mem 4058.54MB
Epoch [0/1]; Iter [40/565]; loss 4.9716; LR 4.96e-06; Iter time 0.06s; ETA 0:00:31; Mem 4058.54MB
Epoch [0/1]; Iter [50/565]; loss 4.9644; LR 5.95e-06; Iter time 0.06s; ETA 0:00:30; Mem 4058.54MB
Epoch [0/1]; Iter [60/565]; loss 4.8763; LR 6.94e-06; Iter time 0.06s; ETA 0:00:30; Mem 4058.54MB
Epoch [0/1]; Iter [70/565]; loss 4.8393; LR 7.93e-06; Iter time 0.06s; ETA 0:00:29; Mem 4058.54MB
Epoch [0/1]; Iter [80/565]; loss 4.8650; LR 8.92e-06; Iter time 0.06s; ETA 0:00:29; Mem 4058.54MB
Epoch [0/1]; Iter [90/565]; loss 4.7768; LR 9.91e-06; Iter time 0.06s; ETA 0:00:28; Mem 4058.54MB
Epoch [0/1]; Iter [100/565]; loss 4.7959; LR 1.09e-05; Iter time 0.06s; ETA 0:00:27; Mem 4058.54MB
Epoch [0/1]; Iter [110/565]; loss 4.7004; LR 1.19e-05; Iter time 0.06s; ETA 0:00:27; Mem 4058.54MB
Epoch [0/1]; Iter [120/565]; loss 4.7109; LR 1.29e-05; Iter time 0.06s; ETA 0:00:26; Mem 4058.54MB
Epoch [0/1]; Iter [130/565]; loss 4.7036; LR 1.39e-05; Iter time 0.06s; ETA 0:00:25; Mem 4058.54MB
Epoch [0/1]; Iter [140/565]; loss 4.7382; LR 1.49e-05; Iter time 0.06s; ETA 0:00:25; Mem 4058.54MB
Epoch [0/1]; Iter [150/565]; loss 4.4372; LR 1.58e-05; Iter time 0.06s; ETA 0:00:24; Mem 4058.54MB
Epoch [0/1]; Iter [160/565]; loss 4.5301; LR 1.68e-05; Iter time 0.06s; ETA 0:00:24; Mem 4058.54MB
Epoch [0/1]; Iter [170/565]; loss 4.5768; LR 1.78e-05; Iter time 0.06s; ETA 0:00:23; Mem 4058.54MB
Epoch [0/1]; Iter [180/565]; loss 4.5786; LR 1.88e-05; Iter time 0.06s; ETA 0:00:23; Mem 4058.54MB
Epoch [0/1]; Iter [190/565]; loss 4.3653; LR 1.98e-05; Iter time 0.06s; ETA 0:00:22; Mem 4058.54MB
Epoch [0/1]; Iter [200/565]; loss 4.6071; LR 2.08e-05; Iter time 0.06s; ETA 0:00:21; Mem 4058.54MB
Epoch [0/1]; Iter [210/565]; loss 4.5965; LR 2.18e-05; Iter time 0.06s; ETA 0:00:21; Mem 4058.54MB
Epoch [0/1]; Iter [220/565]; loss 4.4378; LR 2.28e-05; Iter time 0.06s; ETA 0:00:20; Mem 4058.54MB
Epoch [0/1]; Iter [230/565]; loss 4.5848; LR 2.38e-05; Iter time 0.06s; ETA 0:00:19; Mem 4058.54MB
Epoch [0/1]; Iter [240/565]; loss 4.3780; LR 2.48e-05; Iter time 0.06s; ETA 0:00:19; Mem 4058.54MB
Epoch [0/1]; Iter [250/565]; loss 4.5884; LR 2.58e-05; Iter time 0.06s; ETA 0:00:18; Mem 4058.54MB
Epoch [0/1]; Iter [260/565]; loss 4.4531; LR 2.67e-05; Iter time 0.06s; ETA 0:00:18; Mem 4058.54MB
Epoch [0/1]; Iter [270/565]; loss 4.5504; LR 2.77e-05; Iter time 0.06s; ETA 0:00:17; Mem 4058.54MB
Epoch [0/1]; Iter [280/565]; loss 4.4897; LR 2.87e-05; Iter time 0.06s; ETA 0:00:16; Mem 4058.54MB
Epoch [0/1]; Iter [290/565]; loss 4.5020; LR 2.97e-05; Iter time 0.06s; ETA 0:00:16; Mem 4058.54MB
Epoch [0/1]; Iter [300/565]; loss 4.4945; LR 3.07e-05; Iter time 0.06s; ETA 0:00:15; Mem 4058.54MB
Epoch [0/1]; Iter [310/565]; loss 4.5791; LR 3.17e-05; Iter time 0.06s; ETA 0:00:15; Mem 4058.54MB
Epoch [0/1]; Iter [320/565]; loss 4.3747; LR 3.27e-05; Iter time 0.06s; ETA 0:00:14; Mem 4058.54MB
Epoch [0/1]; Iter [330/565]; loss 4.4417; LR 3.37e-05; Iter time 0.06s; ETA 0:00:14; Mem 4058.54MB
Epoch [0/1]; Iter [340/565]; loss 4.3176; LR 3.47e-05; Iter time 0.06s; ETA 0:00:13; Mem 4058.54MB
Epoch [0/1]; Iter [350/565]; loss 4.5346; LR 3.56e-05; Iter time 0.06s; ETA 0:00:12; Mem 4058.54MB
Epoch [0/1]; Iter [360/565]; loss 4.4192; LR 3.66e-05; Iter time 0.06s; ETA 0:00:12; Mem 4058.54MB
Epoch [0/1]; Iter [370/565]; loss 4.3943; LR 3.76e-05; Iter time 0.06s; ETA 0:00:11; Mem 4058.54MB
Epoch [0/1]; Iter [380/565]; loss 4.4131; LR 3.86e-05; Iter time 0.06s; ETA 0:00:11; Mem 4058.54MB
Epoch [0/1]; Iter [390/565]; loss 4.6129; LR 3.96e-05; Iter time 0.06s; ETA 0:00:10; Mem 4058.54MB
Epoch [0/1]; Iter [400/565]; loss 4.5065; LR 4.06e-05; Iter time 0.06s; ETA 0:00:09; Mem 4058.54MB
Epoch [0/1]; Iter [410/565]; loss 4.3374; LR 4.16e-05; Iter time 0.06s; ETA 0:00:09; Mem 4058.54MB
Epoch [0/1]; Iter [420/565]; loss 4.4549; LR 4.26e-05; Iter time 0.06s; ETA 0:00:08; Mem 4058.54MB
Epoch [0/1]; Iter [430/565]; loss 4.3690; LR 4.36e-05; Iter time 0.06s; ETA 0:00:08; Mem 4058.54MB
Epoch [0/1]; Iter [440/565]; loss 4.4904; LR 4.46e-05; Iter time 0.06s; ETA 0:00:07; Mem 4058.54MB
Epoch [0/1]; Iter [450/565]; loss 4.2940; LR 4.56e-05; Iter time 0.06s; ETA 0:00:06; Mem 4058.54MB
Epoch [0/1]; Iter [460/565]; loss 4.4607; LR 4.65e-05; Iter time 0.06s; ETA 0:00:06; Mem 4058.54MB
Epoch [0/1]; Iter [470/565]; loss 4.3286; LR 4.75e-05; Iter time 0.06s; ETA 0:00:05; Mem 4058.54MB
Epoch [0/1]; Iter [480/565]; loss 4.4255; LR 4.85e-05; Iter time 0.06s; ETA 0:00:05; Mem 4058.54MB
Epoch [0/1]; Iter [490/565]; loss 4.3458; LR 4.95e-05; Iter time 0.06s; ETA 0:00:04; Mem 4058.54MB
Epoch [0/1]; Iter [500/565]; loss 4.4474; LR 5.05e-05; Iter time 0.06s; ETA 0:00:03; Mem 4058.54MB
Epoch [0/1]; Iter [510/565]; loss 4.3405; LR 5.15e-05; Iter time 0.06s; ETA 0:00:03; Mem 4058.54MB
Epoch [0/1]; Iter [520/565]; loss 4.2528; LR 5.25e-05; Iter time 0.06s; ETA 0:00:02; Mem 4058.54MB
Epoch [0/1]; Iter [530/565]; loss 4.5262; LR 5.35e-05; Iter time 0.06s; ETA 0:00:02; Mem 4058.54MB
Epoch [0/1]; Iter [540/565]; loss 4.2408; LR 5.45e-05; Iter time 0.06s; ETA 0:00:01; Mem 4058.54MB
Epoch [0/1]; Iter [550/565]; loss 4.4635; LR 5.55e-05; Iter time 0.06s; ETA 0:00:00; Mem 4058.54MB
Epoch [0/1]; Iter [560/565]; loss 4.3138; LR 5.64e-05; Iter time 0.06s; ETA 0:00:00; Mem 4058.54MB
====================
Evaluate Epoch [0/1]
====================
