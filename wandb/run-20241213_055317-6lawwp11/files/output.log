Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
/root/MeshXL/utils/io.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(last_checkpoint, map_location=torch.device("cpu"))
Found checkpoint at 16. Resuming.
Loaded model and optimizer state at 16. Loaded best val metrics so far.
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=1, batchsize_per_gpu=1, start_epoch=17, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-7): 8 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
Epoch [17/100]; Iter [8760/51500]; loss 1.6791; gen_loss 1.6791; LR 9.31e-05; Iter time 0.40s; ETA 4:44:59; Mem 18775.83MB
Epoch [17/100]; Iter [8770/51500]; loss 1.6337; gen_loss 1.6337; LR 9.31e-05; Iter time 0.36s; ETA 4:19:12; Mem 18775.83MB
Epoch [17/100]; Iter [8780/51500]; loss 1.3703; gen_loss 1.3703; LR 9.31e-05; Iter time 0.36s; ETA 4:19:35; Mem 18775.83MB
Epoch [17/100]; Iter [8790/51500]; loss 1.3753; gen_loss 1.3753; LR 9.31e-05; Iter time 0.36s; ETA 4:19:28; Mem 18775.83MB
Epoch [17/100]; Iter [8800/51500]; loss 0.9408; gen_loss 0.9408; LR 9.30e-05; Iter time 0.36s; ETA 4:19:45; Mem 18775.83MB
Epoch [17/100]; Iter [8810/51500]; loss 1.2679; gen_loss 1.2679; LR 9.30e-05; Iter time 0.36s; ETA 4:19:34; Mem 18775.83MB
Epoch [17/100]; Iter [8820/51500]; loss 1.6388; gen_loss 1.6388; LR 9.30e-05; Iter time 0.36s; ETA 4:19:31; Mem 18775.83MB
Epoch [17/100]; Iter [8830/51500]; loss 1.5244; gen_loss 1.5244; LR 9.30e-05; Iter time 0.37s; ETA 4:19:41; Mem 18775.83MB
Epoch [17/100]; Iter [8840/51500]; loss 1.4697; gen_loss 1.4697; LR 9.30e-05; Iter time 0.37s; ETA 4:19:31; Mem 18775.83MB
Epoch [17/100]; Iter [8850/51500]; loss 1.9759; gen_loss 1.9759; LR 9.30e-05; Iter time 0.37s; ETA 4:19:54; Mem 18775.83MB
Epoch [17/100]; Iter [8860/51500]; loss 1.5191; gen_loss 1.5191; LR 9.29e-05; Iter time 0.37s; ETA 4:20:18; Mem 18775.83MB
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 315, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 305, in main
    do_train(
  File "/root/MeshXL/engine.py", line 74, in do_train
    accelerator.backward(loss)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 2244, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
