Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Loss: 4.809508323669434
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0431, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.67s; ETA 5:56:25; Mem 4290.41MB
Loss: 4.932173728942871
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 5.031808376312256
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.920366287231445
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.816051483154297
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.988270282745361
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.905907154083252
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.963059425354004
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.88218355178833
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.874852657318115
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0090, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.932605266571045
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0199,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.53s; ETA 1:53:01; Mem 4290.41MB
Loss: 4.872658729553223
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.950222015380859
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.858727931976318
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.861661911010742
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.907061576843262
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0002],
        [ 0.0366, -0.0110, -0.0104, -0.0309, -0.0196]], device='cuda:0')
Loss: 4.899861812591553
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.8330397605896
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0175],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.829042911529541
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.857235431671143
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.758286476135254
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 0.53s; ETA 1:53:14; Mem 4290.41MB
Loss: 4.974184036254883
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0131,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.856851100921631
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.628574848175049
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.907247543334961
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.770282745361328
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.706572532653809
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.945944786071777
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.632603168487549
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.589450836181641
First 5 weights of embed_tokens: tensor([[ 0.0292, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0309, -0.0197]], device='cuda:0')
Loss: 4.865379810333252
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0308, -0.0197]], device='cuda:0')
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 0.53s; ETA 1:52:43; Mem 4290.41MB
Loss: 4.662661075592041
First 5 weights of embed_tokens: tensor([[ 0.0293, -0.0007,  0.0045,  0.0080, -0.0041],
        [ 0.0277,  0.0322, -0.0017,  0.0012,  0.0487],
        [ 0.0250, -0.0132,  0.0052, -0.0430, -0.0176],
        [-0.0150,  0.0089, -0.0200,  0.0130,  0.0001],
        [ 0.0366, -0.0111, -0.0104, -0.0308, -0.0197]], device='cuda:0')
Loss: 4.77130126953125
First 5 weights of embed_tokens: tensor([[ 2.9251e-02, -7.4758e-04,  4.4654e-03,  8.0053e-03, -4.0803e-03],
        [ 2.7711e-02,  3.2159e-02, -1.7406e-03,  1.1985e-03,  4.8672e-02],
        [ 2.4964e-02, -1.3185e-02,  5.1746e-03, -4.2978e-02, -1.7594e-02],
        [-1.5033e-02,  8.8845e-03, -1.9999e-02,  1.3028e-02,  9.7891e-05],
        [ 3.6577e-02, -1.1103e-02, -1.0439e-02, -3.0844e-02, -1.9701e-02]],
       device='cuda:0')
Loss: 4.73277473449707
First 5 weights of embed_tokens: tensor([[ 2.9252e-02, -7.4944e-04,  4.4642e-03,  8.0035e-03, -4.0788e-03],
        [ 2.7709e-02,  3.2155e-02, -1.7415e-03,  1.2009e-03,  4.8669e-02],
        [ 2.4961e-02, -1.3189e-02,  5.1720e-03, -4.2975e-02, -1.7597e-02],
        [-1.5036e-02,  8.8806e-03, -2.0001e-02,  1.3031e-02,  9.4280e-05],
        [ 3.6577e-02, -1.1106e-02, -1.0442e-02, -3.0842e-02, -1.9704e-02]],
       device='cuda:0')
Loss: 4.594331741333008
First 5 weights of embed_tokens: tensor([[ 2.9254e-02, -7.5120e-04,  4.4629e-03,  8.0015e-03, -4.0768e-03],
        [ 2.7706e-02,  3.2151e-02, -1.7423e-03,  1.2033e-03,  4.8666e-02],
        [ 2.4957e-02, -1.3192e-02,  5.1696e-03, -4.2972e-02, -1.7600e-02],
        [-1.5039e-02,  8.8766e-03, -2.0002e-02,  1.3034e-02,  9.0649e-05],
        [ 3.6576e-02, -1.1109e-02, -1.0444e-02, -3.0840e-02, -1.9707e-02]],
       device='cuda:0')
Loss: 4.583138465881348
First 5 weights of embed_tokens: tensor([[ 2.9255e-02, -7.5300e-04,  4.4617e-03,  7.9998e-03, -4.0750e-03],
        [ 2.7703e-02,  3.2148e-02, -1.7432e-03,  1.2057e-03,  4.8663e-02],
        [ 2.4953e-02, -1.3196e-02,  5.1672e-03, -4.2969e-02, -1.7603e-02],
        [-1.5043e-02,  8.8725e-03, -2.0004e-02,  1.3038e-02,  8.7065e-05],
        [ 3.6574e-02, -1.1113e-02, -1.0447e-02, -3.0837e-02, -1.9710e-02]],
       device='cuda:0')
Loss: 4.525669574737549
First 5 weights of embed_tokens: tensor([[ 2.9255e-02, -7.5487e-04,  4.4606e-03,  7.9983e-03, -4.0734e-03],
        [ 2.7699e-02,  3.2144e-02, -1.7440e-03,  1.2082e-03,  4.8660e-02],
        [ 2.4948e-02, -1.3200e-02,  5.1649e-03, -4.2966e-02, -1.7606e-02],
        [-1.5047e-02,  8.8683e-03, -2.0006e-02,  1.3041e-02,  8.3400e-05],
        [ 3.6572e-02, -1.1117e-02, -1.0449e-02, -3.0835e-02, -1.9712e-02]],
       device='cuda:0')
Loss: 4.662109375
First 5 weights of embed_tokens: tensor([[ 2.9256e-02, -7.5688e-04,  4.4579e-03,  7.9956e-03, -4.0708e-03],
        [ 2.7695e-02,  3.2140e-02, -1.7448e-03,  1.2106e-03,  4.8656e-02],
        [ 2.4944e-02, -1.3204e-02,  5.1626e-03, -4.2963e-02, -1.7610e-02],
        [-1.5051e-02,  8.8640e-03, -2.0007e-02,  1.3044e-02,  7.9695e-05],
        [ 3.6570e-02, -1.1120e-02, -1.0451e-02, -3.0833e-02, -1.9715e-02]],
       device='cuda:0')
Loss: 4.734846591949463
First 5 weights of embed_tokens: tensor([[ 2.9257e-02, -7.5892e-04,  4.4555e-03,  7.9932e-03, -4.0685e-03],
        [ 2.7691e-02,  3.2135e-02, -1.7455e-03,  1.2132e-03,  4.8653e-02],
        [ 2.4939e-02, -1.3209e-02,  5.1607e-03, -4.2961e-02, -1.7613e-02],
        [-1.5055e-02,  8.8596e-03, -2.0008e-02,  1.3047e-02,  7.5888e-05],
        [ 3.6567e-02, -1.1124e-02, -1.0453e-02, -3.0830e-02, -1.9718e-02]],
       device='cuda:0')
Loss: 4.701282978057861
First 5 weights of embed_tokens: tensor([[ 2.9258e-02, -7.6114e-04,  4.4532e-03,  7.9911e-03, -4.0665e-03],
        [ 2.7686e-02,  3.2131e-02, -1.7463e-03,  1.2158e-03,  4.8649e-02],
        [ 2.4933e-02, -1.3213e-02,  5.1586e-03, -4.2957e-02, -1.7616e-02],
        [-1.5060e-02,  8.8549e-03, -2.0010e-02,  1.3050e-02,  7.2075e-05],
        [ 3.6564e-02, -1.1129e-02, -1.0455e-02, -3.0828e-02, -1.9721e-02]],
       device='cuda:0')
Loss: 4.735190391540527
First 5 weights of embed_tokens: tensor([[ 2.9258e-02, -7.6342e-04,  4.4511e-03,  7.9893e-03, -4.0646e-03],
        [ 2.7681e-02,  3.2126e-02, -1.7469e-03,  1.2184e-03,  4.8646e-02],
        [ 2.4927e-02, -1.3218e-02,  5.1570e-03, -4.2954e-02, -1.7620e-02],
        [-1.5066e-02,  8.8502e-03, -2.0011e-02,  1.3053e-02,  6.8424e-05],
        [ 3.6560e-02, -1.1133e-02, -1.0457e-02, -3.0825e-02, -1.9724e-02]],
       device='cuda:0')
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 0.53s; ETA 1:52:29; Mem 4290.41MB
Loss: 4.678443908691406
First 5 weights of embed_tokens: tensor([[ 2.9259e-02, -7.6565e-04,  4.4493e-03,  7.9876e-03, -4.0629e-03],
        [ 2.7675e-02,  3.2121e-02, -1.7472e-03,  1.2210e-03,  4.8643e-02],
        [ 2.4921e-02, -1.3222e-02,  5.1559e-03, -4.2951e-02, -1.7623e-02],
        [-1.5072e-02,  8.8454e-03, -2.0011e-02,  1.3056e-02,  6.4846e-05],
        [ 3.6556e-02, -1.1138e-02, -1.0458e-02, -3.0823e-02, -1.9727e-02]],
       device='cuda:0')
Loss: 4.244754791259766
First 5 weights of embed_tokens: tensor([[ 2.9258e-02, -7.6797e-04,  4.4478e-03,  7.9862e-03, -4.0614e-03],
        [ 2.7669e-02,  3.2117e-02, -1.7473e-03,  1.2236e-03,  4.8639e-02],
        [ 2.4915e-02, -1.3227e-02,  5.1553e-03, -4.2948e-02, -1.7626e-02],
        [-1.5078e-02,  8.8405e-03, -2.0011e-02,  1.3059e-02,  6.1373e-05],
        [ 3.6551e-02, -1.1142e-02, -1.0458e-02, -3.0820e-02, -1.9730e-02]],
       device='cuda:0')
Loss: 4.75648307800293
First 5 weights of embed_tokens: tensor([[ 2.9258e-02, -7.7033e-04,  4.4464e-03,  7.9849e-03, -4.0601e-03],
        [ 2.7663e-02,  3.2112e-02, -1.7474e-03,  1.2262e-03,  4.8636e-02],
        [ 2.4908e-02, -1.3232e-02,  5.1544e-03, -4.2945e-02, -1.7629e-02],
        [-1.5084e-02,  8.8356e-03, -2.0011e-02,  1.3063e-02,  5.8071e-05],
        [ 3.6546e-02, -1.1147e-02, -1.0459e-02, -3.0817e-02, -1.9733e-02]],
       device='cuda:0')
Loss: 4.7476606369018555
First 5 weights of embed_tokens: tensor([[ 2.9258e-02, -7.7274e-04,  4.4451e-03,  7.9838e-03, -4.0589e-03],
        [ 2.7657e-02,  3.2107e-02, -1.7474e-03,  1.2288e-03,  4.8633e-02],
        [ 2.4901e-02, -1.3237e-02,  5.1540e-03, -4.2942e-02, -1.7632e-02],
        [-1.5090e-02,  8.8305e-03, -2.0011e-02,  1.3066e-02,  5.4647e-05],
        [ 3.6541e-02, -1.1152e-02, -1.0459e-02, -3.0815e-02, -1.9735e-02]],
       device='cuda:0')
Loss: 4.663111686706543
First 5 weights of embed_tokens: tensor([[ 2.9257e-02, -7.7509e-04,  4.4442e-03,  7.9829e-03, -4.0579e-03],
        [ 2.7650e-02,  3.2102e-02, -1.7469e-03,  1.2314e-03,  4.8630e-02],
        [ 2.4894e-02, -1.3242e-02,  5.1544e-03, -4.2939e-02, -1.7635e-02],
        [-1.5097e-02,  8.8255e-03, -2.0009e-02,  1.3069e-02,  5.1275e-05],
        [ 3.6535e-02, -1.1157e-02, -1.0459e-02, -3.0812e-02, -1.9738e-02]],
       device='cuda:0')
Loss: 4.5094828605651855
First 5 weights of embed_tokens: tensor([[ 2.9256e-02, -7.7747e-04,  4.4436e-03,  7.9820e-03, -4.0571e-03],
        [ 2.7643e-02,  3.2096e-02, -1.7461e-03,  1.2338e-03,  4.8627e-02],
        [ 2.4887e-02, -1.3247e-02,  5.1554e-03, -4.2936e-02, -1.7639e-02],
        [-1.5105e-02,  8.8205e-03, -2.0007e-02,  1.3072e-02,  4.7719e-05],
        [ 3.6529e-02, -1.1162e-02, -1.0458e-02, -3.0809e-02, -1.9741e-02]],
       device='cuda:0')
Loss: 4.610309600830078
First 5 weights of embed_tokens: tensor([[ 2.9255e-02, -7.7995e-04,  4.4429e-03,  7.9813e-03, -4.0564e-03],
        [ 2.7636e-02,  3.2091e-02, -1.7454e-03,  1.2362e-03,  4.8624e-02],
        [ 2.4880e-02, -1.3252e-02,  5.1562e-03, -4.2933e-02, -1.7642e-02],
        [-1.5111e-02,  8.8154e-03, -2.0006e-02,  1.3074e-02,  4.4298e-05],
        [ 3.6523e-02, -1.1167e-02, -1.0457e-02, -3.0807e-02, -1.9744e-02]],
       device='cuda:0')
Loss: 4.61173152923584
First 5 weights of embed_tokens: tensor([[ 2.9254e-02, -7.8255e-04,  4.4423e-03,  7.9807e-03, -4.0557e-03],
        [ 2.7629e-02,  3.2086e-02, -1.7450e-03,  1.2387e-03,  4.8621e-02],
        [ 2.4873e-02, -1.3257e-02,  5.1565e-03, -4.2930e-02, -1.7645e-02],
        [-1.5118e-02,  8.8100e-03, -2.0005e-02,  1.3077e-02,  4.1032e-05],
        [ 3.6517e-02, -1.1172e-02, -1.0457e-02, -3.0804e-02, -1.9747e-02]],
       device='cuda:0')
Loss: 4.737344741821289
First 5 weights of embed_tokens: tensor([[ 2.9253e-02, -7.8513e-04,  4.4418e-03,  7.9803e-03, -4.0552e-03],
        [ 2.7623e-02,  3.2080e-02, -1.7443e-03,  1.2411e-03,  4.8618e-02],
        [ 2.4866e-02, -1.3262e-02,  5.1572e-03, -4.2928e-02, -1.7648e-02],
        [-1.5125e-02,  8.8047e-03, -2.0003e-02,  1.3080e-02,  3.7882e-05],
        [ 3.6512e-02, -1.1177e-02, -1.0456e-02, -3.0802e-02, -1.9750e-02]],
       device='cuda:0')
Loss: 4.9607062339782715
First 5 weights of embed_tokens: tensor([[ 2.9251e-02, -7.8787e-04,  4.4413e-03,  7.9798e-03, -4.0548e-03],
        [ 2.7616e-02,  3.2075e-02, -1.7438e-03,  1.2433e-03,  4.8615e-02],
        [ 2.4859e-02, -1.3268e-02,  5.1576e-03, -4.2925e-02, -1.7651e-02],
        [-1.5132e-02,  8.7993e-03, -2.0002e-02,  1.3083e-02,  3.4849e-05],
        [ 3.6505e-02, -1.1183e-02, -1.0456e-02, -3.0799e-02, -1.9753e-02]],
       device='cuda:0')
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 0.54s; ETA 1:55:18; Mem 4290.41MB
Loss: 4.479537487030029
First 5 weights of embed_tokens: tensor([[ 2.9250e-02, -7.9059e-04,  4.4409e-03,  7.9795e-03, -4.0544e-03],
        [ 2.7609e-02,  3.2069e-02, -1.7432e-03,  1.2456e-03,  4.8612e-02],
        [ 2.4851e-02, -1.3273e-02,  5.1584e-03, -4.2922e-02, -1.7654e-02],
        [-1.5139e-02,  8.7938e-03, -2.0000e-02,  1.3085e-02,  3.1899e-05],
        [ 3.6499e-02, -1.1188e-02, -1.0455e-02, -3.0797e-02, -1.9755e-02]],
       device='cuda:0')
Loss: 4.755331993103027
First 5 weights of embed_tokens: tensor([[ 2.9249e-02, -7.9206e-04,  4.4377e-03,  7.9793e-03, -4.0537e-03],
        [ 2.7601e-02,  3.2064e-02, -1.7423e-03,  1.2478e-03,  4.8609e-02],
        [ 2.4844e-02, -1.3279e-02,  5.1596e-03, -4.2919e-02, -1.7657e-02],
        [-1.5147e-02,  8.7883e-03, -1.9998e-02,  1.3088e-02,  2.8850e-05],
        [ 3.6492e-02, -1.1193e-02, -1.0453e-02, -3.0795e-02, -1.9758e-02]],
       device='cuda:0')
Loss: 4.609934329986572
First 5 weights of embed_tokens: tensor([[ 2.9248e-02, -7.9369e-04,  4.4349e-03,  7.9790e-03, -4.0532e-03],
        [ 2.7593e-02,  3.2058e-02, -1.7410e-03,  1.2498e-03,  4.8606e-02],
        [ 2.4836e-02, -1.3284e-02,  5.1614e-03, -4.2917e-02, -1.7660e-02],
        [-1.5155e-02,  8.7827e-03, -1.9996e-02,  1.3090e-02,  2.5838e-05],
        [ 3.6486e-02, -1.1199e-02, -1.0451e-02, -3.0792e-02, -1.9761e-02]],
       device='cuda:0')
Loss: 4.634098052978516
First 5 weights of embed_tokens: tensor([[ 2.9247e-02, -7.9536e-04,  4.4324e-03,  7.9788e-03, -4.0526e-03],
        [ 2.7585e-02,  3.2052e-02, -1.7397e-03,  1.2516e-03,  4.8604e-02],
        [ 2.4828e-02, -1.3290e-02,  5.1632e-03, -4.2915e-02, -1.7662e-02],
        [-1.5162e-02,  8.7771e-03, -1.9993e-02,  1.3092e-02,  2.2930e-05],
        [ 3.6479e-02, -1.1204e-02, -1.0450e-02, -3.0790e-02, -1.9763e-02]],
       device='cuda:0')
Loss: 4.886036396026611
First 5 weights of embed_tokens: tensor([[ 2.9246e-02, -7.9720e-04,  4.4302e-03,  7.9786e-03, -4.0523e-03],
        [ 2.7578e-02,  3.2046e-02, -1.7380e-03,  1.2534e-03,  4.8601e-02],
        [ 2.4820e-02, -1.3295e-02,  5.1655e-03, -4.2913e-02, -1.7665e-02],
        [-1.5170e-02,  8.7715e-03, -1.9990e-02,  1.3094e-02,  1.9841e-05],
        [ 3.6472e-02, -1.1210e-02, -1.0447e-02, -3.0788e-02, -1.9766e-02]],
       device='cuda:0')
Loss: 4.853899955749512
First 5 weights of embed_tokens: tensor([[ 2.9244e-02, -7.9903e-04,  4.4283e-03,  7.9785e-03, -4.0521e-03],
        [ 2.7570e-02,  3.2041e-02, -1.7365e-03,  1.2553e-03,  4.8598e-02],
        [ 2.4812e-02, -1.3301e-02,  5.1677e-03, -4.2910e-02, -1.7668e-02],
        [-1.5178e-02,  8.7660e-03, -1.9987e-02,  1.3096e-02,  1.6854e-05],
        [ 3.6465e-02, -1.1215e-02, -1.0445e-02, -3.0787e-02, -1.9769e-02]],
       device='cuda:0')
Loss: 4.027937412261963
First 5 weights of embed_tokens: tensor([[ 2.9243e-02, -8.0094e-04,  4.4267e-03,  7.9784e-03, -4.0519e-03],
        [ 2.7562e-02,  3.2035e-02, -1.7347e-03,  1.2570e-03,  4.8595e-02],
        [ 2.4803e-02, -1.3306e-02,  5.1703e-03, -4.2908e-02, -1.7671e-02],
        [-1.5186e-02,  8.7606e-03, -1.9984e-02,  1.3098e-02,  1.3904e-05],
        [ 3.6458e-02, -1.1220e-02, -1.0443e-02, -3.0785e-02, -1.9771e-02]],
       device='cuda:0')
Loss: 4.404093265533447
First 5 weights of embed_tokens: tensor([[ 2.9241e-02, -8.0282e-04,  4.4252e-03,  7.9784e-03, -4.0518e-03],
        [ 2.7554e-02,  3.2030e-02, -1.7328e-03,  1.2588e-03,  4.8592e-02],
        [ 2.4795e-02, -1.3312e-02,  5.1731e-03, -4.2906e-02, -1.7674e-02],
        [-1.5194e-02,  8.7553e-03, -1.9980e-02,  1.3100e-02,  1.0981e-05],
        [ 3.6450e-02, -1.1225e-02, -1.0440e-02, -3.0783e-02, -1.9774e-02]],
       device='cuda:0')
Loss: 4.450676441192627
First 5 weights of embed_tokens: tensor([[ 2.9239e-02, -8.0481e-04,  4.4241e-03,  7.9785e-03, -4.0518e-03],
        [ 2.7546e-02,  3.2024e-02, -1.7305e-03,  1.2605e-03,  4.8589e-02],
        [ 2.4787e-02, -1.3317e-02,  5.1764e-03, -4.2904e-02, -1.7677e-02],
        [-1.5202e-02,  8.7500e-03, -1.9976e-02,  1.3102e-02,  8.1404e-06],
        [ 3.6443e-02, -1.1231e-02, -1.0437e-02, -3.0781e-02, -1.9777e-02]],
       device='cuda:0')
Loss: 4.708479404449463
First 5 weights of embed_tokens: tensor([[ 2.9237e-02, -8.0679e-04,  4.4233e-03,  7.9786e-03, -4.0519e-03],
        [ 2.7537e-02,  3.2019e-02, -1.7280e-03,  1.2622e-03,  4.8586e-02],
        [ 2.4778e-02, -1.3322e-02,  5.1802e-03, -4.2902e-02, -1.7680e-02],
        [-1.5210e-02,  8.7447e-03, -1.9972e-02,  1.3104e-02,  5.3271e-06],
        [ 3.6436e-02, -1.1236e-02, -1.0433e-02, -3.0779e-02, -1.9779e-02]],
       device='cuda:0')
Epoch [0/100]; Iter [60/12800]; loss 4.5810; gen_loss 4.5810; LR 6.94e-06; Iter time 0.53s; ETA 1:52:06; Mem 4290.41MB
Loss: 4.811993598937988
First 5 weights of embed_tokens: tensor([[ 2.9235e-02, -8.0903e-04,  4.4227e-03,  7.9787e-03, -4.0520e-03],
        [ 2.7529e-02,  3.2013e-02, -1.7255e-03,  1.2639e-03,  4.8584e-02],
        [ 2.4770e-02, -1.3328e-02,  5.1839e-03, -4.2900e-02, -1.7682e-02],
        [-1.5218e-02,  8.7392e-03, -1.9968e-02,  1.3106e-02,  2.7013e-06],
        [ 3.6428e-02, -1.1241e-02, -1.0430e-02, -3.0777e-02, -1.9782e-02]],
       device='cuda:0')
Loss: 4.774097919464111
First 5 weights of embed_tokens: tensor([[ 2.9232e-02, -8.1127e-04,  4.4223e-03,  7.9788e-03, -4.0521e-03],
        [ 2.7520e-02,  3.2007e-02, -1.7227e-03,  1.2654e-03,  4.8581e-02],
        [ 2.4761e-02, -1.3334e-02,  5.1881e-03, -4.2898e-02, -1.7685e-02],
        [-1.5226e-02,  8.7338e-03, -1.9963e-02,  1.3107e-02,  7.4846e-08],
        [ 3.6421e-02, -1.1246e-02, -1.0426e-02, -3.0776e-02, -1.9784e-02]],
       device='cuda:0')
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 315, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 305, in main
    do_train(
  File "/root/MeshXL/engine.py", line 74, in do_train
    accelerator.backward(loss)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 2248, in backward
    loss.backward(**kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/MeshXL/main.py", line 315, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/MeshXL/main.py", line 305, in main
[rank0]:     do_train(
[rank0]:   File "/root/MeshXL/engine.py", line 74, in do_train
[rank0]:     accelerator.backward(loss)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 2248, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
