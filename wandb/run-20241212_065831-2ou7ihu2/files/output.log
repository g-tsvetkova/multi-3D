Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
torch.Size([2, 800, 2500, 3])
torch.Size([2, 800, 3, 3])
max face index: 1005 num_vertices: 800
min face index: 0
Discrete face coordinates:  tensor([[[[ 45, 116,  54],
          [ 45, 104,  51],
          [ 45, 104,  54]],

         [[ 45, 116,  51],
          [ 45, 104,  51],
          [ 45, 116,  54]],

         [[ 48, 104,  54],
          [ 48,  98,  51],
          [ 48,  98,  54]],

         ...,

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]],

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]],

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]]],


        [[[ 77,  73,  79],
          [ 78,  73,  59],
          [ 78,  65,  59]],

         [[ 77,  73,  79],
          [ 78,  65,  59],
          [ 77,  65,  79]],

         [[ 45,  72,  81],
          [ 75,  72,  81],
          [ 75,  65,  81]],

         ...,

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]],

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]],

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]]]], device='cuda:0')
Discrete face coordinates:  tensor([[[[ 45, 116,  54],
          [ 45, 104,  51],
          [ 45, 104,  54]],

         [[ 45, 116,  51],
          [ 45, 104,  51],
          [ 45, 116,  54]],

         [[ 48, 104,  54],
          [ 48,  98,  51],
          [ 48,  98,  54]],

         ...,

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]],

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]],

         [[ 60,  64,  51],
          [ 60,  64,  51],
          [ 60,  64,  51]]],


        [[[ 77,  73,  79],
          [ 78,  73,  59],
          [ 78,  65,  59]],

         [[ 77,  73,  79],
          [ 78,  65,  59],
          [ 77,  65,  79]],

         [[ 45,  72,  81],
          [ 75,  72,  81],
          [ 75,  65,  81]],

         ...,

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]],

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]],

         [[ 31,  64,  48],
          [ 31,  64,  48],
          [ 31,  64,  48]]]], device='cuda:0')
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 315, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 305, in main
    do_train(
  File "/root/MeshXL/engine.py", line 63, in do_train
    outputs = model(batch_data_label)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 864, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 272, in forward
    return self.train_one_step(data_dict)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 315, in train_one_step
    output = self.transformer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1145, in forward
    outputs = self.model.decoder(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 911, in forward
    layer_outputs = decoder_layer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 864, in forward
    output = self._fsdp_wrapped_module(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 170, in forward
    return self.checkpoint_fn(  # type: ignore[misc]
  File "/root/.multi3d/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 496, in checkpoint
    ret = function(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 552, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 232, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacity of 23.68 GiB of which 2.06 GiB is free. Process 3816112 has 17.25 GiB memory in use. Process 3833870 has 4.37 GiB memory in use. Of the allocated memory 3.71 GiB is allocated by PyTorch, and 204.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/MeshXL/main.py", line 315, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/MeshXL/main.py", line 305, in main
[rank0]:     do_train(
[rank0]:   File "/root/MeshXL/engine.py", line 63, in do_train
[rank0]:     outputs = model(batch_data_label)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 864, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/MeshXL/models/mesh_xl/get_model.py", line 272, in forward
[rank0]:     return self.train_one_step(data_dict)
[rank0]:   File "/root/MeshXL/models/mesh_xl/get_model.py", line 315, in train_one_step
[rank0]:     output = self.transformer(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1145, in forward
[rank0]:     outputs = self.model.decoder(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 911, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 864, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 170, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 496, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 552, in forward
[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 232, in forward
[rank0]:     attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacity of 23.68 GiB of which 2.06 GiB is free. Process 3816112 has 17.25 GiB memory in use. Process 3833870 has 4.37 GiB memory in use. Of the allocated memory 3.71 GiB is allocated by PyTorch, and 204.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
