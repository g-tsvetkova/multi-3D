Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 1024])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 1024])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.12.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.12.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.12.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.12.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.12.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.12.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.12.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.12.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.13.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.13.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.13.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.13.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.13.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.13.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.13.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.13.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.14.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.14.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.14.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.14.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.14.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.14.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.14.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.14.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.15.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.15.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.15.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.15.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.15.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.15.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.15.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.15.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.16.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.16.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.16.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.16.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.16.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.16.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.16.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.16.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.17.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.17.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.17.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.17.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.17.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.17.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.17.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.17.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.18.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.18.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.18.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.18.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.18.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.18.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.18.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.18.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.19.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.19.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.19.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.19.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.19.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.19.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.19.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.19.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.20.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.20.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.20.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.20.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.20.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.20.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.20.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.20.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.21.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.21.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.21.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.21.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.21.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.21.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.21.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.21.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.22.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.22.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.22.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.22.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.22.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.22.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.22.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.22.final_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn.k_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.23.self_attn.k_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn.v_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.23.self_attn.v_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn.q_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.23.self_attn.q_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn.out_proj.weight:	torch.Size([1024, 1024])
	[train]	transformer.model.decoder.layers.23.self_attn.out_proj.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.self_attn_layer_norm.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.fc1.weight:	torch.Size([4096, 1024])
	[train]	transformer.model.decoder.layers.23.fc1.bias:	torch.Size([4096])
	[train]	transformer.model.decoder.layers.23.fc2.weight:	torch.Size([1024, 4096])
	[train]	transformer.model.decoder.layers.23.fc2.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.final_layer_norm.weight:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.23.final_layer_norm.bias:	torch.Size([1024])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-350m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=1, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 1024, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 1024)
        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-23): 24 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=1024, out_features=131, bias=False)
  )
)
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 315, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 305, in main
    do_train(
  File "/root/MeshXL/engine.py", line 63, in do_train
    outputs = model(batch_data_label)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/root/.multi3d/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 92, in forward
    return self.train_one_step(data_dict)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 135, in train_one_step
    output = self.transformer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1145, in forward
    outputs = self.model.decoder(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 911, in forward
    layer_outputs = decoder_layer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 552, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 242, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.09 GiB. GPU 0 has a total capacity of 23.68 GiB of which 218.88 MiB is free. Process 142031 has 15.35 GiB memory in use. Process 144782 has 8.12 GiB memory in use. Of the allocated memory 7.74 GiB is allocated by PyTorch, and 70.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
