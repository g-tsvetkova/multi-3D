Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 315, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 305, in main
    do_train(
  File "/root/MeshXL/engine.py", line 74, in do_train
    accelerator.backward(loss)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 2248, in backward
    loss.backward(**kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 23.68 GiB of which 644.88 MiB is free. Process 3816112 has 17.25 GiB memory in use. Process 3843904 has 5.80 GiB memory in use. Of the allocated memory 5.08 GiB is allocated by PyTorch, and 259.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/MeshXL/main.py", line 315, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/MeshXL/main.py", line 305, in main
[rank0]:     do_train(
[rank0]:   File "/root/MeshXL/engine.py", line 74, in do_train
[rank0]:     accelerator.backward(loss)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 2248, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 23.68 GiB of which 644.88 MiB is free. Process 3816112 has 17.25 GiB memory in use. Process 3843904 has 5.80 GiB memory in use. Of the allocated memory 5.08 GiB is allocated by PyTorch, and 259.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
