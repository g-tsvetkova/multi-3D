	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=8, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
Epoch [0/16]; Iter [0/2048]; loss 5.0264; gen_loss 5.0264; LR 1.00e-06; Iter time 3.33s; ETA 1:53:32; Mem 14330.10MB
Epoch [0/16]; Iter [10/2048]; loss 4.8832; gen_loss 4.8832; LR 1.99e-06; Iter time 2.79s; ETA 1:34:47; Mem 15038.52MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
Epoch [0/16]; Iter [0/2048]; loss 5.0264; gen_loss 5.0264; LR 1.00e-06; Iter time 2.92s; ETA 1:39:37; Mem 14330.10MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttentionLayerBetterTransformer(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
Epoch [0/16]; Iter [0/2048]; loss 5.0264; gen_loss 5.0264; LR 1.00e-06; Iter time 2.92s; ETA 1:39:49; Mem 14330.10MB
Epoch [0/16]; Iter [10/2048]; loss 4.8832; gen_loss 4.8832; LR 1.99e-06; Iter time 2.79s; ETA 1:34:46; Mem 15038.52MB
Epoch [0/16]; Iter [20/2048]; loss 4.6864; gen_loss 4.6864; LR 2.98e-06; Iter time 2.82s; ETA 1:35:27; Mem 15038.52MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(131, 768, padding_idx=130)
          )
          (embed_positions): FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTLearnedPositionalEmbedding(8194, 768)
          )
          (final_layer_norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (v_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (q_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (out_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
              (fc1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=768, out_features=3072, bias=True)
              )
              (fc2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=3072, out_features=768, bias=True)
              )
              (final_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=768, out_features=131, bias=False)
      )
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(131, 768, padding_idx=130)
          )
          (embed_positions): FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTLearnedPositionalEmbedding(8194, 768)
          )
          (final_layer_norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (v_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (q_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (out_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
              (fc1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=768, out_features=3072, bias=True)
              )
              (fc2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=3072, out_features=768, bias=True)
              )
              (final_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=768, out_features=131, bias=False)
      )
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(131, 768, padding_idx=130)
          )
          (embed_positions): FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTLearnedPositionalEmbedding(8194, 768)
          )
          (final_layer_norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (v_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (q_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (out_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
              (fc1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=768, out_features=3072, bias=True)
              )
              (fc2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=3072, out_features=768, bias=True)
              )
              (final_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=768, out_features=131, bias=False)
      )
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([5002, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(131, 768, padding_idx=130)
          )
          (embed_positions): FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTLearnedPositionalEmbedding(5002, 768)
          )
          (final_layer_norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (v_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (q_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (out_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
              (fc1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=768, out_features=3072, bias=True)
              )
              (fc2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=3072, out_features=768, bias=True)
              )
              (final_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=768, out_features=131, bias=False)
      )
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): FullyShardedDataParallel(
            (_fsdp_wrapped_module): Embedding(131, 768, padding_idx=130)
          )
          (embed_positions): FullyShardedDataParallel(
            (_fsdp_wrapped_module): OPTLearnedPositionalEmbedding(8194, 768)
          )
          (final_layer_norm): FullyShardedDataParallel(
            (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (v_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (q_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
                (out_proj): FullyShardedDataParallel(
                  (_fsdp_wrapped_module): Linear(in_features=768, out_features=768, bias=True)
                )
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
              (fc1): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=768, out_features=3072, bias=True)
              )
              (fc2): FullyShardedDataParallel(
                (_fsdp_wrapped_module): Linear(in_features=3072, out_features=768, bias=True)
              )
              (final_layer_norm): FullyShardedDataParallel(
                (_fsdp_wrapped_module): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
      (lm_head): FullyShardedDataParallel(
        (_fsdp_wrapped_module): Linear(in_features=768, out_features=131, bias=False)
      )
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 768, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=768, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-11): 12 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=512, out_features=512, bias=True)
                (v_proj): Linear(in_features=512, out_features=512, bias=True)
                (q_proj): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
DistributedDataParallel(
  (module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=512, out_features=512, bias=True)
                (v_proj): Linear(in_features=512, out_features=512, bias=True)
                (q_proj): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 512, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-7): 8 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=512, out_features=512, bias=True)
              (v_proj): Linear(in_features=512, out_features=512, bias=True)
              (q_proj): Linear(in_features=512, out_features=512, bias=True)
              (out_proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=512, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=512, out_features=512, bias=True)
                (v_proj): Linear(in_features=512, out_features=512, bias=True)
                (q_proj): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x OPTDecoderLayer(
              (self_attn): OPTAttention(
                (k_proj): Linear(in_features=512, out_features=512, bias=True)
                (v_proj): Linear(in_features=512, out_features=512, bias=True)
                (q_proj): Linear(in_features=512, out_features=512, bias=True)
                (out_proj): Linear(in_features=512, out_features=512, bias=True)
              )
              (activation_fn): ReLU()
              (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=512, out_features=2048, bias=True)
              (fc2): Linear(in_features=2048, out_features=512, bias=True)
              (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/6400]; loss 5.1217; gen_loss 5.1217; LR 1.00e-06; Iter time 2.25s; ETA 3:59:32; Mem 16669.43MB
Epoch [0/100]; Iter [10/6400]; loss 4.9407; gen_loss 4.9407; LR 1.99e-06; Iter time 2.08s; ETA 3:41:05; Mem 16669.43MB
Epoch [0/100]; Iter [20/6400]; loss 4.7756; gen_loss 4.7756; LR 2.98e-06; Iter time 2.18s; ETA 3:51:54; Mem 16669.43MB
Epoch [0/100]; Iter [30/6400]; loss 4.6165; gen_loss 4.6165; LR 3.97e-06; Iter time 2.10s; ETA 3:42:43; Mem 16669.43MB
Epoch [0/100]; Iter [40/6400]; loss 4.5322; gen_loss 4.5322; LR 4.96e-06; Iter time 2.03s; ETA 3:35:20; Mem 16669.43MB
Epoch [0/100]; Iter [50/6400]; loss 4.2589; gen_loss 4.2589; LR 5.95e-06; Iter time 2.10s; ETA 3:42:34; Mem 16669.43MB
Epoch [0/100]; Iter [60/6400]; loss 4.2054; gen_loss 4.2054; LR 6.94e-06; Iter time 2.36s; ETA 4:09:12; Mem 16669.43MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=2, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 128])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 128])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([128])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 128, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 128)
          (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=128, out_features=128, bias=True)
                    (v_proj): Linear(in_features=128, out_features=128, bias=True)
                    (q_proj): Linear(in_features=128, out_features=128, bias=True)
                    (out_proj): Linear(in_features=128, out_features=128, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=128, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 128])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 128])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([128, 128])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([512, 128])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([128, 512])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([128])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([128])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 128, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 128)
          (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=128, out_features=128, bias=True)
                    (v_proj): Linear(in_features=128, out_features=128, bias=True)
                    (q_proj): Linear(in_features=128, out_features=128, bias=True)
                    (out_proj): Linear(in_features=128, out_features=128, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=128, out_features=512, bias=True)
                  (fc2): Linear(in_features=512, out_features=128, bias=True)
                  (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=128, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 512])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 512])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([512, 512])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2048, 512])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2048])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([512, 2048])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([512])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([512])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 512, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 512)
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=512, out_features=512, bias=True)
                    (v_proj): Linear(in_features=512, out_features=512, bias=True)
                    (q_proj): Linear(in_features=512, out_features=512, bias=True)
                    (out_proj): Linear(in_features=512, out_features=512, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=512, out_features=2048, bias=True)
                  (fc2): Linear(in_features=2048, out_features=512, bias=True)
                  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=512, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 510])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 510])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([510, 510])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([2040, 510])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([2040])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([510, 2040])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([510])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([510])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 510, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 510)
          (final_layer_norm): LayerNorm((510,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=510, out_features=510, bias=True)
                    (v_proj): Linear(in_features=510, out_features=510, bias=True)
                    (q_proj): Linear(in_features=510, out_features=510, bias=True)
                    (out_proj): Linear(in_features=510, out_features=510, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((510,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=510, out_features=2040, bias=True)
                  (fc2): Linear(in_features=2040, out_features=510, bias=True)
                  (final_layer_norm): LayerNorm((510,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=510, out_features=131, bias=False)
    )
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 6.53s; ETA 23:12:15; Mem 4287.56MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 5.28s; ETA 18:46:33; Mem 4287.56MB
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 5.25s; ETA 18:38:48; Mem 4287.56MB
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 5.27s; ETA 18:42:41; Mem 4287.56MB
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 5.28s; ETA 18:42:13; Mem 4287.56MB
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 5.29s; ETA 18:44:03; Mem 4287.56MB
Epoch [0/100]; Iter [60/12800]; loss 4.5810; gen_loss 4.5810; LR 6.94e-06; Iter time 5.24s; ETA 18:33:27; Mem 4287.56MB
Epoch [0/100]; Iter [70/12800]; loss 4.5715; gen_loss 4.5715; LR 7.93e-06; Iter time 5.32s; ETA 18:49:21; Mem 4287.56MB
Epoch [0/100]; Iter [80/12800]; loss 4.5005; gen_loss 4.5005; LR 8.92e-06; Iter time 5.25s; ETA 18:32:52; Mem 4287.56MB
Epoch [0/100]; Iter [90/12800]; loss 4.5720; gen_loss 4.5720; LR 9.91e-06; Iter time 5.29s; ETA 18:41:12; Mem 4287.56MB
Epoch [0/100]; Iter [100/12800]; loss 4.3283; gen_loss 4.3283; LR 1.09e-05; Iter time 5.35s; ETA 18:52:13; Mem 4287.56MB
Epoch [0/100]; Iter [110/12800]; loss 4.4118; gen_loss 4.4118; LR 1.19e-05; Iter time 5.53s; ETA 19:30:29; Mem 4287.56MB
Epoch [0/100]; Iter [120/12800]; loss 4.2741; gen_loss 4.2741; LR 1.29e-05; Iter time 5.21s; ETA 18:20:47; Mem 4287.56MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.67s; ETA 5:56:25; Mem 4290.41MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.53s; ETA 1:53:01; Mem 4290.41MB
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 0.53s; ETA 1:53:14; Mem 4290.41MB
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 0.53s; ETA 1:52:43; Mem 4290.41MB
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 0.53s; ETA 1:52:29; Mem 4290.41MB
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 0.54s; ETA 1:55:18; Mem 4290.41MB
Epoch [0/100]; Iter [60/12800]; loss 4.5810; gen_loss 4.5810; LR 6.94e-06; Iter time 0.53s; ETA 1:52:06; Mem 4290.41MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.71s; ETA 6:05:27; Mem 4290.41MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.54s; ETA 1:54:26; Mem 4290.41MB
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 0.54s; ETA 1:54:10; Mem 4290.41MB
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 0.54s; ETA 1:54:00; Mem 4290.41MB
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 0.54s; ETA 1:54:32; Mem 4290.41MB
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 0.56s; ETA 1:58:06; Mem 4290.41MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.76s; ETA 6:15:53; Mem 4290.41MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.54s; ETA 1:55:16; Mem 4290.41MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.40s; ETA 4:57:42; Mem 4290.41MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.53s; ETA 1:52:47; Mem 4290.41MB
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 0.53s; ETA 1:52:39; Mem 4290.41MB
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 0.53s; ETA 1:52:26; Mem 4290.41MB
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 0.53s; ETA 1:51:50; Mem 4290.41MB
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 0.54s; ETA 1:54:49; Mem 4290.41MB
Epoch [0/100]; Iter [60/12800]; loss 4.5810; gen_loss 4.5810; LR 6.94e-06; Iter time 0.54s; ETA 1:54:59; Mem 4290.41MB
Epoch [0/100]; Iter [70/12800]; loss 4.5715; gen_loss 4.5715; LR 7.93e-06; Iter time 0.54s; ETA 1:54:30; Mem 4290.41MB
Epoch [0/100]; Iter [80/12800]; loss 4.5005; gen_loss 4.5005; LR 8.92e-06; Iter time 0.55s; ETA 1:55:33; Mem 4290.41MB
Epoch [0/100]; Iter [90/12800]; loss 4.5720; gen_loss 4.5720; LR 9.91e-06; Iter time 0.56s; ETA 1:58:24; Mem 4290.41MB
Epoch [0/100]; Iter [100/12800]; loss 4.3283; gen_loss 4.3283; LR 1.09e-05; Iter time 0.56s; ETA 1:58:17; Mem 4290.41MB
Epoch [0/100]; Iter [110/12800]; loss 4.4118; gen_loss 4.4118; LR 1.19e-05; Iter time 0.55s; ETA 1:56:32; Mem 4290.41MB
Epoch [0/100]; Iter [120/12800]; loss 4.2741; gen_loss 4.2741; LR 1.29e-05; Iter time 0.55s; ETA 1:55:49; Mem 4290.41MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
FullyShardedDataParallel(
  (_fsdp_wrapped_module): MeshXL(
    (tokenizer): MeshTokenizer()
    (transformer): OPTForCausalLM(
      (model): OPTModel(
        (decoder): OPTDecoder(
          (embed_tokens): Embedding(131, 256, padding_idx=130)
          (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layers): ModuleList(
            (0-7): 8 x FullyShardedDataParallel(
              (_fsdp_wrapped_module): CheckpointWrapper(
                (_checkpoint_wrapped_module): OPTDecoderLayer(
                  (self_attn): OPTAttention(
                    (k_proj): Linear(in_features=256, out_features=256, bias=True)
                    (v_proj): Linear(in_features=256, out_features=256, bias=True)
                    (q_proj): Linear(in_features=256, out_features=256, bias=True)
                    (out_proj): Linear(in_features=256, out_features=256, bias=True)
                  )
                  (activation_fn): ReLU()
                  (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (fc1): Linear(in_features=256, out_features=1024, bias=True)
                  (fc2): Linear(in_features=1024, out_features=256, bias=True)
                  (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                )
              )
            )
          )
        )
      )
      (lm_head): Linear(in_features=256, out_features=131, bias=False)
    )
  )
)
Epoch [0/100]; Iter [0/12800]; loss 4.8095; gen_loss 4.8095; LR 1.00e-06; Iter time 1.46s; ETA 5:11:54; Mem 4290.41MB
Epoch [0/100]; Iter [10/12800]; loss 4.9247; gen_loss 4.9247; LR 1.99e-06; Iter time 0.53s; ETA 1:53:43; Mem 4290.41MB
Epoch [0/100]; Iter [20/12800]; loss 4.8628; gen_loss 4.8628; LR 2.98e-06; Iter time 0.53s; ETA 1:52:58; Mem 4290.41MB
Epoch [0/100]; Iter [30/12800]; loss 4.7877; gen_loss 4.7877; LR 3.97e-06; Iter time 0.53s; ETA 1:52:21; Mem 4290.41MB
Epoch [0/100]; Iter [40/12800]; loss 4.6703; gen_loss 4.6703; LR 4.96e-06; Iter time 0.54s; ETA 1:54:37; Mem 4290.41MB
Epoch [0/100]; Iter [50/12800]; loss 4.6520; gen_loss 4.6520; LR 5.95e-06; Iter time 0.54s; ETA 1:55:14; Mem 4290.41MB
Epoch [0/100]; Iter [60/12800]; loss 4.5810; gen_loss 4.5810; LR 6.94e-06; Iter time 0.54s; ETA 1:53:44; Mem 4290.41MB
Epoch [0/100]; Iter [70/12800]; loss 4.5715; gen_loss 4.5715; LR 7.93e-06; Iter time 0.54s; ETA 1:54:31; Mem 4290.41MB
Epoch [0/100]; Iter [80/12800]; loss 4.5005; gen_loss 4.5005; LR 8.92e-06; Iter time 0.54s; ETA 1:54:53; Mem 4290.41MB
Epoch [0/100]; Iter [90/12800]; loss 4.5720; gen_loss 4.5720; LR 9.91e-06; Iter time 0.54s; ETA 1:55:01; Mem 4290.41MB
Epoch [0/100]; Iter [100/12800]; loss 4.3283; gen_loss 4.3283; LR 1.09e-05; Iter time 0.56s; ETA 1:57:42; Mem 4290.41MB
Epoch [0/100]; Iter [110/12800]; loss 4.4118; gen_loss 4.4118; LR 1.19e-05; Iter time 0.55s; ETA 1:55:36; Mem 4290.41MB
Epoch [0/100]; Iter [120/12800]; loss 4.2741; gen_loss 4.2741; LR 1.29e-05; Iter time 0.54s; ETA 1:54:40; Mem 4290.41MB
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=8, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=1, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 256])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 256])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([256, 256])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([1024, 256])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([1024])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([256, 1024])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([256])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([256])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=1, batchsize_per_gpu=1, start_epoch=0, max_epoch=100, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 256, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 256)
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-5): 6 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=256, out_features=256, bias=True)
              (v_proj): Linear(in_features=256, out_features=256, bias=True)
              (q_proj): Linear(in_features=256, out_features=256, bias=True)
              (out_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=256, out_features=131, bias=False)
  )
)
Epoch [0/100]; Iter [0/51500]; loss 4.8792; gen_loss 4.8792; LR 1.00e-06; Iter time 0.61s; ETA 8:47:42; Mem 14271.59MB
Epoch [0/100]; Iter [10/51500]; loss 4.9445; gen_loss 4.9445; LR 1.99e-06; Iter time 0.27s; ETA 3:49:08; Mem 14324.95MB
Epoch [0/100]; Iter [20/51500]; loss 4.8840; gen_loss 4.8840; LR 2.98e-06; Iter time 0.27s; ETA 3:48:35; Mem 14324.95MB
Epoch [0/100]; Iter [30/51500]; loss 4.8905; gen_loss 4.8905; LR 3.97e-06; Iter time 0.27s; ETA 3:48:58; Mem 14324.95MB
Epoch [0/100]; Iter [40/51500]; loss 4.7333; gen_loss 4.7333; LR 4.96e-06; Iter time 0.27s; ETA 3:49:21; Mem 14324.95MB
Epoch [0/100]; Iter [50/51500]; loss 4.7322; gen_loss 4.7322; LR 5.95e-06; Iter time 0.27s; ETA 3:49:58; Mem 14324.95MB
Epoch [0/100]; Iter [60/51500]; loss 4.7196; gen_loss 4.7196; LR 6.94e-06; Iter time 0.27s; ETA 3:50:00; Mem 14324.95MB
Epoch [0/100]; Iter [70/51500]; loss 4.6523; gen_loss 4.6523; LR 7.93e-06; Iter time 0.27s; ETA 3:49:49; Mem 14324.95MB
Epoch [0/100]; Iter [80/51500]; loss 4.5410; gen_loss 4.5410; LR 8.92e-06; Iter time 0.27s; ETA 3:49:44; Mem 14324.95MB
Epoch [0/100]; Iter [90/51500]; loss 4.5749; gen_loss 4.5749; LR 9.91e-06; Iter time 0.27s; ETA 3:50:01; Mem 14324.95MB
Epoch [0/100]; Iter [100/51500]; loss 4.5869; gen_loss 4.5869; LR 1.09e-05; Iter time 0.27s; ETA 3:50:02; Mem 14324.95MB
Epoch [0/100]; Iter [110/51500]; loss 4.3793; gen_loss 4.3793; LR 1.19e-05; Iter time 0.27s; ETA 3:49:27; Mem 14324.95MB
Epoch [0/100]; Iter [120/51500]; loss 4.3908; gen_loss 4.3908; LR 1.29e-05; Iter time 0.27s; ETA 3:50:03; Mem 14324.95MB
Epoch [0/100]; Iter [130/51500]; loss 4.4453; gen_loss 4.4453; LR 1.39e-05; Iter time 0.27s; ETA 3:49:49; Mem 14324.95MB
Epoch [0/100]; Iter [140/51500]; loss 4.4016; gen_loss 4.4016; LR 1.49e-05; Iter time 0.27s; ETA 3:50:03; Mem 14324.95MB
Epoch [0/100]; Iter [150/51500]; loss 4.1250; gen_loss 4.1250; LR 1.58e-05; Iter time 0.27s; ETA 3:51:00; Mem 14324.95MB
Epoch [0/100]; Iter [160/51500]; loss 4.1469; gen_loss 4.1469; LR 1.68e-05; Iter time 0.27s; ETA 3:50:56; Mem 14324.95MB
Epoch [0/100]; Iter [170/51500]; loss 4.4823; gen_loss 4.4823; LR 1.78e-05; Iter time 0.27s; ETA 3:51:11; Mem 14324.95MB
Epoch [0/100]; Iter [180/51500]; loss 4.2438; gen_loss 4.2438; LR 1.88e-05; Iter time 0.27s; ETA 3:50:33; Mem 14324.95MB
Epoch [0/100]; Iter [190/51500]; loss 4.3923; gen_loss 4.3923; LR 1.98e-05; Iter time 0.27s; ETA 3:51:33; Mem 14324.95MB
Epoch [0/100]; Iter [200/51500]; loss 4.1311; gen_loss 4.1311; LR 2.08e-05; Iter time 0.27s; ETA 3:49:59; Mem 14324.95MB
Epoch [0/100]; Iter [210/51500]; loss 4.2860; gen_loss 4.2860; LR 2.18e-05; Iter time 0.27s; ETA 3:49:42; Mem 14324.95MB
Epoch [0/100]; Iter [220/51500]; loss 4.2097; gen_loss 4.2097; LR 2.28e-05; Iter time 0.27s; ETA 3:50:02; Mem 14324.95MB
Epoch [0/100]; Iter [230/51500]; loss 4.2609; gen_loss 4.2609; LR 2.38e-05; Iter time 0.27s; ETA 3:50:12; Mem 14324.95MB
Epoch [0/100]; Iter [240/51500]; loss 4.2654; gen_loss 4.2654; LR 2.48e-05; Iter time 0.27s; ETA 3:50:17; Mem 14324.95MB
Epoch [0/100]; Iter [250/51500]; loss 4.2078; gen_loss 4.2078; LR 2.58e-05; Iter time 0.27s; ETA 3:49:30; Mem 14324.95MB
Epoch [0/100]; Iter [260/51500]; loss 4.2409; gen_loss 4.2409; LR 2.67e-05; Iter time 0.27s; ETA 3:49:56; Mem 14324.95MB
Epoch [0/100]; Iter [270/51500]; loss 4.1826; gen_loss 4.1826; LR 2.77e-05; Iter time 0.27s; ETA 3:50:20; Mem 14324.95MB
Epoch [0/100]; Iter [280/51500]; loss 4.2959; gen_loss 4.2959; LR 2.87e-05; Iter time 0.27s; ETA 3:51:16; Mem 14324.95MB
Epoch [0/100]; Iter [290/51500]; loss 4.0594; gen_loss 4.0594; LR 2.97e-05; Iter time 0.27s; ETA 3:51:03; Mem 14324.95MB
Epoch [0/100]; Iter [300/51500]; loss 4.2117; gen_loss 4.2117; LR 3.07e-05; Iter time 0.27s; ETA 3:50:58; Mem 14324.95MB
Epoch [0/100]; Iter [310/51500]; loss 4.0204; gen_loss 4.0204; LR 3.17e-05; Iter time 0.27s; ETA 3:51:39; Mem 14324.95MB
Epoch [0/100]; Iter [320/51500]; loss 4.1317; gen_loss 4.1317; LR 3.27e-05; Iter time 0.27s; ETA 3:51:54; Mem 14324.95MB
Epoch [0/100]; Iter [330/51500]; loss 4.1007; gen_loss 4.1007; LR 3.37e-05; Iter time 0.27s; ETA 3:51:51; Mem 14324.95MB
Epoch [0/100]; Iter [340/51500]; loss 4.0083; gen_loss 4.0083; LR 3.47e-05; Iter time 0.27s; ETA 3:51:24; Mem 14324.95MB
Epoch [0/100]; Iter [350/51500]; loss 4.1032; gen_loss 4.1032; LR 3.56e-05; Iter time 0.27s; ETA 3:52:05; Mem 14324.95MB
Epoch [0/100]; Iter [360/51500]; loss 3.8752; gen_loss 3.8752; LR 3.66e-05; Iter time 0.27s; ETA 3:51:48; Mem 14324.95MB
Epoch [0/100]; Iter [370/51500]; loss 4.2134; gen_loss 4.2134; LR 3.76e-05; Iter time 0.27s; ETA 3:52:59; Mem 14324.95MB
Epoch [0/100]; Iter [380/51500]; loss 4.1684; gen_loss 4.1684; LR 3.86e-05; Iter time 0.27s; ETA 3:52:55; Mem 14324.95MB
Epoch [0/100]; Iter [390/51500]; loss 3.9422; gen_loss 3.9422; LR 3.96e-05; Iter time 0.27s; ETA 3:53:05; Mem 14324.95MB
Epoch [0/100]; Iter [400/51500]; loss 4.0512; gen_loss 4.0512; LR 4.06e-05; Iter time 0.27s; ETA 3:52:57; Mem 14324.95MB
Epoch [0/100]; Iter [410/51500]; loss 4.0759; gen_loss 4.0759; LR 4.16e-05; Iter time 0.27s; ETA 3:53:18; Mem 14324.95MB
Epoch [0/100]; Iter [420/51500]; loss 4.0341; gen_loss 4.0341; LR 4.26e-05; Iter time 0.27s; ETA 3:53:55; Mem 14324.95MB
Epoch [0/100]; Iter [430/51500]; loss 3.6901; gen_loss 3.6901; LR 4.36e-05; Iter time 0.29s; ETA 4:02:50; Mem 14324.95MB
Epoch [0/100]; Iter [440/51500]; loss 4.2251; gen_loss 4.2251; LR 4.46e-05; Iter time 0.27s; ETA 3:53:45; Mem 14324.95MB
Epoch [0/100]; Iter [450/51500]; loss 4.0165; gen_loss 4.0165; LR 4.56e-05; Iter time 0.27s; ETA 3:53:03; Mem 14324.95MB
Epoch [0/100]; Iter [460/51500]; loss 4.0680; gen_loss 4.0680; LR 4.65e-05; Iter time 0.27s; ETA 3:52:48; Mem 14324.95MB
Epoch [0/100]; Iter [470/51500]; loss 4.1480; gen_loss 4.1480; LR 4.75e-05; Iter time 0.27s; ETA 3:52:29; Mem 14324.95MB
Epoch [0/100]; Iter [480/51500]; loss 3.7303; gen_loss 3.7303; LR 4.85e-05; Iter time 0.27s; ETA 3:53:40; Mem 14324.95MB
Epoch [0/100]; Iter [490/51500]; loss 4.0729; gen_loss 4.0729; LR 4.95e-05; Iter time 0.27s; ETA 3:53:44; Mem 14324.95MB
Epoch [0/100]; Iter [500/51500]; loss 3.7445; gen_loss 3.7445; LR 5.05e-05; Iter time 0.27s; ETA 3:53:02; Mem 14324.95MB
Epoch [0/100]; Iter [510/51500]; loss 4.0099; gen_loss 4.0099; LR 5.15e-05; Iter time 0.27s; ETA 3:52:38; Mem 14324.95MB
Epoch [1/100]; Iter [520/51500]; loss 3.7647; gen_loss 3.7647; LR 5.25e-05; Iter time 0.27s; ETA 3:53:13; Mem 14324.95MB
Epoch [1/100]; Iter [530/51500]; loss 3.6478; gen_loss 3.6478; LR 5.35e-05; Iter time 0.28s; ETA 3:53:51; Mem 14324.95MB
Epoch [1/100]; Iter [540/51500]; loss 3.6717; gen_loss 3.6717; LR 5.45e-05; Iter time 0.28s; ETA 3:53:55; Mem 14324.95MB
Epoch [1/100]; Iter [550/51500]; loss 3.7985; gen_loss 3.7985; LR 5.55e-05; Iter time 0.28s; ETA 3:53:45; Mem 14324.95MB
Epoch [1/100]; Iter [560/51500]; loss 3.7761; gen_loss 3.7761; LR 5.64e-05; Iter time 0.28s; ETA 3:54:19; Mem 14324.95MB
Epoch [1/100]; Iter [570/51500]; loss 3.8373; gen_loss 3.8373; LR 5.74e-05; Iter time 0.28s; ETA 3:53:48; Mem 14324.95MB
Epoch [1/100]; Iter [580/51500]; loss 3.7038; gen_loss 3.7038; LR 5.84e-05; Iter time 0.28s; ETA 3:53:41; Mem 14324.95MB
Epoch [1/100]; Iter [590/51500]; loss 3.7425; gen_loss 3.7425; LR 5.94e-05; Iter time 0.28s; ETA 3:54:01; Mem 14324.95MB
Epoch [1/100]; Iter [600/51500]; loss 3.8425; gen_loss 3.8425; LR 6.04e-05; Iter time 0.28s; ETA 3:54:10; Mem 14324.95MB
Epoch [1/100]; Iter [610/51500]; loss 3.3921; gen_loss 3.3921; LR 6.14e-05; Iter time 0.28s; ETA 3:53:48; Mem 14324.95MB
Epoch [1/100]; Iter [620/51500]; loss 3.8209; gen_loss 3.8209; LR 6.24e-05; Iter time 0.28s; ETA 3:54:17; Mem 14324.95MB
Epoch [1/100]; Iter [630/51500]; loss 3.9961; gen_loss 3.9961; LR 6.34e-05; Iter time 0.28s; ETA 3:53:45; Mem 14324.95MB
Epoch [1/100]; Iter [640/51500]; loss 3.8109; gen_loss 3.8109; LR 6.44e-05; Iter time 0.28s; ETA 3:54:28; Mem 14324.95MB
Epoch [1/100]; Iter [650/51500]; loss 3.9202; gen_loss 3.9202; LR 6.54e-05; Iter time 0.28s; ETA 3:54:54; Mem 14324.95MB
Epoch [1/100]; Iter [660/51500]; loss 4.0284; gen_loss 4.0284; LR 6.63e-05; Iter time 0.28s; ETA 3:53:51; Mem 14324.95MB
Epoch [1/100]; Iter [670/51500]; loss 3.5942; gen_loss 3.5942; LR 6.73e-05; Iter time 0.28s; ETA 3:53:48; Mem 14324.95MB
Epoch [1/100]; Iter [680/51500]; loss 3.7432; gen_loss 3.7432; LR 6.83e-05; Iter time 0.28s; ETA 3:54:19; Mem 14324.95MB
Epoch [1/100]; Iter [690/51500]; loss 3.3083; gen_loss 3.3083; LR 6.93e-05; Iter time 0.28s; ETA 3:53:40; Mem 14324.95MB
Epoch [1/100]; Iter [700/51500]; loss 3.7485; gen_loss 3.7485; LR 7.03e-05; Iter time 0.28s; ETA 3:54:41; Mem 14324.95MB
Epoch [1/100]; Iter [710/51500]; loss 4.0978; gen_loss 4.0978; LR 7.13e-05; Iter time 0.28s; ETA 3:53:53; Mem 14324.95MB
Epoch [1/100]; Iter [720/51500]; loss 3.7304; gen_loss 3.7304; LR 7.23e-05; Iter time 0.28s; ETA 3:53:49; Mem 14324.95MB
Epoch [1/100]; Iter [730/51500]; loss 3.8374; gen_loss 3.8374; LR 7.33e-05; Iter time 0.28s; ETA 3:53:35; Mem 14324.95MB
Epoch [1/100]; Iter [740/51500]; loss 3.9035; gen_loss 3.9035; LR 7.43e-05; Iter time 0.28s; ETA 3:53:27; Mem 14324.95MB
Epoch [1/100]; Iter [750/51500]; loss 3.8245; gen_loss 3.8245; LR 7.52e-05; Iter time 0.28s; ETA 3:53:43; Mem 14324.95MB
Epoch [1/100]; Iter [760/51500]; loss 3.7843; gen_loss 3.7843; LR 7.62e-05; Iter time 0.28s; ETA 3:54:01; Mem 14324.95MB
Epoch [1/100]; Iter [770/51500]; loss 3.9633; gen_loss 3.9633; LR 7.72e-05; Iter time 0.28s; ETA 3:53:31; Mem 14324.95MB
Epoch [1/100]; Iter [780/51500]; loss 3.6726; gen_loss 3.6726; LR 7.82e-05; Iter time 0.28s; ETA 3:52:49; Mem 14324.95MB
Epoch [1/100]; Iter [790/51500]; loss 3.7816; gen_loss 3.7816; LR 7.92e-05; Iter time 0.28s; ETA 3:52:43; Mem 14324.95MB
Epoch [1/100]; Iter [800/51500]; loss 3.3331; gen_loss 3.3331; LR 8.02e-05; Iter time 0.27s; ETA 3:52:02; Mem 14324.95MB
Epoch [1/100]; Iter [810/51500]; loss 3.9168; gen_loss 3.9168; LR 8.12e-05; Iter time 0.28s; ETA 3:53:05; Mem 14324.95MB
Epoch [1/100]; Iter [820/51500]; loss 3.6501; gen_loss 3.6501; LR 8.22e-05; Iter time 0.28s; ETA 3:52:56; Mem 14324.95MB
Epoch [1/100]; Iter [830/51500]; loss 3.8631; gen_loss 3.8631; LR 8.32e-05; Iter time 0.28s; ETA 3:53:42; Mem 14324.95MB
Epoch [1/100]; Iter [840/51500]; loss 3.8692; gen_loss 3.8692; LR 8.42e-05; Iter time 0.28s; ETA 3:54:05; Mem 14324.95MB
Epoch [1/100]; Iter [850/51500]; loss 3.4652; gen_loss 3.4652; LR 8.51e-05; Iter time 0.28s; ETA 3:53:38; Mem 14324.95MB
Epoch [1/100]; Iter [860/51500]; loss 3.7014; gen_loss 3.7014; LR 8.61e-05; Iter time 0.28s; ETA 3:53:55; Mem 14324.95MB
Epoch [1/100]; Iter [870/51500]; loss 3.6329; gen_loss 3.6329; LR 8.71e-05; Iter time 0.28s; ETA 3:53:18; Mem 14324.95MB
Epoch [1/100]; Iter [880/51500]; loss 3.6985; gen_loss 3.6985; LR 8.81e-05; Iter time 0.28s; ETA 3:52:56; Mem 14324.95MB
Epoch [1/100]; Iter [890/51500]; loss 3.9878; gen_loss 3.9878; LR 8.91e-05; Iter time 0.28s; ETA 3:52:55; Mem 14324.95MB
Epoch [1/100]; Iter [900/51500]; loss 3.6898; gen_loss 3.6898; LR 9.01e-05; Iter time 0.28s; ETA 3:53:16; Mem 14324.95MB
Epoch [1/100]; Iter [910/51500]; loss 3.7714; gen_loss 3.7714; LR 9.11e-05; Iter time 0.28s; ETA 3:53:10; Mem 14324.95MB
Epoch [1/100]; Iter [920/51500]; loss 3.7606; gen_loss 3.7606; LR 9.21e-05; Iter time 0.28s; ETA 3:52:54; Mem 14324.95MB
Epoch [1/100]; Iter [930/51500]; loss 3.7948; gen_loss 3.7948; LR 9.31e-05; Iter time 0.28s; ETA 3:52:26; Mem 14324.95MB
Epoch [1/100]; Iter [940/51500]; loss 3.4413; gen_loss 3.4413; LR 9.41e-05; Iter time 0.28s; ETA 3:53:11; Mem 14324.95MB
Epoch [1/100]; Iter [950/51500]; loss 3.7566; gen_loss 3.7566; LR 9.51e-05; Iter time 0.28s; ETA 3:52:52; Mem 14324.95MB
Epoch [1/100]; Iter [960/51500]; loss 3.6321; gen_loss 3.6321; LR 9.60e-05; Iter time 0.28s; ETA 3:53:36; Mem 14324.95MB
Epoch [1/100]; Iter [970/51500]; loss 3.7406; gen_loss 3.7406; LR 9.70e-05; Iter time 0.28s; ETA 3:54:06; Mem 14324.95MB
Epoch [1/100]; Iter [980/51500]; loss 3.6108; gen_loss 3.6108; LR 9.80e-05; Iter time 0.28s; ETA 3:52:19; Mem 14324.95MB
Epoch [1/100]; Iter [990/51500]; loss 3.7336; gen_loss 3.7336; LR 9.90e-05; Iter time 0.28s; ETA 3:53:08; Mem 14324.95MB
Epoch [1/100]; Iter [1000/51500]; loss 3.2330; gen_loss 3.2330; LR 1.00e-04; Iter time 0.28s; ETA 3:52:02; Mem 14324.95MB
Epoch [1/100]; Iter [1010/51500]; loss 3.6994; gen_loss 3.6994; LR 9.99e-05; Iter time 0.28s; ETA 3:52:31; Mem 14324.95MB
Epoch [1/100]; Iter [1020/51500]; loss 3.8771; gen_loss 3.8771; LR 9.99e-05; Iter time 0.28s; ETA 3:52:33; Mem 14324.95MB
Epoch [2/100]; Iter [1030/51500]; loss 3.5924; gen_loss 3.5924; LR 9.99e-05; Iter time 0.28s; ETA 3:52:25; Mem 14324.95MB
Epoch [2/100]; Iter [1040/51500]; loss 3.3917; gen_loss 3.3917; LR 9.99e-05; Iter time 0.28s; ETA 3:52:19; Mem 14324.95MB
Epoch [2/100]; Iter [1050/51500]; loss 3.2591; gen_loss 3.2591; LR 9.99e-05; Iter time 0.27s; ETA 3:51:06; Mem 14324.95MB
Epoch [2/100]; Iter [1060/51500]; loss 3.3007; gen_loss 3.3007; LR 9.99e-05; Iter time 0.27s; ETA 3:50:22; Mem 14324.95MB
Epoch [2/100]; Iter [1070/51500]; loss 3.5923; gen_loss 3.5923; LR 9.99e-05; Iter time 0.28s; ETA 3:51:40; Mem 14324.95MB
Epoch [2/100]; Iter [1080/51500]; loss 3.4978; gen_loss 3.4978; LR 9.99e-05; Iter time 0.27s; ETA 3:50:44; Mem 14324.95MB
Epoch [2/100]; Iter [1090/51500]; loss 3.7486; gen_loss 3.7486; LR 9.99e-05; Iter time 0.27s; ETA 3:50:14; Mem 14324.95MB
Epoch [2/100]; Iter [1100/51500]; loss 3.4315; gen_loss 3.4315; LR 9.99e-05; Iter time 0.28s; ETA 3:51:05; Mem 14324.95MB
Epoch [2/100]; Iter [1110/51500]; loss 3.6288; gen_loss 3.6288; LR 9.99e-05; Iter time 0.27s; ETA 3:50:51; Mem 14324.95MB
Epoch [2/100]; Iter [1120/51500]; loss 3.7056; gen_loss 3.7056; LR 9.99e-05; Iter time 0.28s; ETA 3:50:55; Mem 14324.95MB
Epoch [2/100]; Iter [1130/51500]; loss 3.3687; gen_loss 3.3687; LR 9.99e-05; Iter time 0.27s; ETA 3:50:41; Mem 14324.95MB
Epoch [2/100]; Iter [1140/51500]; loss 3.7117; gen_loss 3.7117; LR 9.99e-05; Iter time 0.28s; ETA 3:51:11; Mem 14324.95MB
Epoch [2/100]; Iter [1150/51500]; loss 3.7495; gen_loss 3.7495; LR 9.99e-05; Iter time 0.27s; ETA 3:50:43; Mem 14324.95MB
Epoch [2/100]; Iter [1160/51500]; loss 3.3852; gen_loss 3.3852; LR 9.99e-05; Iter time 0.27s; ETA 3:50:27; Mem 14324.95MB
Epoch [2/100]; Iter [1170/51500]; loss 3.6031; gen_loss 3.6031; LR 9.99e-05; Iter time 0.28s; ETA 3:51:13; Mem 14324.95MB
Epoch [2/100]; Iter [1180/51500]; loss 3.3213; gen_loss 3.3213; LR 9.99e-05; Iter time 0.28s; ETA 3:51:10; Mem 14324.95MB
Epoch [2/100]; Iter [1190/51500]; loss 3.2108; gen_loss 3.2108; LR 9.99e-05; Iter time 0.28s; ETA 3:51:49; Mem 14324.95MB
Epoch [2/100]; Iter [1200/51500]; loss 3.1378; gen_loss 3.1378; LR 9.99e-05; Iter time 0.28s; ETA 3:51:09; Mem 14324.95MB
Epoch [2/100]; Iter [1210/51500]; loss 3.4480; gen_loss 3.4480; LR 9.99e-05; Iter time 0.28s; ETA 3:51:14; Mem 14324.95MB
Epoch [2/100]; Iter [1220/51500]; loss 3.3497; gen_loss 3.3497; LR 9.99e-05; Iter time 0.28s; ETA 3:51:31; Mem 14324.95MB
Epoch [2/100]; Iter [1230/51500]; loss 3.2337; gen_loss 3.2337; LR 9.99e-05; Iter time 0.28s; ETA 3:51:34; Mem 14324.95MB
Epoch [2/100]; Iter [1240/51500]; loss 3.7101; gen_loss 3.7101; LR 9.99e-05; Iter time 0.28s; ETA 3:51:40; Mem 14324.95MB
Epoch [2/100]; Iter [1250/51500]; loss 3.5012; gen_loss 3.5012; LR 9.99e-05; Iter time 0.28s; ETA 3:51:42; Mem 14324.95MB
Epoch [2/100]; Iter [1260/51500]; loss 3.1430; gen_loss 3.1430; LR 9.99e-05; Iter time 0.28s; ETA 3:52:02; Mem 14324.95MB
Epoch [2/100]; Iter [1270/51500]; loss 3.7144; gen_loss 3.7144; LR 9.99e-05; Iter time 0.28s; ETA 3:51:55; Mem 14324.95MB
Epoch [2/100]; Iter [1280/51500]; loss 3.3296; gen_loss 3.3296; LR 9.98e-05; Iter time 0.28s; ETA 3:51:27; Mem 14324.95MB
Epoch [2/100]; Iter [1290/51500]; loss 3.3221; gen_loss 3.3221; LR 9.98e-05; Iter time 0.28s; ETA 3:51:02; Mem 14324.95MB
Epoch [2/100]; Iter [1300/51500]; loss 3.5267; gen_loss 3.5267; LR 9.98e-05; Iter time 0.28s; ETA 3:51:33; Mem 14324.95MB
Epoch [2/100]; Iter [1310/51500]; loss 3.2426; gen_loss 3.2426; LR 9.98e-05; Iter time 0.28s; ETA 3:50:57; Mem 14324.95MB
Epoch [2/100]; Iter [1320/51500]; loss 3.6723; gen_loss 3.6723; LR 9.98e-05; Iter time 0.28s; ETA 3:51:46; Mem 14324.95MB
Epoch [2/100]; Iter [1330/51500]; loss 3.1615; gen_loss 3.1615; LR 9.98e-05; Iter time 0.28s; ETA 3:52:07; Mem 14324.95MB
Epoch [2/100]; Iter [1340/51500]; loss 2.8642; gen_loss 2.8642; LR 9.98e-05; Iter time 0.28s; ETA 3:51:26; Mem 14324.95MB
Epoch [2/100]; Iter [1350/51500]; loss 3.1275; gen_loss 3.1275; LR 9.98e-05; Iter time 0.28s; ETA 3:51:13; Mem 14324.95MB
Epoch [2/100]; Iter [1360/51500]; loss 3.4152; gen_loss 3.4152; LR 9.98e-05; Iter time 0.28s; ETA 3:51:21; Mem 14324.95MB
Epoch [2/100]; Iter [1370/51500]; loss 3.2727; gen_loss 3.2727; LR 9.98e-05; Iter time 0.28s; ETA 3:51:39; Mem 14324.95MB
Epoch [2/100]; Iter [1380/51500]; loss 3.4653; gen_loss 3.4653; LR 9.98e-05; Iter time 0.28s; ETA 3:51:06; Mem 14324.95MB
Epoch [2/100]; Iter [1390/51500]; loss 3.2355; gen_loss 3.2355; LR 9.98e-05; Iter time 0.28s; ETA 3:50:57; Mem 14324.95MB
Epoch [2/100]; Iter [1400/51500]; loss 3.1503; gen_loss 3.1503; LR 9.98e-05; Iter time 0.28s; ETA 3:50:50; Mem 14324.95MB
Epoch [2/100]; Iter [1410/51500]; loss 3.4111; gen_loss 3.4111; LR 9.98e-05; Iter time 0.28s; ETA 3:51:15; Mem 14324.95MB
Epoch [2/100]; Iter [1420/51500]; loss 2.8984; gen_loss 2.8984; LR 9.98e-05; Iter time 0.28s; ETA 3:50:51; Mem 14324.95MB
Epoch [2/100]; Iter [1430/51500]; loss 3.0863; gen_loss 3.0863; LR 9.98e-05; Iter time 0.28s; ETA 3:50:41; Mem 14324.95MB
Epoch [2/100]; Iter [1440/51500]; loss 3.0626; gen_loss 3.0626; LR 9.98e-05; Iter time 0.28s; ETA 3:50:25; Mem 14324.95MB
Epoch [2/100]; Iter [1450/51500]; loss 2.9311; gen_loss 2.9311; LR 9.98e-05; Iter time 0.28s; ETA 3:51:00; Mem 14324.95MB
Epoch [2/100]; Iter [1460/51500]; loss 3.0647; gen_loss 3.0647; LR 9.98e-05; Iter time 0.28s; ETA 3:51:42; Mem 14324.95MB
Epoch [2/100]; Iter [1470/51500]; loss 2.7386; gen_loss 2.7386; LR 9.98e-05; Iter time 0.27s; ETA 3:48:45; Mem 14324.95MB
Epoch [2/100]; Iter [1480/51500]; loss 3.1155; gen_loss 3.1155; LR 9.98e-05; Iter time 0.27s; ETA 3:48:56; Mem 14324.95MB
Epoch [2/100]; Iter [1490/51500]; loss 3.0077; gen_loss 3.0077; LR 9.98e-05; Iter time 0.27s; ETA 3:49:01; Mem 14324.95MB
Epoch [2/100]; Iter [1500/51500]; loss 3.1205; gen_loss 3.1205; LR 9.98e-05; Iter time 0.27s; ETA 3:49:09; Mem 14324.95MB
Epoch [2/100]; Iter [1510/51500]; loss 3.3813; gen_loss 3.3813; LR 9.98e-05; Iter time 0.27s; ETA 3:48:58; Mem 14324.95MB
Epoch [2/100]; Iter [1520/51500]; loss 3.3646; gen_loss 3.3646; LR 9.98e-05; Iter time 0.28s; ETA 3:49:07; Mem 14324.95MB
Epoch [2/100]; Iter [1530/51500]; loss 2.9986; gen_loss 2.9986; LR 9.98e-05; Iter time 0.28s; ETA 3:49:04; Mem 14324.95MB
Epoch [2/100]; Iter [1540/51500]; loss 2.7664; gen_loss 2.7664; LR 9.98e-05; Iter time 0.27s; ETA 3:48:40; Mem 14324.95MB
Epoch [3/100]; Iter [1550/51500]; loss 2.9081; gen_loss 2.9081; LR 9.98e-05; Iter time 0.28s; ETA 3:50:04; Mem 14324.95MB
Epoch [3/100]; Iter [1560/51500]; loss 3.1814; gen_loss 3.1814; LR 9.98e-05; Iter time 0.28s; ETA 3:51:07; Mem 14324.95MB
Epoch [3/100]; Iter [1570/51500]; loss 3.1888; gen_loss 3.1888; LR 9.98e-05; Iter time 0.28s; ETA 3:50:47; Mem 14324.95MB
Epoch [3/100]; Iter [1580/51500]; loss 3.2915; gen_loss 3.2915; LR 9.98e-05; Iter time 0.28s; ETA 3:50:26; Mem 14324.95MB
Epoch [3/100]; Iter [1590/51500]; loss 3.0278; gen_loss 3.0278; LR 9.98e-05; Iter time 0.28s; ETA 3:50:02; Mem 14324.95MB
Epoch [3/100]; Iter [1600/51500]; loss 3.1365; gen_loss 3.1365; LR 9.98e-05; Iter time 0.28s; ETA 3:50:55; Mem 14324.95MB
Epoch [3/100]; Iter [1610/51500]; loss 2.7420; gen_loss 2.7420; LR 9.98e-05; Iter time 0.28s; ETA 3:50:16; Mem 14324.95MB
Epoch [3/100]; Iter [1620/51500]; loss 2.6500; gen_loss 2.6500; LR 9.98e-05; Iter time 0.28s; ETA 3:50:15; Mem 14324.95MB
Epoch [3/100]; Iter [1630/51500]; loss 2.7531; gen_loss 2.7531; LR 9.98e-05; Iter time 0.28s; ETA 3:49:53; Mem 14324.95MB
Epoch [3/100]; Iter [1640/51500]; loss 2.8980; gen_loss 2.8980; LR 9.98e-05; Iter time 0.28s; ETA 3:51:40; Mem 14324.95MB
Epoch [3/100]; Iter [1650/51500]; loss 2.3198; gen_loss 2.3198; LR 9.97e-05; Iter time 0.28s; ETA 3:50:52; Mem 14324.95MB
Epoch [3/100]; Iter [1660/51500]; loss 2.7689; gen_loss 2.7689; LR 9.97e-05; Iter time 0.28s; ETA 3:50:48; Mem 14324.95MB
Epoch [3/100]; Iter [1670/51500]; loss 2.6963; gen_loss 2.6963; LR 9.97e-05; Iter time 0.28s; ETA 3:51:21; Mem 14324.95MB
Epoch [3/100]; Iter [1680/51500]; loss 2.7949; gen_loss 2.7949; LR 9.97e-05; Iter time 0.28s; ETA 3:51:09; Mem 14324.95MB
Epoch [3/100]; Iter [1690/51500]; loss 2.7662; gen_loss 2.7662; LR 9.97e-05; Iter time 0.28s; ETA 3:50:35; Mem 14324.95MB
Epoch [3/100]; Iter [1700/51500]; loss 2.8647; gen_loss 2.8647; LR 9.97e-05; Iter time 0.28s; ETA 3:49:53; Mem 14324.95MB
Epoch [3/100]; Iter [1710/51500]; loss 2.7363; gen_loss 2.7363; LR 9.97e-05; Iter time 0.28s; ETA 3:49:59; Mem 14324.95MB
Epoch [3/100]; Iter [1720/51500]; loss 2.8212; gen_loss 2.8212; LR 9.97e-05; Iter time 0.28s; ETA 3:49:37; Mem 14324.95MB
Epoch [3/100]; Iter [1730/51500]; loss 2.6886; gen_loss 2.6886; LR 9.97e-05; Iter time 0.28s; ETA 3:50:40; Mem 14324.95MB
Epoch [3/100]; Iter [1740/51500]; loss 2.5342; gen_loss 2.5342; LR 9.97e-05; Iter time 0.28s; ETA 3:49:45; Mem 14324.95MB
Epoch [3/100]; Iter [1750/51500]; loss 2.3278; gen_loss 2.3278; LR 9.97e-05; Iter time 0.28s; ETA 3:49:35; Mem 14324.95MB
Epoch [3/100]; Iter [1760/51500]; loss 2.7907; gen_loss 2.7907; LR 9.97e-05; Iter time 0.28s; ETA 3:50:05; Mem 14324.95MB
Epoch [3/100]; Iter [1770/51500]; loss 2.7685; gen_loss 2.7685; LR 9.97e-05; Iter time 0.28s; ETA 3:49:51; Mem 14324.95MB
Epoch [3/100]; Iter [1780/51500]; loss 2.5900; gen_loss 2.5900; LR 9.97e-05; Iter time 0.28s; ETA 3:49:30; Mem 14324.95MB
Epoch [3/100]; Iter [1790/51500]; loss 2.9925; gen_loss 2.9925; LR 9.97e-05; Iter time 0.28s; ETA 3:49:26; Mem 14324.95MB
Epoch [3/100]; Iter [1800/51500]; loss 2.9356; gen_loss 2.9356; LR 9.97e-05; Iter time 0.28s; ETA 3:49:52; Mem 14324.95MB
Epoch [3/100]; Iter [1810/51500]; loss 2.6277; gen_loss 2.6277; LR 9.97e-05; Iter time 0.28s; ETA 3:48:54; Mem 14324.95MB
Epoch [3/100]; Iter [1820/51500]; loss 2.7226; gen_loss 2.7226; LR 9.97e-05; Iter time 0.27s; ETA 3:47:26; Mem 14324.95MB
Epoch [3/100]; Iter [1830/51500]; loss 2.8401; gen_loss 2.8401; LR 9.97e-05; Iter time 0.28s; ETA 3:48:11; Mem 14324.95MB
Epoch [3/100]; Iter [1840/51500]; loss 3.1706; gen_loss 3.1706; LR 9.97e-05; Iter time 0.28s; ETA 3:48:38; Mem 14324.95MB
Epoch [3/100]; Iter [1850/51500]; loss 3.1490; gen_loss 3.1490; LR 9.97e-05; Iter time 0.28s; ETA 3:48:44; Mem 14324.95MB
Epoch [3/100]; Iter [1860/51500]; loss 2.7826; gen_loss 2.7826; LR 9.97e-05; Iter time 0.28s; ETA 3:48:42; Mem 14324.95MB
Epoch [3/100]; Iter [1870/51500]; loss 3.0180; gen_loss 3.0180; LR 9.97e-05; Iter time 0.28s; ETA 3:49:18; Mem 14324.95MB
Epoch [3/100]; Iter [1880/51500]; loss 2.7624; gen_loss 2.7624; LR 9.97e-05; Iter time 0.28s; ETA 3:49:26; Mem 14324.95MB
Epoch [3/100]; Iter [1890/51500]; loss 2.7887; gen_loss 2.7887; LR 9.97e-05; Iter time 0.28s; ETA 3:49:20; Mem 14324.95MB
Epoch [3/100]; Iter [1900/51500]; loss 2.6277; gen_loss 2.6277; LR 9.97e-05; Iter time 0.28s; ETA 3:50:03; Mem 14324.95MB
Epoch [3/100]; Iter [1910/51500]; loss 2.8613; gen_loss 2.8613; LR 9.97e-05; Iter time 0.28s; ETA 3:49:38; Mem 14324.95MB
Epoch [3/100]; Iter [1920/51500]; loss 2.5811; gen_loss 2.5811; LR 9.97e-05; Iter time 0.28s; ETA 3:48:12; Mem 14324.95MB
Epoch [3/100]; Iter [1930/51500]; loss 2.5996; gen_loss 2.5996; LR 9.97e-05; Iter time 0.28s; ETA 3:48:24; Mem 14324.95MB
Epoch [3/100]; Iter [1940/51500]; loss 2.4687; gen_loss 2.4687; LR 9.97e-05; Iter time 0.28s; ETA 3:48:27; Mem 14324.95MB
Epoch [3/100]; Iter [1950/51500]; loss 2.6619; gen_loss 2.6619; LR 9.97e-05; Iter time 0.28s; ETA 3:48:36; Mem 14324.95MB
Epoch [3/100]; Iter [1960/51500]; loss 2.5185; gen_loss 2.5185; LR 9.96e-05; Iter time 0.28s; ETA 3:48:43; Mem 14324.95MB
Epoch [3/100]; Iter [1970/51500]; loss 3.0745; gen_loss 3.0745; LR 9.96e-05; Iter time 0.28s; ETA 3:48:27; Mem 14324.95MB
Epoch [3/100]; Iter [1980/51500]; loss 3.0272; gen_loss 3.0272; LR 9.96e-05; Iter time 0.28s; ETA 3:48:44; Mem 14324.95MB
Epoch [3/100]; Iter [1990/51500]; loss 2.7885; gen_loss 2.7885; LR 9.96e-05; Iter time 0.28s; ETA 3:48:26; Mem 14324.95MB
Epoch [3/100]; Iter [2000/51500]; loss 2.8839; gen_loss 2.8839; LR 9.96e-05; Iter time 0.28s; ETA 3:49:00; Mem 14324.95MB
Epoch [3/100]; Iter [2010/51500]; loss 2.7748; gen_loss 2.7748; LR 9.96e-05; Iter time 0.28s; ETA 3:47:35; Mem 14324.95MB
Epoch [3/100]; Iter [2020/51500]; loss 2.9108; gen_loss 2.9108; LR 9.96e-05; Iter time 0.28s; ETA 3:48:54; Mem 14324.95MB
Epoch [3/100]; Iter [2030/51500]; loss 2.2187; gen_loss 2.2187; LR 9.96e-05; Iter time 0.28s; ETA 3:48:44; Mem 14324.95MB
Epoch [3/100]; Iter [2040/51500]; loss 2.5310; gen_loss 2.5310; LR 9.96e-05; Iter time 0.28s; ETA 3:50:09; Mem 14324.95MB
Epoch [3/100]; Iter [2050/51500]; loss 2.6516; gen_loss 2.6516; LR 9.96e-05; Iter time 0.28s; ETA 3:48:10; Mem 14324.95MB
Epoch [4/100]; Iter [2060/51500]; loss 3.0482; gen_loss 3.0482; LR 9.96e-05; Iter time 0.28s; ETA 3:48:43; Mem 14324.95MB
Epoch [4/100]; Iter [2070/51500]; loss 2.7862; gen_loss 2.7862; LR 9.96e-05; Iter time 0.28s; ETA 3:48:14; Mem 14324.95MB
Epoch [4/100]; Iter [2080/51500]; loss 2.7302; gen_loss 2.7302; LR 9.96e-05; Iter time 0.28s; ETA 3:48:18; Mem 14324.95MB
Epoch [4/100]; Iter [2090/51500]; loss 2.8098; gen_loss 2.8098; LR 9.96e-05; Iter time 0.28s; ETA 3:49:14; Mem 14324.95MB
Epoch [4/100]; Iter [2100/51500]; loss 2.6633; gen_loss 2.6633; LR 9.96e-05; Iter time 0.28s; ETA 3:48:47; Mem 14324.95MB
Epoch [4/100]; Iter [2110/51500]; loss 2.2896; gen_loss 2.2896; LR 9.96e-05; Iter time 0.28s; ETA 3:48:57; Mem 14324.95MB
Epoch [4/100]; Iter [2120/51500]; loss 2.1630; gen_loss 2.1630; LR 9.96e-05; Iter time 0.28s; ETA 3:48:34; Mem 14324.95MB
Epoch [4/100]; Iter [2130/51500]; loss 2.2805; gen_loss 2.2805; LR 9.96e-05; Iter time 0.28s; ETA 3:48:29; Mem 14324.95MB
Epoch [4/100]; Iter [2140/51500]; loss 2.4592; gen_loss 2.4592; LR 9.96e-05; Iter time 0.28s; ETA 3:47:44; Mem 14324.95MB
Epoch [4/100]; Iter [2150/51500]; loss 2.3034; gen_loss 2.3034; LR 9.96e-05; Iter time 0.28s; ETA 3:46:43; Mem 14324.95MB
Epoch [4/100]; Iter [2160/51500]; loss 1.9968; gen_loss 1.9968; LR 9.96e-05; Iter time 0.28s; ETA 3:46:59; Mem 14324.95MB
Epoch [4/100]; Iter [2170/51500]; loss 2.5700; gen_loss 2.5700; LR 9.96e-05; Iter time 0.28s; ETA 3:47:16; Mem 14324.95MB
Epoch [4/100]; Iter [2180/51500]; loss 2.6858; gen_loss 2.6858; LR 9.96e-05; Iter time 0.28s; ETA 3:46:39; Mem 14324.95MB
Epoch [4/100]; Iter [2190/51500]; loss 2.0103; gen_loss 2.0103; LR 9.96e-05; Iter time 0.28s; ETA 3:48:11; Mem 14324.95MB
Epoch [4/100]; Iter [2200/51500]; loss 2.6335; gen_loss 2.6335; LR 9.96e-05; Iter time 0.28s; ETA 3:47:27; Mem 14324.95MB
Epoch [4/100]; Iter [2210/51500]; loss 2.9136; gen_loss 2.9136; LR 9.96e-05; Iter time 0.28s; ETA 3:47:21; Mem 14324.95MB
Epoch [4/100]; Iter [2220/51500]; loss 2.6154; gen_loss 2.6154; LR 9.95e-05; Iter time 0.28s; ETA 3:47:20; Mem 14324.95MB
Epoch [4/100]; Iter [2230/51500]; loss 2.5947; gen_loss 2.5947; LR 9.95e-05; Iter time 0.28s; ETA 3:47:24; Mem 14324.95MB
Epoch [4/100]; Iter [2240/51500]; loss 2.4906; gen_loss 2.4906; LR 9.95e-05; Iter time 0.28s; ETA 3:48:07; Mem 14324.95MB
Epoch [4/100]; Iter [2250/51500]; loss 2.1707; gen_loss 2.1707; LR 9.95e-05; Iter time 0.28s; ETA 3:47:21; Mem 14324.95MB
Epoch [4/100]; Iter [2260/51500]; loss 2.7093; gen_loss 2.7093; LR 9.95e-05; Iter time 0.28s; ETA 3:46:58; Mem 14324.95MB
Epoch [4/100]; Iter [2270/51500]; loss 2.6834; gen_loss 2.6834; LR 9.95e-05; Iter time 0.28s; ETA 3:47:27; Mem 14324.95MB
Epoch [4/100]; Iter [2280/51500]; loss 2.8058; gen_loss 2.8058; LR 9.95e-05; Iter time 0.28s; ETA 3:47:31; Mem 14324.95MB
Epoch [4/100]; Iter [2290/51500]; loss 2.1656; gen_loss 2.1656; LR 9.95e-05; Iter time 0.28s; ETA 3:47:17; Mem 14324.95MB
Epoch [4/100]; Iter [2300/51500]; loss 2.4344; gen_loss 2.4344; LR 9.95e-05; Iter time 0.28s; ETA 3:47:06; Mem 14324.95MB
Epoch [4/100]; Iter [2310/51500]; loss 2.2584; gen_loss 2.2584; LR 9.95e-05; Iter time 0.28s; ETA 3:47:40; Mem 14324.95MB
Epoch [4/100]; Iter [2320/51500]; loss 2.3562; gen_loss 2.3562; LR 9.95e-05; Iter time 0.28s; ETA 3:47:33; Mem 14324.95MB
Epoch [4/100]; Iter [2330/51500]; loss 2.6557; gen_loss 2.6557; LR 9.95e-05; Iter time 0.28s; ETA 3:47:24; Mem 14324.95MB
Epoch [4/100]; Iter [2340/51500]; loss 2.6924; gen_loss 2.6924; LR 9.95e-05; Iter time 0.28s; ETA 3:46:03; Mem 14324.95MB
Epoch [4/100]; Iter [2350/51500]; loss 2.8621; gen_loss 2.8621; LR 9.95e-05; Iter time 0.28s; ETA 3:47:31; Mem 14324.95MB
Epoch [4/100]; Iter [2360/51500]; loss 2.3929; gen_loss 2.3929; LR 9.95e-05; Iter time 0.28s; ETA 3:46:57; Mem 14324.95MB
Epoch [4/100]; Iter [2370/51500]; loss 2.3284; gen_loss 2.3284; LR 9.95e-05; Iter time 0.28s; ETA 3:46:43; Mem 14324.95MB
Epoch [4/100]; Iter [2380/51500]; loss 2.6186; gen_loss 2.6186; LR 9.95e-05; Iter time 0.28s; ETA 3:46:43; Mem 14324.95MB
Epoch [4/100]; Iter [2390/51500]; loss 2.4276; gen_loss 2.4276; LR 9.95e-05; Iter time 0.28s; ETA 3:46:39; Mem 14324.95MB
Epoch [4/100]; Iter [2400/51500]; loss 2.5143; gen_loss 2.5143; LR 9.95e-05; Iter time 0.28s; ETA 3:46:51; Mem 14324.95MB
Epoch [4/100]; Iter [2410/51500]; loss 2.6660; gen_loss 2.6660; LR 9.95e-05; Iter time 0.28s; ETA 3:45:42; Mem 14324.95MB
Epoch [4/100]; Iter [2420/51500]; loss 2.7524; gen_loss 2.7524; LR 9.95e-05; Iter time 0.27s; ETA 3:44:54; Mem 14324.95MB
Epoch [4/100]; Iter [2430/51500]; loss 2.6651; gen_loss 2.6651; LR 9.95e-05; Iter time 0.28s; ETA 3:45:31; Mem 14324.95MB
Epoch [4/100]; Iter [2440/51500]; loss 2.3792; gen_loss 2.3792; LR 9.95e-05; Iter time 0.28s; ETA 3:45:14; Mem 14324.95MB
Epoch [4/100]; Iter [2450/51500]; loss 2.8160; gen_loss 2.8160; LR 9.94e-05; Iter time 0.28s; ETA 3:45:48; Mem 14324.95MB
Epoch [4/100]; Iter [2460/51500]; loss 2.2151; gen_loss 2.2151; LR 9.94e-05; Iter time 0.28s; ETA 3:45:15; Mem 14324.95MB
Epoch [4/100]; Iter [2470/51500]; loss 1.9893; gen_loss 1.9893; LR 9.94e-05; Iter time 0.28s; ETA 3:46:21; Mem 14324.95MB
Epoch [4/100]; Iter [2480/51500]; loss 2.3875; gen_loss 2.3875; LR 9.94e-05; Iter time 0.28s; ETA 3:46:19; Mem 14324.95MB
Epoch [4/100]; Iter [2490/51500]; loss 2.5153; gen_loss 2.5153; LR 9.94e-05; Iter time 0.28s; ETA 3:46:16; Mem 14324.95MB
Epoch [4/100]; Iter [2500/51500]; loss 2.4324; gen_loss 2.4324; LR 9.94e-05; Iter time 0.28s; ETA 3:45:59; Mem 14324.95MB
Epoch [4/100]; Iter [2510/51500]; loss 2.5082; gen_loss 2.5082; LR 9.94e-05; Iter time 0.28s; ETA 3:45:24; Mem 14324.95MB
Epoch [4/100]; Iter [2520/51500]; loss 2.5803; gen_loss 2.5803; LR 9.94e-05; Iter time 0.28s; ETA 3:45:47; Mem 14324.95MB
Epoch [4/100]; Iter [2530/51500]; loss 2.0531; gen_loss 2.0531; LR 9.94e-05; Iter time 0.28s; ETA 3:46:10; Mem 14324.95MB
Epoch [4/100]; Iter [2540/51500]; loss 2.2793; gen_loss 2.2793; LR 9.94e-05; Iter time 0.28s; ETA 3:45:42; Mem 14324.95MB
Epoch [4/100]; Iter [2550/51500]; loss 2.3822; gen_loss 2.3822; LR 9.94e-05; Iter time 0.28s; ETA 3:45:42; Mem 14324.95MB
Epoch [4/100]; Iter [2560/51500]; loss 2.7020; gen_loss 2.7020; LR 9.94e-05; Iter time 0.28s; ETA 3:45:45; Mem 14324.95MB
Epoch [4/100]; Iter [2570/51500]; loss 2.7978; gen_loss 2.7978; LR 9.94e-05; Iter time 0.28s; ETA 3:45:50; Mem 14324.95MB
Epoch [5/100]; Iter [2580/51500]; loss 2.5908; gen_loss 2.5908; LR 9.94e-05; Iter time 0.28s; ETA 3:44:23; Mem 14324.95MB
Epoch [5/100]; Iter [2590/51500]; loss 2.1275; gen_loss 2.1275; LR 9.94e-05; Iter time 0.28s; ETA 3:45:33; Mem 14324.95MB
Epoch [5/100]; Iter [2600/51500]; loss 2.6554; gen_loss 2.6554; LR 9.94e-05; Iter time 0.28s; ETA 3:45:56; Mem 14324.95MB
Epoch [5/100]; Iter [2610/51500]; loss 2.2120; gen_loss 2.2120; LR 9.94e-05; Iter time 0.28s; ETA 3:45:45; Mem 14324.95MB
Epoch [5/100]; Iter [2620/51500]; loss 2.2834; gen_loss 2.2834; LR 9.94e-05; Iter time 0.28s; ETA 3:45:26; Mem 14324.95MB
Epoch [5/100]; Iter [2630/51500]; loss 2.4143; gen_loss 2.4143; LR 9.94e-05; Iter time 0.28s; ETA 3:46:28; Mem 14324.95MB
Epoch [5/100]; Iter [2640/51500]; loss 2.6038; gen_loss 2.6038; LR 9.94e-05; Iter time 0.28s; ETA 3:46:07; Mem 14324.95MB
Epoch [5/100]; Iter [2650/51500]; loss 2.3411; gen_loss 2.3411; LR 9.94e-05; Iter time 0.28s; ETA 3:45:22; Mem 14324.95MB
Epoch [5/100]; Iter [2660/51500]; loss 2.2957; gen_loss 2.2957; LR 9.93e-05; Iter time 0.28s; ETA 3:46:32; Mem 14324.95MB
Epoch [5/100]; Iter [2670/51500]; loss 2.1733; gen_loss 2.1733; LR 9.93e-05; Iter time 0.28s; ETA 3:46:52; Mem 14324.95MB
Epoch [5/100]; Iter [2680/51500]; loss 2.1137; gen_loss 2.1137; LR 9.93e-05; Iter time 0.28s; ETA 3:45:52; Mem 14324.95MB
Epoch [5/100]; Iter [2690/51500]; loss 2.3048; gen_loss 2.3048; LR 9.93e-05; Iter time 0.28s; ETA 3:45:58; Mem 14324.95MB
Epoch [5/100]; Iter [2700/51500]; loss 2.4367; gen_loss 2.4367; LR 9.93e-05; Iter time 0.28s; ETA 3:45:46; Mem 14324.95MB
Epoch [5/100]; Iter [2710/51500]; loss 2.1105; gen_loss 2.1105; LR 9.93e-05; Iter time 0.28s; ETA 3:45:48; Mem 14324.95MB
Epoch [5/100]; Iter [2720/51500]; loss 1.9603; gen_loss 1.9603; LR 9.93e-05; Iter time 0.28s; ETA 3:45:20; Mem 14324.95MB
Epoch [5/100]; Iter [2730/51500]; loss 2.1473; gen_loss 2.1473; LR 9.93e-05; Iter time 0.28s; ETA 3:46:05; Mem 14324.95MB
Epoch [5/100]; Iter [2740/51500]; loss 2.1484; gen_loss 2.1484; LR 9.93e-05; Iter time 0.28s; ETA 3:46:14; Mem 14324.95MB
Epoch [5/100]; Iter [2750/51500]; loss 2.1871; gen_loss 2.1871; LR 9.93e-05; Iter time 0.28s; ETA 3:45:52; Mem 14324.95MB
Epoch [5/100]; Iter [2760/51500]; loss 2.8689; gen_loss 2.8689; LR 9.93e-05; Iter time 0.28s; ETA 3:45:53; Mem 14324.95MB
Epoch [5/100]; Iter [2770/51500]; loss 2.2006; gen_loss 2.2006; LR 9.93e-05; Iter time 0.28s; ETA 3:45:01; Mem 14324.95MB
Epoch [5/100]; Iter [2780/51500]; loss 2.2962; gen_loss 2.2962; LR 9.93e-05; Iter time 0.28s; ETA 3:44:40; Mem 14324.95MB
Epoch [5/100]; Iter [2790/51500]; loss 2.0128; gen_loss 2.0128; LR 9.93e-05; Iter time 0.28s; ETA 3:44:37; Mem 14324.95MB
Epoch [5/100]; Iter [2800/51500]; loss 2.1702; gen_loss 2.1702; LR 9.93e-05; Iter time 0.28s; ETA 3:44:24; Mem 14324.95MB
Epoch [5/100]; Iter [2810/51500]; loss 1.7357; gen_loss 1.7357; LR 9.93e-05; Iter time 0.28s; ETA 3:45:39; Mem 14324.95MB
Epoch [5/100]; Iter [2820/51500]; loss 2.4241; gen_loss 2.4241; LR 9.93e-05; Iter time 0.28s; ETA 3:45:14; Mem 14324.95MB
Epoch [5/100]; Iter [2830/51500]; loss 2.4449; gen_loss 2.4449; LR 9.93e-05; Iter time 0.28s; ETA 3:44:43; Mem 14324.95MB
Epoch [5/100]; Iter [2840/51500]; loss 2.2855; gen_loss 2.2855; LR 9.93e-05; Iter time 0.28s; ETA 3:45:29; Mem 14324.95MB
Epoch [5/100]; Iter [2850/51500]; loss 2.2663; gen_loss 2.2663; LR 9.93e-05; Iter time 0.28s; ETA 3:44:41; Mem 14324.95MB
Epoch [5/100]; Iter [2860/51500]; loss 2.1972; gen_loss 2.1972; LR 9.92e-05; Iter time 0.28s; ETA 3:45:14; Mem 14324.95MB
Epoch [5/100]; Iter [2870/51500]; loss 2.2943; gen_loss 2.2943; LR 9.92e-05; Iter time 0.28s; ETA 3:44:45; Mem 14324.95MB
Epoch [5/100]; Iter [2880/51500]; loss 2.1217; gen_loss 2.1217; LR 9.92e-05; Iter time 0.28s; ETA 3:44:29; Mem 14324.95MB
Epoch [5/100]; Iter [2890/51500]; loss 1.5746; gen_loss 1.5746; LR 9.92e-05; Iter time 0.28s; ETA 3:45:03; Mem 14324.95MB
Epoch [5/100]; Iter [2900/51500]; loss 2.2249; gen_loss 2.2249; LR 9.92e-05; Iter time 0.28s; ETA 3:44:42; Mem 14324.95MB
Epoch [5/100]; Iter [2910/51500]; loss 2.3997; gen_loss 2.3997; LR 9.92e-05; Iter time 0.28s; ETA 3:44:52; Mem 14324.95MB
Epoch [5/100]; Iter [2920/51500]; loss 2.4555; gen_loss 2.4555; LR 9.92e-05; Iter time 0.28s; ETA 3:44:07; Mem 14324.95MB
Epoch [5/100]; Iter [2930/51500]; loss 2.3120; gen_loss 2.3120; LR 9.92e-05; Iter time 0.28s; ETA 3:44:15; Mem 14324.95MB
Epoch [5/100]; Iter [2940/51500]; loss 2.0000; gen_loss 2.0000; LR 9.92e-05; Iter time 0.28s; ETA 3:43:56; Mem 14324.95MB
Epoch [5/100]; Iter [2950/51500]; loss 2.2508; gen_loss 2.2508; LR 9.92e-05; Iter time 0.28s; ETA 3:43:53; Mem 14324.95MB
Epoch [5/100]; Iter [2960/51500]; loss 2.4408; gen_loss 2.4408; LR 9.92e-05; Iter time 0.28s; ETA 3:44:45; Mem 14324.95MB
Epoch [5/100]; Iter [2970/51500]; loss 2.6827; gen_loss 2.6827; LR 9.92e-05; Iter time 0.28s; ETA 3:44:11; Mem 14324.95MB
Epoch [5/100]; Iter [2980/51500]; loss 2.1250; gen_loss 2.1250; LR 9.92e-05; Iter time 0.28s; ETA 3:44:09; Mem 14324.95MB
Epoch [5/100]; Iter [2990/51500]; loss 2.0131; gen_loss 2.0131; LR 9.92e-05; Iter time 0.28s; ETA 3:44:07; Mem 14324.95MB
Epoch [5/100]; Iter [3000/51500]; loss 2.3216; gen_loss 2.3216; LR 9.92e-05; Iter time 0.28s; ETA 3:44:20; Mem 14324.95MB
Epoch [5/100]; Iter [3010/51500]; loss 2.2196; gen_loss 2.2196; LR 9.92e-05; Iter time 0.28s; ETA 3:44:00; Mem 14324.95MB
Epoch [5/100]; Iter [3020/51500]; loss 2.2321; gen_loss 2.2321; LR 9.92e-05; Iter time 0.28s; ETA 3:44:11; Mem 14324.95MB
Epoch [5/100]; Iter [3030/51500]; loss 2.0833; gen_loss 2.0833; LR 9.92e-05; Iter time 0.28s; ETA 3:44:13; Mem 14324.95MB
Epoch [5/100]; Iter [3040/51500]; loss 1.9310; gen_loss 1.9310; LR 9.92e-05; Iter time 0.28s; ETA 3:44:20; Mem 14324.95MB
Epoch [5/100]; Iter [3050/51500]; loss 2.1565; gen_loss 2.1565; LR 9.91e-05; Iter time 0.28s; ETA 3:43:54; Mem 14324.95MB
Epoch [5/100]; Iter [3060/51500]; loss 2.4953; gen_loss 2.4953; LR 9.91e-05; Iter time 0.28s; ETA 3:43:31; Mem 14324.95MB
Epoch [5/100]; Iter [3070/51500]; loss 1.7984; gen_loss 1.7984; LR 9.91e-05; Iter time 0.28s; ETA 3:43:40; Mem 14324.95MB
Epoch [5/100]; Iter [3080/51500]; loss 2.0974; gen_loss 2.0974; LR 9.91e-05; Iter time 0.28s; ETA 3:43:13; Mem 14324.95MB
Epoch [6/100]; Iter [3090/51500]; loss 2.0030; gen_loss 2.0030; LR 9.91e-05; Iter time 0.28s; ETA 3:43:34; Mem 14324.95MB
Epoch [6/100]; Iter [3100/51500]; loss 2.0020; gen_loss 2.0020; LR 9.91e-05; Iter time 0.28s; ETA 3:42:48; Mem 14324.95MB
Epoch [6/100]; Iter [3110/51500]; loss 2.0889; gen_loss 2.0889; LR 9.91e-05; Iter time 0.28s; ETA 3:42:34; Mem 14324.95MB
Epoch [6/100]; Iter [3120/51500]; loss 1.8627; gen_loss 1.8627; LR 9.91e-05; Iter time 0.28s; ETA 3:43:04; Mem 14324.95MB
Epoch [6/100]; Iter [3130/51500]; loss 2.3088; gen_loss 2.3088; LR 9.91e-05; Iter time 0.28s; ETA 3:42:23; Mem 14324.95MB
Epoch [6/100]; Iter [3140/51500]; loss 2.1576; gen_loss 2.1576; LR 9.91e-05; Iter time 0.28s; ETA 3:42:42; Mem 14324.95MB
Epoch [6/100]; Iter [3150/51500]; loss 2.4005; gen_loss 2.4005; LR 9.91e-05; Iter time 0.28s; ETA 3:42:57; Mem 14324.95MB
Epoch [6/100]; Iter [3160/51500]; loss 2.0291; gen_loss 2.0291; LR 9.91e-05; Iter time 0.28s; ETA 3:43:11; Mem 14324.95MB
Epoch [6/100]; Iter [3170/51500]; loss 2.0444; gen_loss 2.0444; LR 9.91e-05; Iter time 0.28s; ETA 3:42:59; Mem 14324.95MB
Epoch [6/100]; Iter [3180/51500]; loss 2.4417; gen_loss 2.4417; LR 9.91e-05; Iter time 0.28s; ETA 3:43:19; Mem 14324.95MB
Epoch [6/100]; Iter [3190/51500]; loss 2.0392; gen_loss 2.0392; LR 9.91e-05; Iter time 0.28s; ETA 3:42:08; Mem 14324.95MB
Epoch [6/100]; Iter [3200/51500]; loss 2.0785; gen_loss 2.0785; LR 9.91e-05; Iter time 0.28s; ETA 3:41:57; Mem 14324.95MB
Epoch [6/100]; Iter [3210/51500]; loss 1.9796; gen_loss 1.9796; LR 9.91e-05; Iter time 0.28s; ETA 3:42:33; Mem 14324.95MB
Epoch [6/100]; Iter [3220/51500]; loss 2.2098; gen_loss 2.2098; LR 9.90e-05; Iter time 0.28s; ETA 3:42:18; Mem 14324.95MB
Epoch [6/100]; Iter [3230/51500]; loss 1.8824; gen_loss 1.8824; LR 9.90e-05; Iter time 0.28s; ETA 3:41:40; Mem 14324.95MB
Epoch [6/100]; Iter [3240/51500]; loss 2.1668; gen_loss 2.1668; LR 9.90e-05; Iter time 0.28s; ETA 3:42:39; Mem 14324.95MB
Epoch [6/100]; Iter [3250/51500]; loss 1.9667; gen_loss 1.9667; LR 9.90e-05; Iter time 0.28s; ETA 3:42:26; Mem 14324.95MB
Epoch [6/100]; Iter [3260/51500]; loss 2.0425; gen_loss 2.0425; LR 9.90e-05; Iter time 0.28s; ETA 3:42:31; Mem 14324.95MB
Epoch [6/100]; Iter [3270/51500]; loss 1.9973; gen_loss 1.9973; LR 9.90e-05; Iter time 0.28s; ETA 3:43:02; Mem 14324.95MB
Epoch [6/100]; Iter [3280/51500]; loss 2.0336; gen_loss 2.0336; LR 9.90e-05; Iter time 0.28s; ETA 3:43:10; Mem 14324.95MB
Epoch [6/100]; Iter [3290/51500]; loss 2.1395; gen_loss 2.1395; LR 9.90e-05; Iter time 0.28s; ETA 3:41:15; Mem 14324.95MB
Epoch [6/100]; Iter [3300/51500]; loss 2.1576; gen_loss 2.1576; LR 9.90e-05; Iter time 0.28s; ETA 3:41:46; Mem 14324.95MB
Epoch [6/100]; Iter [3310/51500]; loss 1.9000; gen_loss 1.9000; LR 9.90e-05; Iter time 0.28s; ETA 3:42:34; Mem 14324.95MB
Epoch [6/100]; Iter [3320/51500]; loss 1.9765; gen_loss 1.9765; LR 9.90e-05; Iter time 0.28s; ETA 3:42:23; Mem 14324.95MB
Epoch [6/100]; Iter [3330/51500]; loss 2.1701; gen_loss 2.1701; LR 9.90e-05; Iter time 0.28s; ETA 3:41:25; Mem 14324.95MB
Epoch [6/100]; Iter [3340/51500]; loss 2.1208; gen_loss 2.1208; LR 9.90e-05; Iter time 0.28s; ETA 3:41:58; Mem 14324.95MB
Epoch [6/100]; Iter [3350/51500]; loss 1.8407; gen_loss 1.8407; LR 9.90e-05; Iter time 0.28s; ETA 3:41:57; Mem 14324.95MB
Epoch [6/100]; Iter [3360/51500]; loss 1.8903; gen_loss 1.8903; LR 9.90e-05; Iter time 0.28s; ETA 3:42:06; Mem 14324.95MB
Epoch [6/100]; Iter [3370/51500]; loss 1.5898; gen_loss 1.5898; LR 9.90e-05; Iter time 0.28s; ETA 3:42:13; Mem 14324.95MB
Epoch [6/100]; Iter [3380/51500]; loss 1.6920; gen_loss 1.6920; LR 9.90e-05; Iter time 0.28s; ETA 3:42:38; Mem 14324.95MB
Epoch [6/100]; Iter [3390/51500]; loss 2.0208; gen_loss 2.0208; LR 9.89e-05; Iter time 0.28s; ETA 3:41:51; Mem 14324.95MB
Epoch [6/100]; Iter [3400/51500]; loss 1.8412; gen_loss 1.8412; LR 9.89e-05; Iter time 0.28s; ETA 3:41:43; Mem 14324.95MB
Epoch [6/100]; Iter [3410/51500]; loss 2.1696; gen_loss 2.1696; LR 9.89e-05; Iter time 0.28s; ETA 3:42:01; Mem 14324.95MB
Epoch [6/100]; Iter [3420/51500]; loss 1.9812; gen_loss 1.9812; LR 9.89e-05; Iter time 0.28s; ETA 3:42:32; Mem 14324.95MB
Epoch [6/100]; Iter [3430/51500]; loss 2.0160; gen_loss 2.0160; LR 9.89e-05; Iter time 0.28s; ETA 3:42:13; Mem 14324.95MB
Epoch [6/100]; Iter [3440/51500]; loss 2.0753; gen_loss 2.0753; LR 9.89e-05; Iter time 0.28s; ETA 3:41:27; Mem 14324.95MB
